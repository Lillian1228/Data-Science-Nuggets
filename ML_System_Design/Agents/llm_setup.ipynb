{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFQ/bYuwLGYWo1dYjsAUkT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lillian1228/Data-Science-Nuggets/blob/main/ML_System_Design/Agents/llm_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnWWOfTPxRyq",
        "outputId": "f96eaabe-ac35-455d-9487-534eb04e40c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#!pip install openai python-dotenv\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from typing import List, Dict\n",
        "\n",
        "# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HelloAgentsLLM:\n",
        "    \"\"\"\n",
        "    ä¸ºæœ¬ä¹¦ \"Hello Agents\" å®šåˆ¶çš„LLMå®¢æˆ·ç«¯ã€‚\n",
        "    å®ƒç”¨äºè°ƒç”¨ä»»ä½•å…¼å®¹OpenAIæ¥å£çš„æœåŠ¡ï¼Œå¹¶é»˜è®¤ä½¿ç”¨æµå¼å“åº”ã€‚\n",
        "    \"\"\"\n",
        "    def __init__(self, model: str = None, apiKey: str = None, baseUrl: str = None, timeout: int = None):\n",
        "        \"\"\"\n",
        "        åˆå§‹åŒ–å®¢æˆ·ç«¯ã€‚ä¼˜å…ˆä½¿ç”¨ä¼ å…¥å‚æ•°ï¼Œå¦‚æœæœªæä¾›ï¼Œåˆ™ä»ç¯å¢ƒå˜é‡åŠ è½½ã€‚\n",
        "        \"\"\"\n",
        "        self.model = model or os.getenv(\"LLM_MODEL_ID\")\n",
        "        apiKey = apiKey or os.getenv(\"LLM_API_KEY\")\n",
        "        baseUrl = baseUrl or os.getenv(\"LLM_BASE_URL\")\n",
        "        timeout = timeout or int(os.getenv(\"LLM_TIMEOUT\", 60))\n",
        "\n",
        "        if not all([self.model, apiKey, baseUrl]):\n",
        "            raise ValueError(\"æ¨¡å‹IDã€APIå¯†é’¥å’ŒæœåŠ¡åœ°å€å¿…é¡»è¢«æä¾›æˆ–åœ¨.envæ–‡ä»¶ä¸­å®šä¹‰ã€‚\")\n",
        "\n",
        "        self.client = OpenAI(api_key=apiKey, base_url=baseUrl, timeout=timeout)\n",
        "\n",
        "    def think(self, messages: List[Dict[str, str]], temperature: float = 0) -> str:\n",
        "        \"\"\"\n",
        "        è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ€è€ƒï¼Œå¹¶è¿”å›å…¶å“åº”ã€‚\n",
        "        \"\"\"\n",
        "        print(f\"ğŸ§  æ­£åœ¨è°ƒç”¨ {self.model} æ¨¡å‹...\")\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=messages,\n",
        "                temperature=temperature,\n",
        "                stream=True,\n",
        "            )\n",
        "\n",
        "            # å¤„ç†æµå¼å“åº”\n",
        "            print(\"âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:\")\n",
        "            collected_content = []\n",
        "            for chunk in response:\n",
        "                content = chunk.choices[0].delta.content or \"\"\n",
        "                print(content, end=\"\", flush=True)\n",
        "                collected_content.append(content)\n",
        "            print()  # åœ¨æµå¼è¾“å‡ºç»“æŸåæ¢è¡Œ\n",
        "            return \"\".join(collected_content)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {e}\")\n",
        "            return None\n",
        "\n"
      ],
      "metadata": {
        "id": "qxo22DhYxaPW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llmClient = HelloAgentsLLM()\n",
        "\n",
        "exampleMessages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes Python code.\"},\n",
        "        {\"role\": \"user\", \"content\": \"å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•\"}\n",
        "    ]\n",
        "\n",
        "llmClient.think(exampleMessages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "wfuSJCo_zy7m",
        "outputId": "1b6c7450-0665-46b1-f6f3-541d562f6958"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'exampleMessages' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-799935780.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LLM_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mllmClient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHelloAgentsLLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mllmClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexampleMessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'exampleMessages' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "try:\n",
        "    llmClient = HelloAgentsLLM()\n",
        "    exampleMessages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes Python code.\"},\n",
        "            {\"role\": \"user\", \"content\": \"å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•\"}\n",
        "        ]\n",
        "\n",
        "\n",
        "    print(\"--- è°ƒç”¨LLM ---\")\n",
        "    responseText = llmClient.think(exampleMessages)\n",
        "    if responseText:\n",
        "        print(\"\\n\\n--- å®Œæ•´æ¨¡å‹å“åº” ---\")\n",
        "        print(responseText)\n",
        "\n",
        "except ValueError as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g4DVC6Bz2iL",
        "outputId": "88312850-84e2-410a-db20-9cfb3a453127"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- è°ƒç”¨LLM ---\n",
            "ğŸ§  æ­£åœ¨è°ƒç”¨ gpt-5 æ¨¡å‹...\n",
            "âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: Error code: 404 - {'error': {'message': 'Invalid URL (POST /v1/chat/completions/chat/completions)', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SH3CWhbJz33g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}