{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Pandas Interview â€” Handsâ€‘On Practice (with Solutions)\n",
        "\n",
        "**Goal:** Practice advanced, interviewâ€‘style Pandas problems that emphasize *correctness*, *expressiveness*, and *performance*. Most tasks create small synthetic datasets so you can run everything locally.\n",
        "\n",
        "> Tip: Try to complete the **Your turn** cells before revealing/reading the **Solution** cells right below them.\n",
        "\n",
        "**Covers:** GroupBy, Window/Rolling/EWM, Reshaping & MultiIndex, Joins (incl. asâ€‘of & interval), Time series, Missing data & conditionals, Text & explode, Categoricals, Index alignment & broadcasting, Method chaining & `pipe`, and a Cohort bonus.\n",
        "\n",
        "Tested with **pandas â‰¥ 1.5** and **Python â‰¥ 3.9**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "pd.set_option('display.width', 120)\n",
        "pd.set_option('display.max_columns', 50)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(pd.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) GroupBy Mastery\n",
        "We'll build a synthetic dataset with groups `A`, `B` and numeric columns `X`, `Y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Data: GroupBy ---\n",
        "N = 500\n",
        "A = np.random.choice(list('ABC'), size=N, p=[0.4, 0.4, 0.2])\n",
        "B = np.random.choice(['u','v','w'], size=N)\n",
        "X = np.random.normal(loc=100, scale=20, size=N)\n",
        "Y = np.random.lognormal(mean=4.0, sigma=0.5, size=N)\n",
        "\n",
        "df = pd.DataFrame({'A': A, 'B': B, 'X': X, 'Y': Y})\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Mixed aggregations with named outputs\n",
        "**Your turn:** Aggregate by `['A','B']` computing:\n",
        "- `mean_x = mean(X)`\n",
        "- `n = size`\n",
        "- `p90_y = 90th percentile of Y`\n",
        "\n",
        "Return a tidy DataFrame with a simple index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: your solution here\n",
        "# out = (\n",
        "#     df.groupby(['A', 'B'])\n",
        "#       .agg(\n",
        "#           mean_x=(..., ...),\n",
        "#           n=(..., ...),\n",
        "#           p90_y=(..., ...),\n",
        "#       )\n",
        "#       .reset_index()\n",
        "# )\n",
        "# out.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out = (\n",
        "    df.groupby(['A', 'B'])\n",
        "      .agg(\n",
        "          mean_x=('X', 'mean'),\n",
        "          n=('X', 'size'),\n",
        "          p90_y=('Y', lambda s: s.quantile(0.90))\n",
        "      )\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "# simple checks\n",
        "assert {'A','B','mean_x','n','p90_y'} <= set(out.columns)\n",
        "assert out.shape[0] <= df[['A','B']].drop_duplicates().shape[0]\n",
        "out.sort_values(['A','B']).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 `agg` vs `transform` â€” groupwise zâ€‘score\n",
        "**Your turn:** Create `zscore_X` = zâ€‘score of `X` within each `A` group: `(X - mean_A) / std_A` using **`transform`**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: your solution here\n",
        "# df['zscore_X'] = df.groupby('A')['X'].transform(lambda s: ...)\n",
        "# df[['A','X','zscore_X']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['zscore_X'] = df.groupby('A')['X'].transform(lambda s: (s - s.mean()) / s.std(ddof=0))\n",
        "assert np.isclose(df.groupby('A')['zscore_X'].mean().abs().max(), 0, atol=1e-6)\n",
        "df[['A','X','zscore_X']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Filter groups efficiently\n",
        "**Your turn:** Keep only groups with at least **30** rows (by `A,B`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: your solution here\n",
        "# keep = df.groupby(['A','B'])['X'].transform('size') >= 30\n",
        "# df_big = df[keep]\n",
        "# df_big['AB'] = df_big['A'] + df_big['B']\n",
        "# df_big.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "keep = df.groupby(['A','B'])['X'].transform('size') >= 30\n",
        "df_big = df[keep].copy()\n",
        "assert df_big.groupby(['A','B']).size().min() >= 30\n",
        "len(df), len(df_big)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Window / Rolling / Expanding / EWM\n",
        "Create perâ€‘ID time series and compute rolling/exp-weighted features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Data: Time series ---\n",
        "ids = np.repeat(np.arange(5), 60)\n",
        "base = pd.Timestamp('2023-01-01')\n",
        "ts = np.concatenate([pd.date_range(base, periods=60, freq='D').values for _ in range(5)])\n",
        "val = np.random.normal(0, 1, size=len(ids)).cumsum() + np.repeat(np.random.randn(5)*5, 60)\n",
        "\n",
        "ts_df = pd.DataFrame({'id': ids, 'ts': pd.to_datetime(ts), 'value': val}).sort_values(['id','ts'])\n",
        "ts_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Timeâ€‘based rolling window\n",
        "**Your turn:** For each `id`, compute a 30â€‘day rolling **sum** of `value` with `min_periods=1` using a **timeâ€‘based** window (i.e., `rolling('30D')`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: your solution here\n",
        "# roll_sum = (ts_df.set_index('ts')\n",
        "#                 .groupby('id')['value']\n",
        "#                 .rolling('30D', min_periods=1)\n",
        "#                 .sum()\n",
        "#                 .reset_index(level=0, drop=True))\n",
        "# ts_df['roll30d_sum'] = roll_sum\n",
        "# ts_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "roll_sum = (ts_df.set_index('ts')\n",
        "                .groupby('id')['value']\n",
        "                .rolling('30D', min_periods=1)\n",
        "                .sum()\n",
        "                .reset_index(level=0, drop=True))\n",
        "\n",
        "ts_df['roll30d_sum'] = roll_sum\n",
        "assert ts_df['roll30d_sum'].notna().all()\n",
        "ts_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Exponentiallyâ€‘weighted mean per `id`\n",
        "**Your turn:** Compute `ewm_mean` with `span=10` per `id`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: your solution here\n",
        "# ts_df['ewm_mean'] = ts_df.groupby('id')['value'].transform(lambda s: s.ewm(span=10).mean())\n",
        "# ts_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ts_df['ewm_mean'] = ts_df.groupby('id')['value'].transform(lambda s: s.ewm(span=10).mean())\n",
        "assert ts_df['ewm_mean'].isna().sum() == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Custom rolling statistic\n",
        "**Your turn:** Compute a 7â€‘point rolling **range** (`max - min`) per `id` with `min_periods=3`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: your solution here\n",
        "# r = (ts_df.groupby('id')['value']\n",
        "#           .rolling(7, min_periods=3)\n",
        "#           .agg(lambda x: x.max() - x.min())\n",
        "#           .reset_index(level=0, drop=True))\n",
        "# ts_df['roll7_range'] = r\n",
        "# ts_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "r = (ts_df.groupby('id')['value']\n",
        "          .rolling(7, min_periods=3)\n",
        "          .agg(lambda x: x.max() - x.min())\n",
        "          .reset_index(level=0, drop=True))\n",
        "\n",
        "ts_df['roll7_range'] = r\n",
        "assert ts_df['roll7_range'].isna().sum() > 0  # first few rows per id are NaN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Reshaping & MultiIndex: `pivot`, `pivot_table`, `melt`, `stack`, `unstack`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Data: transactions ---\n",
        "_dates = pd.date_range('2023-01-01', periods=30, freq='D')\n",
        "_cats = ['electronics','books','toys']\n",
        "rows = []\n",
        "for d in _dates:\n",
        "    for c in _cats:\n",
        "        sales = np.random.poisson(lam={'electronics':15,'books':25,'toys':10}[c])\n",
        "        qty = sales // np.random.randint(1,4)\n",
        "        rows.append((d, c, sales, qty))\n",
        "trans = pd.DataFrame(rows, columns=['date','category','sales','qty'])\n",
        "trans.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Pivot table with multiple values and margins\n",
        "**Your turn:** Create a wide table with index=`date`, columns=`category`, values=`['sales','qty']`, aggregating by **sum**, `fill_value=0`, and include totals (`margins=True`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: your solution here\n",
        "# w = trans.pivot_table(index='date', columns='category', values=['sales','qty'], aggfunc='sum', fill_value=0, margins=True)\n",
        "# w.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "w = trans.pivot_table(index='date', columns='category', values=['sales','qty'], aggfunc='sum', fill_value=0, margins=True)\n",
        "assert isinstance(w.columns, pd.MultiIndex)\n",
        "w.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Long â†” Wide with `stack` / `melt`\n",
        "**Your turn:** Convert `w` back to long form with columns `date`, `category`, `metric` (`sales` or `qty`), and `value`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: your solution here\n",
        "# long = (w.drop(index='All')\n",
        "#          .stack(level=1)\n",
        "#          .rename_axis(['date','category'])\n",
        "#          .reset_index())\n",
        "# long = long.rename(columns={0:'value'})\n",
        "# long['metric'] = long.columns.get_level_values(0) if isinstance(long.columns, pd.MultiIndex) else 'value'\n",
        "# long.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop the grand total row to keep tidy semantics\n",
        "w_no_all = w.drop(index='All')\n",
        "long = (\n",
        "    w_no_all.stack(level=1)  # stack categories\n",
        "          .rename_axis(['date','category'])\n",
        "          .reset_index()\n",
        ")\n",
        "# After stacking, columns are still MultiIndex (level 0: metric)\n",
        "long = long.rename(columns={'sales':'sales', 'qty':'qty'})\n",
        "long = long.melt(id_vars=['date','category'], value_vars=['sales','qty'], var_name='metric', value_name='value')\n",
        "assert set(long['metric'].unique()) == {'sales','qty'}\n",
        "long.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Joins (incl. asâ€‘of, cross, and interval mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Data: joins ---\n",
        "users = pd.DataFrame({\n",
        "    'user_id': [1,2,3,4],\n",
        "    'tier': ['silver','gold','gold','platinum']\n",
        "})\n",
        "orders = pd.DataFrame({\n",
        "    'order_id':[10,11,12,13,14],\n",
        "    'user_id':[1,2,2,3,4],\n",
        "    'amount':[120.0, 55.0, 75.0, 200.0, 500.0]\n",
        "})\n",
        "users, orders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Basic merge with integrity checks\n",
        "**Your turn:** Leftâ€‘join `orders` with `users` on `user_id`, include `_merge` indicator, and validate `m:1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: your solution here\n",
        "# merged = pd.merge(orders, users, on='user_id', how='left', indicator=True, validate='m:1')\n",
        "# merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged = pd.merge(orders, users, on='user_id', how='left', indicator=True, validate='m:1')\n",
        "assert set(merged['_merge'].unique()) == {'both'}\n",
        "merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Asâ€‘of join (nearest by time per key)\n",
        "**Your turn:** For each trade, attach the latest quote up to that time (per symbol) within 2 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data: trades & quotes\n",
        "quotes = pd.DataFrame({\n",
        "    'symbol':['A']*5 + ['B']*5,\n",
        "    'time':  list(pd.date_range('2023-01-01 09:30', periods=5, freq='T')) +\n",
        "             list(pd.date_range('2023-01-01 09:30', periods=5, freq='2T')),\n",
        "    'bid':[99,99.5,100,100.5,101, 50,50.5,51,51.5,52],\n",
        "})\n",
        "trades = pd.DataFrame({\n",
        "    'symbol':['A','A','B','B'],\n",
        "    'time':  [pd.Timestamp('2023-01-01 09:31:20'), pd.Timestamp('2023-01-01 09:33:00'),\n",
        "              pd.Timestamp('2023-01-01 09:31:00'), pd.Timestamp('2023-01-01 09:35:00')],\n",
        "    'qty':[100,200,150,300],\n",
        "})\n",
        "\n",
        "quotes = quotes.sort_values(['symbol','time'])\n",
        "trades = trades.sort_values(['symbol','time'])\n",
        "\n",
        "# TODO: your solution here\n",
        "# matched = pd.merge_asof(trades, quotes, on='time', by='symbol', direction='backward', tolerance=pd.Timedelta('2min'))\n",
        "# matched"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "matched = pd.merge_asof(trades, quotes, on='time', by='symbol', direction='backward', tolerance=pd.Timedelta('2min'))\n",
        "assert matched['bid'].notna().all()\n",
        "matched"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Cross join (Cartesian product)\n",
        "**Your turn:** Create all combinations of `users` and a small `scenarios` table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scenarios = pd.DataFrame({'scenario':['base','stress']})\n",
        "# TODO: your solution here\n",
        "# cx = users.merge(scenarios, how='cross')\n",
        "# cx.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cx = users.merge(scenarios, how='cross')\n",
        "assert cx.shape[0] == len(users) * len(scenarios)\n",
        "cx.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Interval mapping (nonâ€‘equi join)\n",
        "**Your turn:** Map each value to a `[low, high]` band."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data: bands and values\n",
        "bands = pd.DataFrame({'band':['L','M','H'], 'low':[0, 100, 200], 'high':[99, 199, 1_000]})\n",
        "values = pd.DataFrame({'value':[10, 150, 450, 220, 90]})\n",
        "\n",
        "# TODO: your solution here â€” use IntervalIndex and merge\n",
        "# intervals = pd.IntervalIndex.from_arrays(bands['low'], bands['high'], closed='both')\n",
        "# bands2 = bands.set_index(pd.Index(intervals, name='interval'))\n",
        "# out = pd.merge(values, bands2.reset_index(), left_on='value', right_on='interval', how='left')\n",
        "# out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "intervals = pd.IntervalIndex.from_arrays(bands['low'], bands['high'], closed='both')\n",
        "bands2 = bands.set_index(pd.Index(intervals, name='interval'))\n",
        "out_intervals = pd.merge(values, bands2.reset_index(), left_on='value', right_on='interval', how='left')\n",
        "assert out_intervals['band'].notna().sum() == len(values)\n",
        "out_intervals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Time Series Essentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Data: irregular events ---\n",
        "raw = pd.DataFrame({\n",
        "    'ts': pd.to_datetime(['2023-01-01 09:00','2023-01-02 13:30','2023-01-05 10:00']),\n",
        "    'value':[10, np.nan, 25]\n",
        "})\n",
        "raw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Resample to business days with forward fill (limit=2)\n",
        "**Your turn:** Set `ts` as index, resample to 'B', forwardâ€‘fill with a maximum gap of 2 steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: your solution here\n",
        "# out = (raw.set_index('ts')\n",
        "#            .resample('B')\n",
        "#            .ffill(limit=2)\n",
        "#            .reset_index())\n",
        "# out.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out_ts = (raw.set_index('ts')\n",
        "             .resample('B')\n",
        "             .ffill(limit=2)\n",
        "             .reset_index())\n",
        "assert out_ts['value'].isna().sum() >= 0\n",
        "out_ts.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Timezones â€” normalize to UTC then convert\n",
        "**Your turn:** Treat the times as US/Eastern, convert to UTC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: your solution here\n",
        "# tz = pd.Series(pd.to_datetime(raw['ts'])).dt.tz_localize('US/Eastern').dt.tz_convert('UTC')\n",
        "# tz.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tz = pd.Series(pd.to_datetime(raw['ts'])).dt.tz_localize('US/Eastern').dt.tz_convert('UTC')\n",
        "assert str(tz.dt.tz.iloc[0]) == 'UTC'\n",
        "tz.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Missing Data & Conditional Logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "miss = pd.DataFrame({'x':[1, -1, 5, -3, 2], 'y':[np.nan, 3.0, np.nan, 4.0, 5.0], 'auto':[None, 'a', None, 'b', None], 'manual':['A', None, None, None, 'E']})\n",
        "miss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 NAâ€‘safe dtypes and `where`\n",
        "**Your turn:** Create `adj` = `x` if `x â‰¥ 0` else `NA`, using nullable `Int64` dtype."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: your solution here\n",
        "# miss['adj'] = miss['x'].where(miss['x'] >= 0, other=pd.NA).astype('Int64')\n",
        "# miss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "miss['adj'] = miss['x'].where(miss['x'] >= 0, other=pd.NA).astype('Int64')\n",
        "assert str(miss['adj'].dtype) == 'Int64'\n",
        "miss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Coalesce with `combine_first`\n",
        "**Your turn:** Create `best` that prefers `manual` over `auto`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: your solution here\n",
        "# miss['best'] = pd.Series(miss['manual'], dtype='string').combine_first(pd.Series(miss['auto'], dtype='string'))\n",
        "# miss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "miss['best'] = pd.Series(miss['manual'], dtype='string').combine_first(pd.Series(miss['auto'], dtype='string'))\n",
        "assert miss['best'].isna().sum() == 0\n",
        "miss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Groupwise interpolate by time\n",
        "**Your turn:** Interpolate missing `value` per `id` using `method='time'`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data: groupwise interpolation\n",
        "interp = pd.DataFrame({\n",
        "    'id': np.repeat([1,2], 5),\n",
        "    'ts': list(pd.date_range('2023-01-01', periods=5, freq='D'))*2,\n",
        "    'value':[1, np.nan, 3, np.nan, 5, 10, np.nan, 14, np.nan, 18]\n",
        "})\n",
        "\n",
        "# TODO: your solution here\n",
        "# out = (interp.sort_values(['id','ts'])\n",
        "#             .set_index('ts')\n",
        "#             .groupby('id')['value']\n",
        "#             .apply(lambda s: s.interpolate('time'))\n",
        "#             .reset_index(level=0, drop=True)\n",
        "#             .reset_index())\n",
        "# out.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out_interp = (interp.sort_values(['id','ts'])\n",
        "                .set_index('ts')\n",
        "                .groupby('id')['value']\n",
        "                .apply(lambda s: s.interpolate('time'))\n",
        "                .reset_index(level=0, drop=True)\n",
        "                .reset_index())\n",
        "assert out_interp['value'].isna().sum() == 0\n",
        "out_interp.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Vectorized Text & `explode`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_df = pd.DataFrame({\n",
        "    'user':['u1','u2','u3'],\n",
        "    'text':['Area: 120; Height: 50', 'Area: 90; Height: 60', 'Height: 70'],\n",
        "    'tags':['red;green', 'blue', 'green;yellow']\n",
        "})\n",
        "text_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Regex extract with named group\n",
        "**Your turn:** Extract `area` as integer from `text` (missing if absent)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: your solution here\n",
        "# pat = r\"Area:\\s*(?P<area>\\d+)\"\n",
        "# text_df['area'] = text_df['text'].str.extract(pat, expand=False).astype('Int64')\n",
        "# text_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pat = r\"Area:\\s*(?P<area>\\d+)\"\n",
        "text_df['area'] = text_df['text'].str.extract(pat, expand=False).astype('Int64')\n",
        "assert text_df['area'].dtype == 'Int64'\n",
        "text_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Split to rows with `explode` and aggregate\n",
        "**Your turn:** Split `tags` on `;`, explode to rows, and compute number of unique tags per user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: your solution here\n",
        "# exploded = text_df.assign(tag=text_df['tags'].str.split(';')).explode('tag')\n",
        "# out = exploded.groupby('user')['tag'].nunique().reset_index(name='n_tags')\n",
        "# out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "exploded = text_df.assign(tag=text_df['tags'].str.split(';')).explode('tag')\n",
        "out_tags = exploded.groupby('user')['tag'].nunique().reset_index(name='n_tags')\n",
        "assert out_tags['n_tags'].min() >= 1\n",
        "out_tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Categoricals & custom sort order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_df = pd.DataFrame({'tier':['gold','platinum','silver','gold','silver'], 'revenue':[200, 1000, 120, 350, 180]})\n",
        "cat_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1 Sort tiers by custom order\n",
        "**Your turn:** Use ordered `CategoricalDtype(['silver','gold','platinum'])` then sort by `tier` asc, `revenue` desc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: your solution here\n",
        "# order = pd.CategoricalDtype(['silver','gold','platinum'], ordered=True)\n",
        "# cat_df['tier'] = cat_df['tier'].astype(order)\n",
        "# out = cat_df.sort_values(['tier','revenue'], ascending=[True, False])\n",
        "# out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "order = pd.CategoricalDtype(['silver','gold','platinum'], ordered=True)\n",
        "cat_df['tier'] = cat_df['tier'].astype(order)\n",
        "out_cat = cat_df.sort_values(['tier','revenue'], ascending=[True, False]).reset_index(drop=True)\n",
        "assert list(out_cat['tier'].astype(str).unique()) == ['silver','gold','platinum']\n",
        "out_cat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Index alignment, broadcasting, and smart sorting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "align_df = pd.DataFrame({'grp': list('AAABBB'), 'x':[5,7,9, 2,4,6]})\n",
        "align_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.1 Center `x` by group mean (alignment)\n",
        "**Your turn:** Create `x_centered = x - mean(x) per grp`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: your solution here\n",
        "# m = align_df.groupby('grp')['x'].transform('mean')\n",
        "# align_df['x_centered'] = align_df['x'] - m\n",
        "# align_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m = align_df.groupby('grp')['x'].transform('mean')\n",
        "align_df['x_centered'] = align_df['x'] - m\n",
        "assert np.isclose(align_df.groupby('grp')['x_centered'].mean().abs().max(), 0)\n",
        "align_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.2 Vectorized conditionals with `map` / `replace`\n",
        "**Your turn:** Map `grp` Aâ†’'alpha', Bâ†’'beta' into `label`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: your solution here\n",
        "# align_df['label'] = align_df['grp'].map({'A':'alpha','B':'beta'})\n",
        "# align_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "align_df['label'] = align_df['grp'].map({'A':'alpha','B':'beta'})\n",
        "assert set(align_df['label'].unique()) == {'alpha','beta'}\n",
        "align_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.3 Sort with a key function\n",
        "**Your turn:** Sort strings by length using `key=`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data\n",
        "srt = pd.DataFrame({'s':['bbb','a','ccccc','dddd']})\n",
        "# TODO: your solution here\n",
        "# srt_sorted = srt.sort_values('s', key=lambda s: s.str.len())\n",
        "# srt_sorted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "srt = pd.DataFrame({'s':['bbb','a','ccccc','dddd']})\n",
        "srt_sorted = srt.sort_values('s', key=lambda s: s.str.len()).reset_index(drop=True)\n",
        "assert list(srt_sorted['s']) == ['a','bbb','dddd','ccccc']\n",
        "srt_sorted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Method chaining & `pipe`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_df = pd.DataFrame({\n",
        "    'account':['x','x','y','y','y'],\n",
        "    'status':['open','closed','open','open','closed'],\n",
        "    'qty':[10,5,2,8,4],\n",
        "    'price':[100,110,200,50,80]\n",
        "})\n",
        "pipe_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.1 Build a clean pipeline\n",
        "**Your turn:**\n",
        "1. Filter `status == \"open\"`\n",
        "2. Add `cost = qty * price`\n",
        "3. Keep only rows with `cost > 300`\n",
        "4. Sort by `account` asc, `cost` desc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: your solution here\n",
        "# def keep_costly(d, th=300):\n",
        "#     return d[d['cost'] > th]\n",
        "# out = (\n",
        "#     pipe_df.query('status == \"open\"')\n",
        "#            .assign(cost=lambda d: d['qty'] * d['price'])\n",
        "#            .pipe(keep_costly, th=300)\n",
        "#            .sort_values(['account','cost'], ascending=[True, False])\n",
        "# )\n",
        "# out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def keep_costly(d, th=300):\n",
        "    return d[d['cost'] > th]\n",
        "\n",
        "out_pipe = (\n",
        "    pipe_df.query('status == \"open\"')\n",
        "           .assign(cost=lambda d: d['qty'] * d['price'])\n",
        "           .pipe(keep_costly, th=300)\n",
        "           .sort_values(['account','cost'], ascending=[True, False])\n",
        ")\n",
        "assert (out_pipe['cost'] > 300).all()\n",
        "out_pipe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus) Cohort Analysis (first purchase month)\n",
        "**Your turn:** Compute each user's first purchase month and add a `cohort_index` (months since first purchase)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data\n",
        "co = pd.DataFrame({\n",
        "    'user': np.repeat(['u1','u2','u3'], [3,4,2]),\n",
        "    'ts':   pd.to_datetime(['2023-01-15','2023-03-10','2023-03-25',\n",
        "                            '2023-02-02','2023-02-15','2023-04-01','2023-04-20',\n",
        "                            '2023-05-05','2023-07-10']),\n",
        "    'amount':[100,50,75, 40,60,80,90, 120, 200]\n",
        "})\n",
        "co"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: your solution here\n",
        "# co['month'] = co['ts'].dt.to_period('M')\n",
        "# first = co.groupby('user')['month'].min().rename('cohort')\n",
        "# co = co.join(first, on='user')\n",
        "# co['cohort_index'] = (co['month'] - co['cohort']).apply(lambda p: p.n)\n",
        "# co.sort_values(['user','ts'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "co['month'] = co['ts'].dt.to_period('M')\n",
        "first = co.groupby('user')['month'].min().rename('cohort')\n",
        "co = co.join(first, on='user')\n",
        "co['cohort_index'] = (co['month'] - co['cohort']).apply(lambda p: p.n)\n",
        "assert co.groupby('user')['cohort_index'].min().eq(0).all()\n",
        "co.sort_values(['user','ts'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Tips for Interviews\n",
        "- Prefer `agg`/`transform` to `apply` where possible; vectorize.\n",
        "- Use `validate=` in merges to catch integrity issues early.\n",
        "- For time series, always `sort_values` and standardize timezones.\n",
        "- Use nullable dtypes (`Int64`, `boolean`, `string`) for NAâ€‘safe operations.\n",
        "- Consider categoricals for memory/speed and correct custom ordering.\n",
        "\n",
        "Good luck! ðŸš€"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
