

Off-Policy Evaluation (OPE) for contextual bandits, also referred to as the **non-dynamic setting**, focuses on estimating the expected reward of a given decision policy using historical data generated by a potentially different policy. This is a fundamental problem in Reinforcement Learning (RL) and is particularly crucial in offline RL scenarios where direct experimentation with new policies is expensive, risky, or unethical


The most common OPE techniques in RL fall into 3 families: 
1. **Direct Method (DM)**, 
2. **Importance Sampling (IS)** and its variants 
3. **Doubly Robust (DR)** estimators

Under the non-dynamic setting, **replay** (sometimes called **replay evaluation**, **counterfactual replay**, or **trajectory replay**) is another OPE approach for contextual bandits. It’s often introduced as the most intuitive one before IPS/DR.



## 1. **Direct Method (DM)**

**Idea:**
Fit a supervised model to predict expected rewards given context and action, then evaluate the target policy by averaging these predicted rewards over the contexts.

**Assumptions:**

* The reward model is well-specified (low bias if accurate).
* Stationarity: reward distribution does not change between behavior and target policies.

**Data Requirements:**

* Logged triples $(x, a, r)$: context, action, reward.
* Enough coverage of actions in the logged data to fit a predictive model.

**Limitations:**

* High bias if the reward model is misspecified (e.g., poor generalization on unseen actions/contexts).
* Doesn’t use knowledge of how the data was collected (behavior policy).
* Typically lower variance than IS, but higher bias.


## 2.1 **Importance Sampling (IS) / Inverse Propensity Scoring (IPS)**

**Idea:**
Reweight logged data by the ratio of the target policy’s probability of taking the action to the behavior policy’s probability.

$$
\hat{V}_{\text{IPS}} = \frac{1}{n}\sum_{i=1}^n \frac{\pi(a_i \mid x_i)}{b(a_i \mid x_i)} r_i
$$

**Assumptions:**

* Logged data includes the **propensities** (action probabilities) under the behavior policy.
* **Overlap assumption:** if the target policy chooses an action with nonzero probability, the behavior policy must also have assigned that action nonzero probability.

**Data Requirements:**

* Contexts, actions, rewards, **and behavior policy probabilities** $b(a \mid x)$.

**Limitations:**

* High variance when propensities are small (rare actions).
* Unbiased if assumptions hold, but very data inefficient in large action spaces.


## 2.2 **Self-Normalized Importance Sampling (SNIPS)**

**Idea:**
Normalize IS weights to reduce variance.

$$
\hat{V}_{\text{SNIPS}} = \frac{\sum_{i=1}^n w_i r_i}{\sum_{i=1}^n w_i}, \quad w_i = \frac{\pi(a_i \mid x_i)}{b(a_i \mid x_i)}
$$

**Assumptions:**
Same as IS.

**Data Requirements:**
Same as IS.

**Limitations:**

* Introduces small bias, but reduces variance significantly compared to plain IS.
* Still unstable when target policy differs a lot from the behavior policy.

## 3. **Doubly Robust (DR) Estimator**

**Idea:**
Combine DM and IS. Use the reward model as a baseline and correct it with IS for unbiasedness.

$$
\hat{V}_{\text{DR}} = \frac{1}{n}\sum_{i=1}^n \Big[ \hat{r}(x_i, \pi) + \frac{\pi(a_i \mid x_i)}{b(a_i \mid x_i)} (r_i - \hat{r}(x_i, a_i)) \Big]
$$

**Assumptions:**

* Either the reward model is accurate (like DM), or propensities are correct (like IS).
* Only one of the two needs to be correct for consistent estimation.

**Data Requirements:**

* Everything from DM + IS: logged data with contexts, actions, rewards, behavior probabilities, and a trained reward model.

**Limitations:**

* Higher computational cost (need to fit a reward model + use importance weights).
* Still subject to high variance if weights are extreme.

## 4. **Replay Method (a.k.a. Counterfactual Evaluation)**

**Idea:**

* You "replay" the logged dataset through the target policy.
* Whenever the action chosen by the target policy matches the action actually taken in the logged data, you use the observed reward; otherwise, you skip the data point.
* The estimated value is the average reward over the kept samples.

$$
\hat{V}_{\text{replay}} = \frac{1}{|\{i : \pi(x_i) = a_i\}|} \sum_{i: \pi(x_i)=a_i} r_i
$$

**Assumptions:**

* **Logging policy must have chosen actions randomly enough** so that the target policy’s actions appear with reasonable frequency in the logged data.
* **Overlap assumption:** The behavior policy must cover the support of the target policy.

**Data Requirements**

* Logged dataset of $(x, a, r)$.
* Unlike IPS/DR, **you do not need to know the logging policy probabilities**.

**Limitations**

* **Sample inefficiency:** Most logged data gets discarded if the target policy often disagrees with the behavior policy.
* **High variance:** Because effective sample size can be much smaller than $n$.
* **Bias:** If the logging policy wasn’t sufficiently exploratory, replay underestimates target policy performance.
* Only practical when behavior policy is close to uniform/randomized.

**When is Replay Used?**

* Educational settings (bandit tutorials, proofs of concept).
* When propensities are **not logged** but you know the logging was close to uniform random.
* Small action spaces where overlap between logging and target policy is high.


✅ **Quick Comparison with IPS**

* **Replay** throws away unmatched samples; **IPS** keeps all samples but reweights them.
* **Replay** doesn’t need behavior policy probabilities; **IPS/DR** do.
* **Replay** is less general, but can be simpler and safer if you’re missing propensities.

---

✅ **Summary Table**

| Method | Needs behavior probs? | Needs reward model? | Bias                                    | Variance      | Key limitation                                          |
| ------ | --------------------- | ------------------- | --------------------------------------- | ------------- | ------------------------------------------------------- |
| DM     | No                    | Yes                 | High if model wrong                     | Low           | Model misspecification                                  |
| IS/IPS | Yes                   | No                  | Unbiased                                | High          | Extreme variance                                        |
| SNIPS  | Yes                   | No                  | Slight bias                             | Lower than IS | Still unstable for large action space                   |
| DR     | Yes                   | Yes                 | Low if either model or behavior correct | Moderate      | More complex, can still blow up with small propensities |
|Replay | No | No | Biased if logging policy not exploratory | High with small sample | sample inefficiency, less general |