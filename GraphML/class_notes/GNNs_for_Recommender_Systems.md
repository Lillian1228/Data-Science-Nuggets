### 7.1 Recommender Systems

- Preliminary of Recommendation

    Information Explosion in the era of Internet:
  - 10K+ movies in Netflix
  - 12M products in Amazon
  - 70M+ music tracks in Spotify
  - 10B+ videos on YouTube
  - 200B+ pins (images) in Pinterest

    Personalized recommendation (i.e., suggesting a small number of interesting items for each user) is critical for users to effectively explore the content of their interest.

#### 7.1.1 Task and Evaluation

- Recommender System as a Graph

    Recommender system can be naturally modeled as a **bipartite graph**:
    - A graph with two node types: users and items.
    - Edges connect users and items: 
      - Indicates user-item interaction (e.g., click, purchase, review etc.)
      - Often associated with timestamp (timing of the interaction).


- Recommendation Task

    Given past user-item interactions, Predict new items each user will interact in the future.
    
    &rarr; Can be cast as **link prediction** problem: Predict new user-item interaction edges given the past edges.

    &rarr; For ğ‘¢ âˆˆ ğ‘¼, ğ‘£ âˆˆ ğ‘½, we need to get a real-valued score ğ‘“(ğ‘¢, ğ‘£). For example, ğ‘“(ğ‘¢, ğ‘£) = $z_u \cdot z_v$

- Modern Recommender System

    Problem: Cannot evaluate **ğ‘“(ğ‘¢, ğ‘£)** for every user ğ‘¢ â€“ item ğ‘£ pair.

    Solution: 2-stage process

    - Candidate generation (cheap, fast)
    - Ranking (slow, accurate)

    <img src="src/L7/7.1.1.png" width="500"> 

- Top-K Recommendation

    For each user, we recommend ğ¾ items.
    - For recommendation to be effective, ğ‘² needs to be much smaller than the total number of items (up to billions)
    - ğ¾ is typically in the order of 10â€”100.
  
    The goal is to include as many **positive items** as possible in the top-ğ¾ recommended items.
    - Positive items = Items that the user will interact with in the future.

    Evaluation metric: Recall@ğ¾ (defined next)

- Evaluation metric: **Recall@ğ¾**

    For each user ğ’–,
    - Let ğ‘ƒğ‘¢ be a set of positive items the user will interact in the future.
    - Let ğ‘…ğ‘¢ be a set of items recommended by the model.
        - In top-ğ¾ recommendation, |ğ‘…ğ‘¢| = ğ¾.
        - Items that the user has already interacted are excluded.

        <img src="src/L7/7.1.2.png" width="300">  

    **Recall@ğ‘² for user ğ’– is |ğ‘·ğ’– âˆ© ğ‘¹ğ’–| / |ğ‘·ğ’–|**.
    - Higher value indicates more positive items are recommended in top-ğ¾ for user ğ‘¢.

    The final Recall@ğ¾ is computed by averaging the recall values across all users.

#### 7.1.2 Embedding-Based Models

- Notation:
  - ğ‘¼: A set of all users
  - ğ‘½: A set of all items
  - ğ‘¬: A set of observed user-item interactions
    - ğ‘¬ = {(ğ‘¢, ğ‘£) | ğ‘¢ âˆˆ ğ‘¼, ğ‘£ âˆˆ ğ‘½, ğ‘¢ interacted with ğ‘£}

- Score Function

    To get the top-ğ¾ items, we need a score function for user-item interaction:
    - For ğ‘¢ âˆˆ ğ‘¼, ğ‘£ âˆˆ ğ‘½, we need to get a real-valued scalar score(ğ‘¢, ğ‘£).
    - ğ‘² items with the largest scores for a given user ğ‘¢ (excluding already interacted items) are then recommended.

    <img src="src/L7/7.1.3.png" width="200">   

    For ğ¾ = 2, recommended items for user ğ‘¢ would be {ğ‘£1, ğ‘£3} .

- Embedding-Based Models

    We consider embedding-based models for scoring user-item interactions:
    - For each user ğ‘¢ âˆˆ ğ‘¼, let $ğ’– âˆˆ â„^ğ·$ be its ğ·-dimensional embedding.
    - For each item ğ‘£ âˆˆ ğ‘½, let $ğ’— âˆˆ â„^ğ·$ be its ğ·-dimensional embedding.
    - Let $ğ‘“_ğœƒ(â‹…,â‹…): â„^ğ· Ã— â„^ğ· â†’ â„$ be a parametrized function.
    - Then, $score(u,v)=ğ‘“_ğœƒ(u,v)$

- Training Objective

    Embedding-based models have three kinds of parameters:
    - An encoder to generate user embeddings {ğ’–}$_{ğ‘¢âˆˆğ‘ˆ}$
    - An encoder to generate item embeddings {ğ’—}$_{ğ‘£âˆˆğ‘‰}$ 
    - Score function $ğ‘“_ğœƒ(â‹…,â‹…)$

    Training objective: Optimize the model parameters to achieve high recall@ğ¾ on *seen (i.e., training)* user-item interactions
  
    &rarr; We hope this objective would lead to high recall@ğ¾ on *unseen (i.e., test)* interactions.

- Surrogate Loss Functions

    The original training objective (recall@ğ¾) is **not differentiable**.
    - Cannot apply efficient gradient-based optimization

    Two **surrogate loss functions** are widely-used to enable efficient gradient-based optimization:
    - Binary Loss
    - Bayesian Personalized Ranking (BPR) loss

    Surrogate losses are **differentiable** and should **align well with the original training objective**.

- Binary Loss: Binary classification of **positive/negative** edges using $ğœ(ğ‘“_ğœƒ(ğ’–, ğ’—))$

    Define positive/negative edges:
    - A set of positive edges ğ‘¬ (i.e., observed/training user-item interactions)
    - A set of negative edges $ğ‘¬_{ğ§ğğ } = \{(ğ‘¢, ğ‘£) | (ğ‘¢, ğ‘£) âˆ‰ ğ¸, ğ‘¢ âˆˆ ğ‘¼, ğ‘£ âˆˆ ğ‘½\}$

    Define sigmoid function: <img src="src/L7/7.1.4.png" width="150">
    - Maps real-valued scores into binary likelihood scores, i.e., in the range of [0,1].

    <img src="src/L7/7.1.5.png" width="500"> 

    &rarr; Binary loss pushes the scores of positive edges higher than those of negative edges.
    - This aligns with the training recall metric since positive edges need to be recalled.

- Issue with Binary Loss

    In the binary loss, the scores of **ALL** positive edges are pushed higher than those of **ALL** negative edges.

    This would unnecessarily penalize model predictions even if the training recall metric is perfect.

    Letâ€™s consider the simplest case:
    - Two users, two items
    - Metric: Recall@1.
    - A model assigns the score for every user-item pair (as shown in the right).

    <img src="src/L7/7.1.6.png" width="200">  

    Training Recall@1 is 1.0 (perfect score), because $ğ‘£_0$ (resp. $ğ‘£_1$) is correctly recommended to $ğ‘¢_0$ (resp. $ğ‘¢_1$).

    However, the **binary loss would still penalize the model prediction** because the negative $(ğ‘¢_1 , ğ‘£_0)$ edge gets the higher score than the positive edge $(ğ‘¢_0 , ğ‘£_0)$ .

    - Key insight: The binary loss is **non-personalized** in the sense that the positive/negative edges are **considered across ALL users at once**.
    - However, the recall metric is inherently **personalized (defined for each user)**.

    &rarr; The non-personalized binary loss is overly-stringent
for the personalized recall metric.

    &rarr; Lesson learned: Surrogate loss function should be defined in a **personalized** manner.
    - For each user, we want the scores of positive items to be higher than those of the negative items
    - **We do not care about the score ordering across users**.

    &rarr; **Bayesian Personalized Ranking (BPR)** loss achieves this!

- Bayesian Personalized Ranking (BPR) Loss: 
  - Personalized surrogate loss that aligns better with the recall@K metric.

    Note: The term â€œBayesianâ€ is not essential to the loss definition. The original paper [Rendle et al. 2009] considers the Bayesian prior over parameters (essentially acts as a parameter regularization), which we omit here.

    For each user $ğ‘¢^âˆ— âˆˆ ğ‘¼$, define the **rooted positive/negative edges** as
    - Positive edges rooted at $ğ‘¢^âˆ—$: $ğ‘¬(ğ‘¢^âˆ—) â‰¡ \{(ğ‘¢^âˆ—, ğ‘£) | (ğ‘¢^âˆ—, ğ‘£) âˆˆ ğ‘¬\}$
    - Negative edges rooted at $ğ‘¢^âˆ—$: $ğ‘¬_{neg}(ğ‘¢^âˆ—) â‰¡ \{(ğ‘¢^âˆ—, ğ‘£) | (ğ‘¢^âˆ—, ğ‘£) âˆˆ ğ‘¬_{neg}\}$

  - Training objective: For each user $ğ‘¢^âˆ—$, we want the scores of rooted positive edges $ğ‘¬(ğ‘¢^âˆ—)$ to be higher than those of rooted negative edges $ğ‘¬_{neg}(ğ‘¢^âˆ—)$.
  - BPR Loss for user $ğ‘¢^âˆ—$:

    <img src="src/L7/7.1.7.png" width="600">  

  - Final BPR Loss: <img src="src/L7/7.1.8.png" width="200">  

  - Mini-batch training for the BPR loss:

    In each mini-batch, we sample a subset of users $ğ‘¼_{mini} âŠ‚ ğ‘¼$.
    - For each user $ğ‘¢^âˆ— âˆˆ ğ‘¼_{mini}$ , we sample one positive item $ğ‘£_{pos}$ and a set of sampled negative items $ğ‘‰_{neg} = \{ğ‘£_{neg}\}$ .
  
        <img src="src/L7/7.1.10.png" width="150">  

    The mini-batch loss is computed as

    <img src="src/L7/7.1.9.png" width="500">

- Why Embedding Models Work?

    - Underlying idea: Collaborative filtering
      - Recommend items for a user by collecting preferences of many other similar users.
      - Similar users tend to prefer similar items.

        <img src="src/L7/7.1.11.png" width="300"> 

    - Key question: How to capture similarity between users/items?

    - Embedding-based models can capture similarity of users/items!
      - Low-dimensional embeddings cannot simply memorize all user-item interaction data.
      - Embeddings are forced to **capture similarity between users/items to fit the data**.
      - This allows the models to make effective prediction on unseen user-item interactions.

- This Lecture: GNNs for Recsys

    1. Neural Graph Collab. Filtering (NGCF) [Wang et al. 2019]
    2. LightGCN [He et al. 2020]
        - Improve the conventional collaborative filtering models (i.e., shallow encoders) by explicitly modeling graph structure using GNNs.
        - Assumes no user/item features.
    3. PinSAGE [Ying et al. 2018]
        - Use GNNs to generate high-quality embeddings by simultaneously capturing rich node attributes (e.g., images) and the graph structure.

#### 7.1.3 Neural Graph Collaborative Filtering

- Conventional Collaborative Filtering

    Conventional collaborative filtering model is based on shallow encoders:
    - No user/item features.
    - Use shallow encoders for users and items:
      - For every ğ‘¢ âˆˆ ğ‘¼ and ğ‘£ âˆˆ ğ‘½, we prepare shallow learnable embeddings $ğ’–, ğ’— âˆˆ â„^ğ·$.
    - Score function for user ğ‘¢ and item ğ‘£ is $ğ‘“_ğœƒ(ğ’–, ğ’—) â‰¡ ğ’›_ğ’–^ğ‘‡ğ’›_ğ’—$.

    Limitations of Shallow Encoders:
    - The model itself does **not explicitly capture graph structure**
      - The graph structure is **only implicitly** captured in the training objective.
    - Only the **first-order graph structure** (i.e., edges) is captured in the training objective.
      - **High-order** graph structure (e.g., ğ¾-hop paths between two nodes) is **not explicitly captured**.

- GNN Motivation

    We want a model thatâ€¦
    - **explicitly captures graph structure** (beyond implicitly through the training objective)
    - captures **high-order graph structure** (beyond the first-order edge connectivity structure)

    &rarr; GNNs are a natural approach to achieve both!

    - Neural Graph Collaborative Filtering (NGCF) [Wang et al. 2019]
    - LightGCN [He et al. 2020]
      - A simplified and improved version of NGCF

- NGCF Framework

    Key idea: Use a GNN to generate graph-aware user/item embeddings.

    <img src="src/L7/7.1.12.png" width="500">  

    Given: User-item bipartite graph.
    - Prepare shallow learnable embedding for each node.
    - Use multi-layer GNNs to propagate embeddings along the bipartite graph.
      - High-order graph structure is captured.
    - Final embeddings are explicitly graph-aware!

    Two kinds of learnable params are jointly learned:
    - Shallow user/item embeddings
    - GNNâ€™s parameters

- Initial Node Embeddings

    Set the shallow learnable embeddings as the initial node features:
    - For every user ğ‘¢ âˆˆ ğ‘¼, set $ğ’‰_ğ‘¢^{(0)}$ as the userâ€™s shallow embedding.
    - For every item ğ‘£ âˆˆ ğ‘½, set $ğ’‰_v^{(0)}$ as the itemâ€™s shallow embedding.

- Neighbor Aggregation

  Iteratively update user and item node embeddings using neighboring embeddings.

    $h^{k+1}_v=COMBINE(h^{k}_v,AGGR(\{h^{k}_u \}_{u\isin N(v)}))$

    $h^{k+1}_u=COMBINE(h^{k}_u,AGGR(\{h^{k}_v \}_{v\isin N(u)}))$

    High-order graph structure is captured through iterative neighbor aggregation.

    Different architecture choices are possible for AGGR and COMBINE.
    - AGGR() can be MEAN()
    - COMBINE(x,y) can be RELU(Linear(Concat(x,y)))
  
- Final Embeddings and Score Function

    After ğ¾ rounds of neighbor aggregation, we get the **final user/item embeddings** $h^{k}_u$ and $h^{k}_v$.

    For all ğ‘¢ âˆˆ ğ‘¼, ğ‘£ âˆˆ ğ‘½, we set ğ‘¢ &larr; $h^{k}_u$, ğ‘£ &larr; $h^{k}_v$.

    Score function is the inner product $score(ğ‘¢, ğ‘£) = ğ‘¢^Tğ‘£$

- Summary
  - Conventional collaborative filtering uses shallow user/item embeddings.
    - The embeddings do not explicitly model graph structure.
    - The training objective does not model high-order graph structure.
  - NGCF uses a GNN to propagate the shallow embeddings.
    - The embeddings are explicitly aware of highorder graph structure.


#### 7.1.4 LightGCN

- Motivation

    NGCF jointly learns two kinds of parameters:
    - Shallow user/item embeddings
    - GNNâ€™s parameters

    Observation: Shallow learnable embeddings are already quite expressive.
    - They are learned for every (user/item) node.
    - Most of the parameter counts are in shallow embeddings when ğ‘ (#nodes) â‰« ğ· (embedding dimensionality)
      - Shallow embeddings: ğ‘‚(ğ‘ğ·).
      - GNN: $ğ‘‚(ğ·^2)$.
    - The GNN parameters may not be so essential for performance

    Can we simplify the GNN used in NGCF (e.g., remove its learnable parameters)?
    - Answer: Yes!
    - Bonus: Simplification improves the recommendation performance!

    Overview of the idea:
    - Adjacency matrix for a bipartite graph
    - Matrix formulation of GCN
    - Simplification of GCN by removing non-linearity
    - Related: SGC for scalable GNN [Wu et al. 2019]

- Adjacency and Embedding Matrices

    - Adjacency matrix of a (undirected) bipartite graph.
    - Shallow embedding matrix

    <img src="src/L7/7.1.13.png" width="500">  

- Matrix formulation of GCN

    Recall: The diffusion matrix of C&S

    - Let $D=Diag(d_1, ..., d_N)$ be the degree matrix of ğ‘¨.
    - Define the normalized diffusion matrix as $\tilde{A}=D^{-1/2}AD^{-1/2}$
    - Let $ğ‘¬^{(ğ‘˜)}$ be the embedding matrix at ğ‘˜-th layer.
    - Each layer of GCNâ€™s aggregation can be written in a matrix form: $ğ‘¬^{(ğ‘˜+1)}=ReLU(\tilde{A} ğ‘¬^{(ğ‘˜)}W^{(ğ‘˜)})$
    -  Neighbor aggregation first ($\tilde{A} ğ‘¬^{(ğ‘˜)}$) and then learnable linear transformation ($W^{(ğ‘˜)}$)

    <img src="src/L7/7.1.14.png" width="100"> 

- Simplifying GCN

    - Removing ReLU non-linearity significantly simplifies GCN!
      -  $ğ‘¬^{(ğ‘˜+1)}=\tilde{A} ğ‘¬^{(ğ‘˜)}W^{(ğ‘˜)}$ 
      - Original idea from SGC [Wu et al. 2019]
      - The final node embedding matrix is given as

      <img src="src/L7/7.1.15.png" width="400">

      - Similar to C&S that diffuses soft labels along the graph:

      <img src="src/L7/7.1.16.png" width="400"> 

    - Algorithm: Apply $ğ‘¬ â† \tilde{A} ğ‘¬$ for ğ¾ times.
      - Each matrix multiplication diffuses the current embeddings to their one-hop neighbors.
      - Note: $\tilde{A}^K$ is dense and never gets materialized. Instead, the above iterative matrix-vector product is used to compute $\tilde{A}^KE$.
    - Multi-Scale Diffusion

        Embeddings can be diffused at multiple hop scales:

        $a_0E^{(0)}+a_1E^{(1)}+a_2E^{(2)}+ ... + a_KE^{(K)}$

        - $a_0E^{(0)}=a_0\tilde{A}E^{(0)}$ acts as a self-connection (that is omitted in the definition $\tilde{A}$)
        - The coefficients, $ğ›¼_0 , â€¦ , ğ›¼_ğ¾$ , are hyper-parameters.
        - For simplicity, LightGCN uses the uniform coefficient, i.e., $ğ›¼_ğ¾=1/(K+1)$ for k = 0, ... K.
- LightGCN: Model Overview

    - Given:
      - adjacency matrix A
      - initial learnable embedding matrix E

      Obtain normalized Adj. matrix $\tilde{A}$:

      <img src="src/L7/7.1.20.png" width="400">  

    - Iteratively diffuse embedding matrix ğ‘¬ using $\tilde{A}$:

        For ğ‘˜ = 0 â€¦ ğ¾ âˆ’ 1,

        <img src="src/L7/7.1.17.png" width="400"> 
    - Average the embedding matrices at different scales:

        <img src="src/L7/7.1.18.png" width="400">  

    - Score function: Use user/item vectors from $ğ‘¬_{final}$ to score user-item interaction

        <img src="src/L7/7.1.19.png" width="300">  

- LightGCN: Intuition and Comparison

    - Question: Why does the simple diffusion propagation work well?

        Answer: The diffusion directly encourages the embeddings of similar users/items to be similar.
        - Similar users share many common neighbors (items) and are expected to have similar future preferences (interact with similar items).

    - The embedding propagation of LightGCN is closely related to GCN/C&S.

        Recall: GCN/C&S (neighbor aggregation part)

        <img src="src/L7/7.1.21.png" width="300"> 

        - Self-loop is added in the neighborhood definition.

        LightGCN uses the same equation **except that**
        - Self-loop is not added in the neighborhood definition.
        - Final embedding takes the average of embeddings from all the layers: $h_v = 1/(K+1)\sum^K_{k=0}h^{(k)}_v$
    - LightGCN and MF Comparison

        - Both LightGCN and shallow encoders **learn a unique embedding for each user/item**.
        - The difference is that LightGCN uses the **diffused** user/item embeddings for scoring.
        - LightGCN performs better than shallow encoders but are also more computationally expensive due to the additional diffusion step.
          - The final embedding of a user/item is obtained by aggregating embeddings of its multi-hop neighboring nodes.


    - Summary
        - LightGCN simplifies NGCF by **removing the learnable parameters of GNNs**.
        - **Learnable parameters are all in the shallow input node embeddings**.
          - Diffusion propagation only involves matrix-vector multiplication.
          - The simplification leads to better empirical performance than NGCF.


#### 7.1.4 PinSAGE

- PinSAGE: Pin Embedding
  - Unifies visual, textual, and graph information.
  - The largest industry deployment of a Graph Convolutional Networks: tens of billions of nodes and edges
  - Huge Adoption across Pinterest
  - Works for fresh content and is available in a few seconds after pin creation

- PinSage graph convolutional network:
    - Goal: Generate embeddings for nodes in a large-scale Pinterest graph containing billions of objects
    - Key Idea: Borrow information from nearby nodes
      - E.g., bed rail Pin might look like a garden fence, but gates and beds are rarely adjacent in the graph
      - Pin embeddings are essential to various tasks like Pin recommendations, classification, ranking.
      - Services like â€œRelated Pinsâ€, â€œSearchâ€, â€œShoppingâ€, â€œAdsâ€

    <img src="src/L7/7.1.22.png" width="500"> 

    - Task: Recommend related pins to users

        Learn node embeddings $ğ‘§_ğ‘–$ such that $ğ‘‘(ğ‘§_{ğ‘ğ‘ğ‘˜ğ‘’1}, ğ‘§_{ğ‘ğ‘ğ‘˜ğ‘’2}) < ğ‘‘(ğ‘§_{ğ‘ğ‘ğ‘˜ğ‘’1}, ğ‘§_{sweater})$

    - Training Data: 1+B repin pairs
        - From Related Pins surface
        - Capture semantic relatedness
        - Goal: Embed such pairs to be â€œneighborsâ€

        Example positive training pairs (Q,X): 
        
        <img src="src/L7/7.1.23.png" width="300">


- Methods for Scaling Up

    In addition to the GNN model, the PinSAGE paper introduces several methods to scale the GNN to a billion-scale recommender system (e.g., Pinterest).
    - Shared negative samples across users in a mini-batch

        Recall: In BPR loss, for each user $ğ‘¢_âˆ— âˆˆ ğ‘¼_{mini}$, we sample one positive item $ğ‘£_{pos}$ and a set of sampled negative items $ğ‘½_{neg} = ğ‘£_{neg}$.

        Using more negative samples per user improves the recommendation performance, but is also expensive.
        - We need to generate $|ğ‘¼_{mini}| â‹… |ğ‘½_{neg}|$ embeddings for negative nodes.
        - We need to apply $|ğ‘¼_{mini}| â‹… |ğ‘½_{neg}|$ GNN computational graphs, which is expensive.

        Key idea: We can share the same set of negative samples $ğ‘½_{neg} = ğ‘£_{neg}$ **across all users** $ğ‘¼_{mini}$ in the mini-batch.

        This way, we only need to generate $|ğ‘½_{neg}|$ embeddings for negative nodes.
        
        &rarr; This saves the node embedding generation computation **by a factor of $|ğ‘¼_{mini}|$**! Empirically, the performance stays similar to the non-shared negative sampling scheme.

    - Hard negative samples

        Challenge: Industrial recsys needs to make **extremely fine-grained predictions**.
        - #Total items: Up to billions.
        - #Items to recommend for each user: 10 to 100.

        Issue: The shared negative items are randomly sampled from all items
        - Most of them are â€œ**easy negatives**â€, i.e., a model does not need to be fine-grained to distinguish them from positive items.
        - We need a way to sample â€œ**hard negatives**â€ to force the model to be fine-grained!

        **For each user node**, the hard negatives are item nodes that are close (but not connected) to the user node in the graph

        Hard negatives for user ğ‘¢ âˆˆ ğ‘¼ are obtained as follows:
        - Compute personalized page rank (PPR) for user ğ‘¢.
        - Sort items in the descending order of their PPR scores.
        - Randomly sample item nodes that are ranked high but not too high, e.g., 2000th â€”5000th.
          - Item nodes that are close but not too close (connected) to the user node.
        - The hard negatives for each user are used in addition to the shared negatives.

        Negative Sampling: (q, p) positive pairs are given but various methods to sample negatives to form (q, p, n)
        - Distance Weighted Sampling (Wu et al., 2017): Sample negatives so that query-negative distance distribution is approx U[0.5, 1.4]

        <img src="src/L7/7.1.25.png" width="600">

    - Curriculum learning

        Idea: Include more and more hard negative samples for each epoch

        <img src="src/L7/7.1.24.png" width="400"> 

        Key insight: It is effective to **make the negative samples gradually harder** in the process of training.

        At ğ‘›-th epoch, we add ğ‘› âˆ’ 1 hard negative items.
        - #(Hard negatives) gradually increases in the process of training.

        &rarr; The model will gradually learn to make finergrained predictions.

    - Mini-batch training of GNNs on a large-graph (to be covered in the future lecture)
- PinSAGE Summary

  - PinSAGE uses GNNs to generate high-quality user/item embeddings that **capture both the rich node attributes and graph structure**.
  - The PinSAGE model is effectively trained using sophisticated **negative sampling strategies**.
  - PinSAGE is successfully deployed at Pinterest, a billion-scale image content recommendation service.
  - Uncovered in this lecture: How to scale up GNNs to large-scale graphs. Will be covered in a later lecture.