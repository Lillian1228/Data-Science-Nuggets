{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuXWJLEm2UWS"
      },
      "source": [
        "# **CS224W - Colab 5**\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/scpd-proed/XCS224W-Colab5/blob/main/Notebook/XCS224W_Colab5.ipynb)\n",
        "\n",
        "Before opening the colab with the badge, you would need to allow Google Colab to access the GitHub private repositories. Please check therefore [this tutorial](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb#:~:text=Navigate%20to%20http%3A%2F%2Fcolab,to%20read%20the%20private%20files.).\n",
        "\n",
        "If colab is opened with this badge, make sure please **save copy to drive** in 'File' menu before running the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gzsP50bF6Gb"
      },
      "source": [
        "In this final Colab we will continue experimenting with advanced topics in GNNs. Specifically, we will look at different techniques for scaling up GNNs using PyTorch Geometric, DeepSNAP and NetworkX. In the previous Colab we worked with PyTorch Geometric's `NeighborSampler` to scale up training and testing on the OGB `arxiv` dataset and now we will be using DeepSNAP and NetworkX, to implement our own simplified version of `NeighborSampler` and run experiments with different sampling ratios on the Cora graph.\n",
        "\n",
        "Lastly, we will partition the Cora graph into clusters by using different partition algorithms and then train the models using a vanilla Cluster-GCN.\n",
        "\n",
        "**Note**: Make sure to **sequentially run all the cells in each section**, so that the intermediate variables / packages will carry over to the next cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSaetj53YnT6"
      },
      "source": [
        "# Device\n",
        "You likely will want to us a GPU for this Colab.\n",
        "\n",
        "Please click `Runtime` and then `Change runtime type`. Then set the `hardware accelerator` to **GPU**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCK7krJdp4o8"
      },
      "source": [
        "# Setup\n",
        "As discussed in the first Colabs, the installation of PyG on Colab can be a little bit tricky. First let us check which version of PyTorch you are running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqRrJVjr4akh",
        "outputId": "b548aa98-84be-4b44-934d-881ff69b482a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch\n",
            "Collecting torch==2.5.1+cu124\n",
            "  Downloading https://download.pytorch.org/whl/cu124/torch-2.5.1%2Bcu124-cp311-cp311-linux_x86_64.whl (908.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1+cu124) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1+cu124) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1+cu124) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1+cu124) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1+cu124) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1+cu124)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1+cu124)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1+cu124)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1+cu124)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1+cu124)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1+cu124)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1+cu124)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1+cu124)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1+cu124)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1+cu124) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1+cu124) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1+cu124)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch==2.5.1+cu124)\n",
            "  Downloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1+cu124) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1+cu124) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1+cu124) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.5.1+cu124 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.5.1+cu124 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch-2.5.1+cu124 triton-3.1.0\n"
          ]
        }
      ],
      "source": [
        "# Install PyTorch\n",
        "import os\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "    !pip install torch==2.5.1+cu124 -f https://download.pytorch.org/whl/torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vkP8pA1qBE5",
        "outputId": "93857c6b-4e7e-40f9-b6a9-2c67e3349a88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch has version 2.5.1+cu124\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"PyTorch has version {}\".format(torch.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67gOQITlCNQi"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_m9l6OYCQZP",
        "outputId": "7e427c42-0a14-43ab-8d9d-7f08490a8e4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_scatter-2.1.2%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt25cu124\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_sparse-0.6.18%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt25cu124\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deepsnap (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install torch geometric\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
        "  !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
        "  !pip install torch-geometric\n",
        "  !pip install -q ogb\n",
        "  # Fix for Deepsnap PyG 2.4.x compatibility issue (https://github.com/snap-stanford/deepsnap/issues/53)\n",
        "  !pip install -q git+https://github.com/SebastianHurubaru/deepsnap.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PRfgbfTjCRD_",
        "outputId": "9984cc8e-f3e1-4937-ca89-ad426cd896c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.6.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import torch_geometric\n",
        "torch_geometric.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFb2OAvOSn_O"
      },
      "source": [
        "# 1) Neighbor Sampling with Different Ratios\n",
        "\n",
        "We will implement our own simplified version of Neighbor Sampling using DeepSNAP and NetworkX. Then we will use our sampler to train models with different neighborhood sampling ratios and compare their performance.\n",
        "\n",
        "To make our experiments faster, we will use the Cora graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9U0F7bnSz9u"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUF4on-fSxcq",
        "outputId": "cf539633-d6e8-4e79-ad58-4af134b0a573"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch.nn import Sequential, Linear, ReLU\n",
        "from deepsnap.dataset import GraphDataset\n",
        "from deepsnap.graph import Graph\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  pyg_dataset = Planetoid('./tmp', \"Cora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qw6k-KdFTEYw"
      },
      "source": [
        "## GNN Model\n",
        "\n",
        "We use a simple GraphSage GNN model, which we provide to you below. Similar to in section one, notice the slightly different implementations of the forward method depending on the data `mode`. When `mode = \"batch\"` we use Neighbor sampling. Thus, the data parameter contains our graph's node features (`x`) and a list `edge_indices` containing the connectivity of each GNN layer (i.e. an edge_index for each layer, defining the bipartite neighborhood computation graph).\n",
        "\n",
        "**NOTE:** Refer to sections *Neighbor Sampling* and *PyTorch Geometric Neighbor Sampler* from Colab4 for a detailed overview of the Neighbor Sampling technique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PvUlNi2TS09i"
      },
      "outputs": [],
      "source": [
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, args):\n",
        "        super(GNN, self).__init__()\n",
        "        self.dropout = args['dropout']\n",
        "        self.num_layers = args['num_layers']\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.bns = nn.ModuleList()\n",
        "\n",
        "        self.convs.append(SAGEConv(input_dim, hidden_dim))\n",
        "        self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
        "\n",
        "        for l in range(self.num_layers - 2):\n",
        "            self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
        "            self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
        "        self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
        "\n",
        "        self.post_mp = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data, mode=\"batch\"):\n",
        "        # Observe the difference between mode == \"batch\" and mode == \"all\".\n",
        "        # In mode == \"batch\" we pass in an edge index for each conv layer\n",
        "        # corresponding to that layer's bipartite graph structure.\n",
        "        if mode == \"batch\":\n",
        "            edge_indices, x = data\n",
        "            for i in range(len(self.convs) - 1):\n",
        "                edge_index = edge_indices[i]\n",
        "                x = self.convs[i](x, edge_index)\n",
        "                x = self.bns[i](x)\n",
        "                x = F.relu(x)\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "            x = self.convs[-1](x, edge_indices[len(self.convs) - 1])\n",
        "        else:\n",
        "            x, edge_index = data.node_feature, data.edge_index\n",
        "            for i in range(len(self.convs) - 1):\n",
        "                x = self.convs[i](x, edge_index)\n",
        "                x = self.bns[i](x)\n",
        "                x = F.relu(x)\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "            x = self.convs[-1](x, edge_index)\n",
        "        x = self.post_mp(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ulp1A3evcJ-I"
      },
      "source": [
        "## Implementing Neighbor Sampling\n",
        "\n",
        "Now let's take a stab at implementing our own basic version of Neighbor Sampling using DeepSNAP and NetworkX. To decompose the process, we will define several helper functions before finally defining our own `neighbor_sampling` function!\n",
        "\n",
        "**NOTE:** Before working through this section, we highly recommend reviewing sections `Neighbor Sampling` and `PyTorch Geometric Neighbor Sampler`. Specifically, it is important to understand how we explicitly define an `edge_index` for each GNN layer, representing the bipartite computation graph connecting the `target_nodes` that we are embedding for that layer to their relevant neighbors from the previous layer needed for message passing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWneVr3_Hj4n"
      },
      "source": [
        "## **Question 1.1a**: Implementing the `sample_neighbors` function. (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1TZXvJhHjRK",
        "outputId": "df5d6c69-5de7-4064-bb87-3a9d45ad9624"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index fields: train_mask ignored.\n",
            "Index fields: test_mask ignored.\n",
            "Index fields: val_mask ignored.\n",
            "Neighbors with ratio = 1: {1632, 1090, 1315, 1316, 1093, 970, 2444, 2642, 1271, 24, 927, 2140, 2367}\n",
            "Neighbors with ratio = 0.3: {2642, 1315, 1093}\n",
            "Ratio of sampled neighbors: 0.23076923076923078\n"
          ]
        }
      ],
      "source": [
        "def sample_neighbors(nodes, G, ratio, all_nodes):\n",
        "    # TODO: Implement a function that takes as input a set of nodes,\n",
        "    # a NetworkX graph G, a neighbor sampling ratio, and a set containing all nodes\n",
        "    # and returns:\n",
        "    #   1. A set of the sampled nodes\n",
        "    #   2. A set union between `all_nodes` and the newly sampled neighbor nodes.\n",
        "    #      This allows us to track the nodes needed across all message passing layers.\n",
        "    #   3. The set of edges connecting the sampled neighboring nodes to our input\n",
        "    #      set of nodes. Represents a bi-partite graph between targets (nodes)\n",
        "    #      and source (neighbor) nodes.\n",
        "\n",
        "    neighbors = set()\n",
        "    edges = []\n",
        "\n",
        "    ############# Your code here ############\n",
        "    ## (~8-10 line of code)\n",
        "    ## Note:\n",
        "    ## 1. You will will need to sample neighbors from each node given to you in\n",
        "    ##    `nodes` list.\n",
        "    ##    Hint: Use graph `G` to assist in obtaining the neighbors of each node.\n",
        "    ## 2. The number of neighbors to be sampled based on the `ratio` parameter\n",
        "    ##    must be rounded **down** to get an integer value\n",
        "    ## 3. Randomly sample neighbors without replacement (i.e. the same neighbors\n",
        "    ##    should not be selected more than once for a given node)\n",
        "    ## 4. The neighbors are stored in a set data structure to ensure that duplicates\n",
        "    ##    are avoided.  This is useful as the set union will be taken with `all_nodes`.\n",
        "    ## 5. The edges list should contain all edges sampled in the form of a tuple\n",
        "    ##    of (neighbor, node)\n",
        "    for node in nodes:\n",
        "        all_neighbors = [n for n in G.neighbors(node)]\n",
        "        num_neighbors = int(len(all_neighbors) * ratio)\n",
        "        if num_neighbors>0:\n",
        "          sampled_neighbors = random.sample(all_neighbors, num_neighbors)\n",
        "          for neighbor in sampled_neighbors:\n",
        "              neighbors.add(neighbor)\n",
        "              edges.append((neighbor, node))\n",
        "    ##########################################\n",
        "    return neighbors, neighbors.union(all_nodes), edges\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  graphs_train, _, _ = \\\n",
        "      GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "  graph_train = graphs_train[0]\n",
        "\n",
        "  nodes = [15, 16, 17]\n",
        "  neighbors_full, _, _ = sample_neighbors(nodes, graph_train.G, 1, set())\n",
        "  neighbors_sampled, _, _ = sample_neighbors(nodes, graph_train.G, 0.3, set())\n",
        "  print (\"Neighbors with ratio = 1:\", neighbors_full)\n",
        "  print (\"Neighbors with ratio = 0.3:\", neighbors_sampled)\n",
        "  # Note that this is not expected to be 0.3. Since we apply\n",
        "  # our sampling ratio for each node, the number of neighbors\n",
        "  # for each node may not evenly divide by the ratio\n",
        "  print (\"Ratio of sampled neighbors:\", len(neighbors_sampled) / len(neighbors_full))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NKNh4TEJ8_p"
      },
      "source": [
        "## Tensor transformation and node relabeling helper functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "H2PeBMJIJ9Tn"
      },
      "outputs": [],
      "source": [
        "def nodes_to_tensor(nodes):\n",
        "    \"\"\"\n",
        "      Transforms a set of nodes into a node index tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    node_label_index = torch.tensor(list(nodes), dtype=torch.long)\n",
        "    return node_label_index\n",
        "\n",
        "\n",
        "def edges_to_tensor(edges):\n",
        "    \"\"\"\n",
        "      Transforms a list of undirected edges into the corresponding PyG\n",
        "      edge_index tensor representation. Notice that we explicitly make\n",
        "      sure to include both edge directions.\n",
        "    \"\"\"\n",
        "\n",
        "    edge_index = torch.tensor(list(edges), dtype=torch.long)\n",
        "    edge_index = torch.cat([edge_index, torch.flip(edge_index, [1])], dim=0)\n",
        "    edge_index = edge_index.permute(1, 0)\n",
        "\n",
        "    return edge_index\n",
        "\n",
        "def relabel(nodes, labeled_nodes, edges_list):\n",
        "    \"\"\"\n",
        "      Relabel nodes with 0 based indeces.\n",
        "\n",
        "      During the sampling process, we are likely to sample a list of\n",
        "      non-continuous node ids. However, our GNN models rely on continuous\n",
        "      0 based indexing to index into the rows of our node features matrix\n",
        "      based on edges in the graph (edge_index)\n",
        "    \"\"\"\n",
        "\n",
        "    relabeled_edges_list = []\n",
        "    sorted_nodes = sorted(nodes)\n",
        "    node_mapping = {node : i for i, node in enumerate(sorted_nodes)}\n",
        "    for orig_edges in edges_list:\n",
        "        relabeled_edges = []\n",
        "        for edge in orig_edges:\n",
        "            relabeled_edges.append((node_mapping[edge[0]], node_mapping[edge[1]]))\n",
        "        relabeled_edges_list.append(relabeled_edges)\n",
        "    relabeled_labeled_nodes = [node_mapping[node] for node in labeled_nodes]\n",
        "    relabeled_nodes = [node_mapping[node] for node in nodes]\n",
        "\n",
        "    return relabeled_edges_list, relabeled_nodes, relabeled_labeled_nodes, sorted_nodes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El-2h_ApOJYP"
      },
      "source": [
        "## **Question 1.1b**: Putting it all together - Implementing our own Neighbor Sampling function. (4 points)\n",
        "\n",
        "Now that we've developed a better understanding of what the Neighbor Sampling function does, we will implement our own version of it. Instead of choosing $H_k$ number of samples at each layer, we will use a ratio of the number of neigbors that a givn node has. Can you think of the pros and cons of using a ratio of the number of neighbors for a node at different layers?  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI4qHkE4cQOh",
        "outputId": "ad046f6c-df3c-497a-d3d7-10020cc1b92c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index fields: train_mask ignored.\n",
            "Index fields: test_mask ignored.\n",
            "Index fields: val_mask ignored.\n",
            "Sampled 1888 nodes, 5038 edges, 140 labeled nodes\n"
          ]
        }
      ],
      "source": [
        "def neighbor_sampling(graph, K=2, ratios=(0.1, 0.1, 0.1)):\n",
        "    # TODO: Implement a function that performs Neighbor Sampling on an input\n",
        "    # graph G for a K layer GNN. Notice that len(ratios) = K + 1. Ratios[-1]\n",
        "    # determines size of our mini-batch (i.e. the number of labeled\n",
        "    # nodes we sample computation graphs for).\n",
        "\n",
        "    assert K + 1 == len(ratios)\n",
        "\n",
        "    labeled_nodes = graph.node_label_index.tolist()\n",
        "    random.shuffle(labeled_nodes)\n",
        "    num = int(len(labeled_nodes) * ratios[-1])\n",
        "    if num > 0:\n",
        "        labeled_nodes = labeled_nodes[:num]\n",
        "    nodes_list = [set(labeled_nodes)]\n",
        "    edges_list = []\n",
        "    all_nodes = labeled_nodes\n",
        "\n",
        "    ############# Your code here ############\n",
        "    ## (~4-6 line of code)\n",
        "    ## Note:\n",
        "    ## 1. Using your previously defined `sample_neighbors` function, build up the\n",
        "    ##    edges_list for our K layer network.\n",
        "    ## 2. nodes_list is a list where nodes_list[i] is set of nodes used for message\n",
        "    ##    passing in layer i+1 of our GNN (i.e. level i in our computation graph).\n",
        "    ##    Notice, nodes_list[-1] represents the target nodes we want to\n",
        "    ##    embedd in the mini-batch.\n",
        "    ## 3. edges_list is a list of the bi-partite edge conections between layers\n",
        "    ##    in the computation graph.\n",
        "    ## 4. all_nodes is used to track all the nodes needed for message passing.\n",
        "    ## 5. Remember in a GNN, information flows from the base of the computation\n",
        "    ##    graph to the root. How does this affect the way we add to the nodes_list\n",
        "    ##    and edges_list, as well as how we read from ratios (ratios[-1]\n",
        "    ##    represents the root nodes in our computation graph)?\n",
        "    for i in range(K):\n",
        "        nodes = nodes_list[i]\n",
        "        neighbors, all_nodes, edges = sample_neighbors(nodes, graph.G, ratios[i], all_nodes)\n",
        "        nodes_list.append(neighbors)\n",
        "        edges_list.append(edges)\n",
        "    nodes_list.reverse()\n",
        "    edges_list.reverse()\n",
        "    #########################################\n",
        "\n",
        "    relabeled_edges_list, relabeled_all_nodes, relabeled_labeled_nodes, sorted_original_nodes = \\\n",
        "        relabel(all_nodes, labeled_nodes, edges_list)\n",
        "\n",
        "    node_index = nodes_to_tensor(sorted_original_nodes)\n",
        "    # All node features that will be used\n",
        "    node_feature = graph.node_feature[node_index]\n",
        "    edge_indices = [edges_to_tensor(edges) for edges in relabeled_edges_list]\n",
        "    node_label_index = nodes_to_tensor(relabeled_labeled_nodes)\n",
        "    orig_node_label_index = nodes_to_tensor(labeled_nodes)\n",
        "    log = \"Sampled {} nodes, {} edges, {} labeled nodes\"\n",
        "    print(log.format(node_feature.shape[0], edge_indices[0].shape[1] // 2, node_label_index.shape[0]))\n",
        "    return node_feature, edge_indices, node_label_index, orig_node_label_index\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  # Need to define some basic test! Primarily to test whether they build\n",
        "  # in the correct reverse order. So ideally something like ratio = (0.3, 0.5, 0.8).\n",
        "  # Just need to check shapes.\n",
        "\n",
        "  graphs_train, _, _ = \\\n",
        "      GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "  graph_train = graphs_train[0]\n",
        "\n",
        "  node_feature, edge_indices, node_label_index, _ = neighbor_sampling(graph_train, K=3, ratios=(0.5, 0.8, 1, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooy6Hcf7TIhI"
      },
      "source": [
        "## Training and Testing\n",
        "\n",
        "Additionally, notice that node classification task on Cora is a semi-supervised classification task, here we keep all the labeled training nodes (140 nodes) by setting the last ratio to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iSmZhpzPTGPY"
      },
      "outputs": [],
      "source": [
        "def train(train_graphs, val_graphs, args, model, optimizer, mode=\"batch\"):\n",
        "    best_val = 0\n",
        "    best_model = None\n",
        "    accs = []\n",
        "    graph_train = train_graphs[0]\n",
        "    graph_train.to(args['device'])\n",
        "    for epoch in range(1, 1 + args['epochs']):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        if mode == \"batch\":\n",
        "            node_feature, edge_indices, node_label_index, orig_node_index = neighbor_sampling(graph_train, args['num_layers'], args['ratios'])\n",
        "            node_feature = node_feature.to(args['device'])\n",
        "            node_label_index = node_label_index.to(args['device'])\n",
        "            for i in range(len(edge_indices)):\n",
        "                edge_indices[i] = edge_indices[i].to(args['device'])\n",
        "            pred = model([edge_indices, node_feature])\n",
        "            pred = pred[node_label_index]\n",
        "            label = graph_train.node_label[orig_node_index]\n",
        "        elif mode == \"community\":\n",
        "            graph = random.choice(train_graphs)\n",
        "            graph = graph.to(args['device'])\n",
        "            pred = model(graph, mode=\"all\")\n",
        "            pred = pred[graph.node_label_index]\n",
        "            label = graph.node_label[graph.node_label_index]\n",
        "        else:\n",
        "            pred = model(graph_train, mode=\"all\")\n",
        "            label = graph_train.node_label\n",
        "            pred = pred[graph_train.node_label_index]\n",
        "        loss = F.nll_loss(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_acc, val_acc, test_acc = test(val_graphs, model)\n",
        "        accs.append((train_acc, val_acc, test_acc))\n",
        "        if val_acc > best_val:\n",
        "            best_val = val_acc\n",
        "            best_model = copy.deepcopy(model)\n",
        "        print(f'Epoch: {epoch:02d}, '\n",
        "              f'Loss: {loss:.4f}, '\n",
        "              f'Train: {100 * train_acc:.2f}%, '\n",
        "              f'Valid: {100 * val_acc:.2f}% '\n",
        "              f'Test: {100 * test_acc:.2f}%')\n",
        "    return best_model, accs\n",
        "\n",
        "def test(graphs, model, save_model_results=False, batch_type=\"batch\", title=None):\n",
        "    model.eval()\n",
        "    accs = []\n",
        "\n",
        "    for graph in graphs:\n",
        "        graph = graph.to(args['device'])\n",
        "        pred = model(graph, mode=\"all\")\n",
        "        label = graph.node_label\n",
        "        pred = pred[graph.node_label_index].max(1)[1]\n",
        "        acc = pred.eq(label).sum().item()\n",
        "        acc /= len(label)\n",
        "        accs.append(acc)\n",
        "\n",
        "    if save_model_results:\n",
        "      print (\"Saving Model Predictions for Model:\", batch_type, title)\n",
        "\n",
        "      data = {}\n",
        "      # The last dataset we test is the test graph\n",
        "      data['pred'] = pred.view(-1).cpu().detach().numpy()\n",
        "      data['label'] = label.view(-1).cpu().detach().numpy()\n",
        "\n",
        "      df = pd.DataFrame(data=data)\n",
        "      # Save locally as csv\n",
        "      file_name = 'CORA_Node_' + batch_type\n",
        "      if title is not None:\n",
        "        file_name = file_name + \"_\" + title\n",
        "\n",
        "      df.to_csv(file_name + '.csv', sep=',', index=False)\n",
        "\n",
        "    return accs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HV7i0v0ETKzf"
      },
      "outputs": [],
      "source": [
        "args = {\n",
        "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "    'dropout': 0.5,\n",
        "    'num_layers': 2,\n",
        "    'hidden_size': 64,\n",
        "    'lr': 0.005,\n",
        "    'epochs': 50,\n",
        "    'ratios': (0.8, 0.8, 1),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kfiKId8-iBj1"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=224):\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLpRYKbnTQnj"
      },
      "source": [
        "## Full-Batch Training\n",
        "\n",
        "As a baseline, we train our GNN model over the entire graph without any Neighbor Sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMGGjbJBTOo1",
        "outputId": "d922ab40-1cae-44aa-fdbf-b98ee4290114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index fields: train_mask ignored.\n",
            "Index fields: test_mask ignored.\n",
            "Index fields: val_mask ignored.\n",
            "Epoch: 01, Loss: 2.0228, Train: 65.00%, Valid: 29.20% Test: 35.30%\n",
            "Epoch: 02, Loss: 1.0957, Train: 97.86%, Valid: 54.60% Test: 58.40%\n",
            "Epoch: 03, Loss: 0.6141, Train: 100.00%, Valid: 69.00% Test: 70.10%\n",
            "Epoch: 04, Loss: 0.2814, Train: 100.00%, Valid: 74.20% Test: 74.00%\n",
            "Epoch: 05, Loss: 0.1553, Train: 100.00%, Valid: 75.80% Test: 75.60%\n",
            "Epoch: 06, Loss: 0.0629, Train: 100.00%, Valid: 76.40% Test: 76.10%\n",
            "Epoch: 07, Loss: 0.0454, Train: 100.00%, Valid: 76.60% Test: 75.80%\n",
            "Epoch: 08, Loss: 0.0232, Train: 100.00%, Valid: 77.40% Test: 76.30%\n",
            "Epoch: 09, Loss: 0.0108, Train: 100.00%, Valid: 77.20% Test: 76.30%\n",
            "Epoch: 10, Loss: 0.0120, Train: 100.00%, Valid: 77.20% Test: 76.40%\n",
            "Epoch: 11, Loss: 0.0047, Train: 100.00%, Valid: 76.60% Test: 76.40%\n",
            "Epoch: 12, Loss: 0.0061, Train: 100.00%, Valid: 76.60% Test: 76.20%\n",
            "Epoch: 13, Loss: 0.0046, Train: 100.00%, Valid: 76.20% Test: 76.10%\n",
            "Epoch: 14, Loss: 0.0012, Train: 100.00%, Valid: 76.20% Test: 76.00%\n",
            "Epoch: 15, Loss: 0.0049, Train: 100.00%, Valid: 76.00% Test: 75.90%\n",
            "Epoch: 16, Loss: 0.0019, Train: 100.00%, Valid: 75.40% Test: 75.70%\n",
            "Epoch: 17, Loss: 0.0006, Train: 100.00%, Valid: 75.20% Test: 75.80%\n",
            "Epoch: 18, Loss: 0.0013, Train: 100.00%, Valid: 75.20% Test: 75.90%\n",
            "Epoch: 19, Loss: 0.0003, Train: 100.00%, Valid: 75.20% Test: 75.80%\n",
            "Epoch: 20, Loss: 0.0005, Train: 100.00%, Valid: 75.00% Test: 75.70%\n",
            "Epoch: 21, Loss: 0.0003, Train: 100.00%, Valid: 75.20% Test: 75.50%\n",
            "Epoch: 22, Loss: 0.0001, Train: 100.00%, Valid: 75.20% Test: 75.60%\n",
            "Epoch: 23, Loss: 0.0005, Train: 100.00%, Valid: 74.80% Test: 75.40%\n",
            "Epoch: 24, Loss: 0.0004, Train: 100.00%, Valid: 74.40% Test: 75.20%\n",
            "Epoch: 25, Loss: 0.0001, Train: 100.00%, Valid: 74.60% Test: 75.10%\n",
            "Epoch: 26, Loss: 0.0002, Train: 100.00%, Valid: 74.60% Test: 75.10%\n",
            "Epoch: 27, Loss: 0.0005, Train: 100.00%, Valid: 74.60% Test: 75.20%\n",
            "Epoch: 28, Loss: 0.0002, Train: 100.00%, Valid: 74.80% Test: 75.10%\n",
            "Epoch: 29, Loss: 0.0007, Train: 100.00%, Valid: 75.00% Test: 75.30%\n",
            "Epoch: 30, Loss: 0.0001, Train: 100.00%, Valid: 75.00% Test: 75.30%\n",
            "Epoch: 31, Loss: 0.0007, Train: 100.00%, Valid: 75.00% Test: 75.20%\n",
            "Epoch: 32, Loss: 0.0000, Train: 100.00%, Valid: 75.00% Test: 75.00%\n",
            "Epoch: 33, Loss: 0.0001, Train: 100.00%, Valid: 75.00% Test: 75.00%\n",
            "Epoch: 34, Loss: 0.0000, Train: 100.00%, Valid: 75.00% Test: 75.20%\n",
            "Epoch: 35, Loss: 0.0001, Train: 100.00%, Valid: 75.00% Test: 75.20%\n",
            "Epoch: 36, Loss: 0.0000, Train: 100.00%, Valid: 74.80% Test: 75.30%\n",
            "Epoch: 37, Loss: 0.0000, Train: 100.00%, Valid: 74.60% Test: 75.40%\n",
            "Epoch: 38, Loss: 0.0000, Train: 100.00%, Valid: 74.60% Test: 75.40%\n",
            "Epoch: 39, Loss: 0.0001, Train: 100.00%, Valid: 74.60% Test: 75.40%\n",
            "Epoch: 40, Loss: 0.0000, Train: 100.00%, Valid: 74.60% Test: 75.40%\n",
            "Epoch: 41, Loss: 0.0002, Train: 100.00%, Valid: 74.40% Test: 75.30%\n",
            "Epoch: 42, Loss: 0.0002, Train: 100.00%, Valid: 74.40% Test: 74.90%\n",
            "Epoch: 43, Loss: 0.0000, Train: 100.00%, Valid: 74.60% Test: 75.00%\n",
            "Epoch: 44, Loss: 0.0000, Train: 100.00%, Valid: 74.60% Test: 74.90%\n",
            "Epoch: 45, Loss: 0.0001, Train: 100.00%, Valid: 74.60% Test: 74.90%\n",
            "Epoch: 46, Loss: 0.0000, Train: 100.00%, Valid: 74.60% Test: 74.90%\n",
            "Epoch: 47, Loss: 0.0001, Train: 100.00%, Valid: 74.40% Test: 74.80%\n",
            "Epoch: 48, Loss: 0.0007, Train: 100.00%, Valid: 74.40% Test: 74.90%\n",
            "Epoch: 49, Loss: 0.0001, Train: 100.00%, Valid: 74.40% Test: 74.80%\n",
            "Epoch: 50, Loss: 0.0000, Train: 100.00%, Valid: 74.40% Test: 74.90%\n",
            "Epoch: 51, Loss: 0.0001, Train: 100.00%, Valid: 74.40% Test: 74.90%\n",
            "Epoch: 52, Loss: 0.0001, Train: 100.00%, Valid: 74.40% Test: 75.00%\n",
            "Epoch: 53, Loss: 0.0001, Train: 100.00%, Valid: 74.60% Test: 75.00%\n",
            "Epoch: 54, Loss: 0.0001, Train: 100.00%, Valid: 74.60% Test: 74.90%\n",
            "Epoch: 55, Loss: 0.0005, Train: 100.00%, Valid: 74.60% Test: 74.90%\n",
            "Epoch: 56, Loss: 0.0001, Train: 100.00%, Valid: 74.60% Test: 74.90%\n",
            "Epoch: 57, Loss: 0.0001, Train: 100.00%, Valid: 74.60% Test: 74.90%\n",
            "Epoch: 58, Loss: 0.0001, Train: 100.00%, Valid: 74.60% Test: 75.00%\n",
            "Epoch: 59, Loss: 0.0000, Train: 100.00%, Valid: 74.60% Test: 75.00%\n",
            "Epoch: 60, Loss: 0.0001, Train: 100.00%, Valid: 74.60% Test: 75.00%\n",
            "Epoch: 61, Loss: 0.0002, Train: 100.00%, Valid: 74.60% Test: 75.00%\n",
            "Epoch: 62, Loss: 0.0002, Train: 100.00%, Valid: 74.60% Test: 75.00%\n",
            "Epoch: 63, Loss: 0.0000, Train: 100.00%, Valid: 74.60% Test: 75.00%\n",
            "Epoch: 64, Loss: 0.0001, Train: 100.00%, Valid: 74.60% Test: 75.00%\n",
            "Epoch: 65, Loss: 0.0000, Train: 100.00%, Valid: 74.60% Test: 75.00%\n",
            "Epoch: 66, Loss: 0.0000, Train: 100.00%, Valid: 74.60% Test: 74.90%\n",
            "Epoch: 67, Loss: 0.0001, Train: 100.00%, Valid: 74.60% Test: 75.00%\n",
            "Epoch: 68, Loss: 0.0000, Train: 100.00%, Valid: 74.60% Test: 75.00%\n",
            "Epoch: 69, Loss: 0.0000, Train: 100.00%, Valid: 74.60% Test: 75.10%\n",
            "Epoch: 70, Loss: 0.0001, Train: 100.00%, Valid: 74.60% Test: 75.10%\n",
            "Epoch: 71, Loss: 0.0001, Train: 100.00%, Valid: 74.60% Test: 75.10%\n",
            "Epoch: 72, Loss: 0.0001, Train: 100.00%, Valid: 74.80% Test: 75.10%\n",
            "Epoch: 73, Loss: 0.0000, Train: 100.00%, Valid: 74.80% Test: 75.10%\n",
            "Epoch: 74, Loss: 0.0000, Train: 100.00%, Valid: 74.80% Test: 75.20%\n",
            "Epoch: 75, Loss: 0.0001, Train: 100.00%, Valid: 74.80% Test: 75.20%\n",
            "Epoch: 76, Loss: 0.0001, Train: 100.00%, Valid: 74.80% Test: 75.20%\n",
            "Epoch: 77, Loss: 0.0002, Train: 100.00%, Valid: 74.80% Test: 75.20%\n",
            "Epoch: 78, Loss: 0.0001, Train: 100.00%, Valid: 74.80% Test: 75.20%\n",
            "Epoch: 79, Loss: 0.0000, Train: 100.00%, Valid: 74.80% Test: 75.20%\n",
            "Epoch: 80, Loss: 0.0000, Train: 100.00%, Valid: 74.80% Test: 75.20%\n",
            "Epoch: 81, Loss: 0.0000, Train: 100.00%, Valid: 74.80% Test: 75.20%\n",
            "Epoch: 82, Loss: 0.0000, Train: 100.00%, Valid: 74.80% Test: 75.20%\n",
            "Epoch: 83, Loss: 0.0000, Train: 100.00%, Valid: 74.80% Test: 75.20%\n",
            "Epoch: 84, Loss: 0.0000, Train: 100.00%, Valid: 74.80% Test: 75.20%\n",
            "Epoch: 85, Loss: 0.0000, Train: 100.00%, Valid: 74.80% Test: 75.20%\n",
            "Epoch: 86, Loss: 0.0000, Train: 100.00%, Valid: 75.00% Test: 75.40%\n",
            "Epoch: 87, Loss: 0.0001, Train: 100.00%, Valid: 75.00% Test: 75.40%\n",
            "Epoch: 88, Loss: 0.0001, Train: 100.00%, Valid: 75.00% Test: 75.60%\n",
            "Epoch: 89, Loss: 0.0001, Train: 100.00%, Valid: 74.80% Test: 75.50%\n",
            "Epoch: 90, Loss: 0.0000, Train: 100.00%, Valid: 75.00% Test: 75.50%\n",
            "Epoch: 91, Loss: 0.0000, Train: 100.00%, Valid: 75.00% Test: 75.50%\n",
            "Epoch: 92, Loss: 0.0000, Train: 100.00%, Valid: 75.00% Test: 75.40%\n",
            "Epoch: 93, Loss: 0.0000, Train: 100.00%, Valid: 75.00% Test: 75.40%\n",
            "Epoch: 94, Loss: 0.0000, Train: 100.00%, Valid: 75.00% Test: 75.40%\n",
            "Epoch: 95, Loss: 0.0001, Train: 100.00%, Valid: 75.00% Test: 75.40%\n",
            "Epoch: 96, Loss: 0.0005, Train: 100.00%, Valid: 75.00% Test: 75.30%\n",
            "Epoch: 97, Loss: 0.0001, Train: 100.00%, Valid: 75.00% Test: 75.10%\n",
            "Epoch: 98, Loss: 0.0000, Train: 100.00%, Valid: 75.00% Test: 75.00%\n",
            "Epoch: 99, Loss: 0.0000, Train: 100.00%, Valid: 75.00% Test: 75.00%\n",
            "Epoch: 100, Loss: 0.0000, Train: 100.00%, Valid: 75.00% Test: 75.00%\n",
            "Epoch: 101, Loss: 0.0000, Train: 100.00%, Valid: 75.00% Test: 75.00%\n",
            "Epoch: 102, Loss: 0.0000, Train: 100.00%, Valid: 75.00% Test: 75.10%\n",
            "Epoch: 103, Loss: 0.0000, Train: 100.00%, Valid: 75.00% Test: 75.10%\n",
            "Epoch: 104, Loss: 0.0001, Train: 100.00%, Valid: 74.80% Test: 75.10%\n",
            "Epoch: 105, Loss: 0.0000, Train: 100.00%, Valid: 74.80% Test: 75.10%\n",
            "Epoch: 106, Loss: 0.0000, Train: 100.00%, Valid: 74.80% Test: 75.20%\n",
            "Epoch: 107, Loss: 0.0003, Train: 100.00%, Valid: 74.60% Test: 75.30%\n",
            "Epoch: 108, Loss: 0.0000, Train: 100.00%, Valid: 74.60% Test: 75.20%\n",
            "Epoch: 109, Loss: 0.0000, Train: 100.00%, Valid: 74.60% Test: 75.20%\n",
            "Epoch: 110, Loss: 0.0000, Train: 100.00%, Valid: 74.40% Test: 75.20%\n",
            "Epoch: 111, Loss: 0.0000, Train: 100.00%, Valid: 74.40% Test: 75.10%\n",
            "Epoch: 112, Loss: 0.0000, Train: 100.00%, Valid: 74.40% Test: 75.10%\n",
            "Epoch: 113, Loss: 0.0000, Train: 100.00%, Valid: 74.40% Test: 75.10%\n",
            "Epoch: 114, Loss: 0.0000, Train: 100.00%, Valid: 74.40% Test: 75.10%\n",
            "Epoch: 115, Loss: 0.0000, Train: 100.00%, Valid: 74.40% Test: 75.10%\n",
            "Epoch: 116, Loss: 0.0000, Train: 100.00%, Valid: 74.40% Test: 75.10%\n",
            "Epoch: 117, Loss: 0.0000, Train: 100.00%, Valid: 74.40% Test: 75.10%\n",
            "Epoch: 118, Loss: 0.0001, Train: 100.00%, Valid: 74.40% Test: 75.10%\n",
            "Epoch: 119, Loss: 0.0000, Train: 100.00%, Valid: 74.40% Test: 75.10%\n",
            "Epoch: 120, Loss: 0.0000, Train: 100.00%, Valid: 74.40% Test: 75.10%\n",
            "Epoch: 121, Loss: 0.0008, Train: 100.00%, Valid: 74.40% Test: 75.10%\n",
            "Epoch: 122, Loss: 0.0003, Train: 100.00%, Valid: 74.40% Test: 75.10%\n",
            "Epoch: 123, Loss: 0.0000, Train: 100.00%, Valid: 74.40% Test: 75.10%\n",
            "Epoch: 124, Loss: 0.0000, Train: 100.00%, Valid: 74.40% Test: 75.20%\n",
            "Epoch: 125, Loss: 0.0001, Train: 100.00%, Valid: 74.40% Test: 75.20%\n",
            "Epoch: 126, Loss: 0.0002, Train: 100.00%, Valid: 74.40% Test: 75.20%\n",
            "Epoch: 127, Loss: 0.0000, Train: 100.00%, Valid: 74.40% Test: 75.20%\n",
            "Epoch: 128, Loss: 0.0000, Train: 100.00%, Valid: 74.40% Test: 75.20%\n",
            "Epoch: 129, Loss: 0.0000, Train: 100.00%, Valid: 74.40% Test: 75.30%\n",
            "Epoch: 130, Loss: 0.0000, Train: 100.00%, Valid: 74.40% Test: 75.30%\n",
            "Epoch: 131, Loss: 0.0001, Train: 100.00%, Valid: 74.40% Test: 75.30%\n",
            "Epoch: 132, Loss: 0.0000, Train: 100.00%, Valid: 74.40% Test: 75.30%\n",
            "Epoch: 133, Loss: 0.0001, Train: 100.00%, Valid: 74.40% Test: 75.40%\n",
            "Epoch: 134, Loss: 0.0000, Train: 100.00%, Valid: 74.40% Test: 75.40%\n",
            "Epoch: 135, Loss: 0.0000, Train: 100.00%, Valid: 74.40% Test: 75.40%\n",
            "Epoch: 136, Loss: 0.0028, Train: 100.00%, Valid: 74.40% Test: 75.30%\n",
            "Epoch: 137, Loss: 0.0000, Train: 100.00%, Valid: 74.20% Test: 75.40%\n",
            "Epoch: 138, Loss: 0.0000, Train: 100.00%, Valid: 74.20% Test: 75.20%\n",
            "Epoch: 139, Loss: 0.0000, Train: 100.00%, Valid: 74.20% Test: 75.20%\n",
            "Epoch: 140, Loss: 0.0000, Train: 100.00%, Valid: 74.20% Test: 75.10%\n",
            "Epoch: 141, Loss: 0.0000, Train: 100.00%, Valid: 74.20% Test: 75.20%\n",
            "Epoch: 142, Loss: 0.0000, Train: 100.00%, Valid: 74.20% Test: 75.10%\n",
            "Epoch: 143, Loss: 0.0000, Train: 100.00%, Valid: 74.40% Test: 75.20%\n",
            "Epoch: 144, Loss: 0.0000, Train: 100.00%, Valid: 74.20% Test: 75.00%\n",
            "Epoch: 145, Loss: 0.0000, Train: 100.00%, Valid: 74.00% Test: 74.90%\n",
            "Epoch: 146, Loss: 0.0000, Train: 100.00%, Valid: 74.00% Test: 74.90%\n",
            "Epoch: 147, Loss: 0.0000, Train: 100.00%, Valid: 74.00% Test: 74.90%\n",
            "Epoch: 148, Loss: 0.0000, Train: 100.00%, Valid: 74.00% Test: 74.90%\n",
            "Epoch: 149, Loss: 0.0000, Train: 100.00%, Valid: 74.00% Test: 74.90%\n",
            "Epoch: 150, Loss: 0.0000, Train: 100.00%, Valid: 74.00% Test: 74.90%\n",
            "Best model: Train: 100.00%, Valid: 77.40% Test: 76.30%\n"
          ]
        }
      ],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "\n",
        "  set_seed()\n",
        "\n",
        "  graphs_train, graphs_val, graphs_test = \\\n",
        "      GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "  graph_train = graphs_train[0]\n",
        "  graph_val = graphs_val[0]\n",
        "  graph_test = graphs_test[0]\n",
        "\n",
        "  model = GNN(graph_train.num_node_features, args['hidden_size'], graph_train.num_node_labels, args).to(args['device'])\n",
        "\n",
        "  # Disable compile as this does not seem to work yet in PyTorch 2.0.1/PyG 2.3.1\n",
        "  # try:\n",
        "  #   model = torch_geometric.compile(model)\n",
        "  #   print(f\"GNN Model compiled\")\n",
        "  # except Exception as err:\n",
        "  #   print(f\"Model compile not supported: {err}\")\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "  graphs = [graph_train, graph_val, graph_test]\n",
        "  all_best_model, all_accs = train(graphs, graphs, args, model, optimizer, mode=\"all\")\n",
        "  train_acc, val_acc, test_acc = test([graph_train, graph_val, graph_test], all_best_model)\n",
        "  print('Best model:',\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * val_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWkGiwB6Thr4"
      },
      "source": [
        "## **Question 1.2a:** What is the maximum test accuracy using samping ratios = (0.7, 0.9, 1)? (12 points)\n",
        "\n",
        "Running the cell below will show the results of your best model and save your best model's predictions to a file named CORA_Node_batch_(0.7, 0.9, 1).csv'.\n",
        "\n",
        "As we have seen before you can view this file by clicking on the Folder icon on the left side pannel. When you sumbit your assignment, you will have to download this file and attatch it to your submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWusJ9u3Tfhv",
        "outputId": "a538a10e-d369-46eb-c338-7529c6e46e71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index fields: train_mask ignored.\n",
            "Index fields: test_mask ignored.\n",
            "Index fields: val_mask ignored.\n",
            "Sampled 1347 nodes, 2036 edges, 140 labeled nodes\n",
            "Epoch: 01, Loss: 2.0381, Train: 52.14%, Valid: 24.20% Test: 29.00%\n",
            "Sampled 1284 nodes, 1935 edges, 140 labeled nodes\n",
            "Epoch: 02, Loss: 1.3649, Train: 90.71%, Valid: 49.60% Test: 53.90%\n",
            "Sampled 1307 nodes, 1966 edges, 140 labeled nodes\n",
            "Epoch: 03, Loss: 0.9251, Train: 96.43%, Valid: 62.40% Test: 65.80%\n",
            "Sampled 1306 nodes, 1913 edges, 140 labeled nodes\n",
            "Epoch: 04, Loss: 0.5872, Train: 97.86%, Valid: 67.00% Test: 69.70%\n",
            "Sampled 1304 nodes, 1941 edges, 140 labeled nodes\n",
            "Epoch: 05, Loss: 0.4140, Train: 98.57%, Valid: 70.80% Test: 73.20%\n",
            "Sampled 1347 nodes, 2003 edges, 140 labeled nodes\n",
            "Epoch: 06, Loss: 0.2707, Train: 99.29%, Valid: 72.60% Test: 75.40%\n",
            "Sampled 1276 nodes, 1942 edges, 140 labeled nodes\n",
            "Epoch: 07, Loss: 0.1619, Train: 100.00%, Valid: 72.80% Test: 75.50%\n",
            "Sampled 1330 nodes, 1981 edges, 140 labeled nodes\n",
            "Epoch: 08, Loss: 0.1170, Train: 100.00%, Valid: 73.40% Test: 76.00%\n",
            "Sampled 1344 nodes, 1985 edges, 140 labeled nodes\n",
            "Epoch: 09, Loss: 0.0893, Train: 100.00%, Valid: 73.20% Test: 76.00%\n",
            "Sampled 1343 nodes, 2014 edges, 140 labeled nodes\n",
            "Epoch: 10, Loss: 0.0642, Train: 100.00%, Valid: 73.60% Test: 76.10%\n",
            "Sampled 1333 nodes, 1994 edges, 140 labeled nodes\n",
            "Epoch: 11, Loss: 0.0526, Train: 100.00%, Valid: 74.20% Test: 76.10%\n",
            "Sampled 1344 nodes, 2053 edges, 140 labeled nodes\n",
            "Epoch: 12, Loss: 0.0373, Train: 100.00%, Valid: 73.80% Test: 76.40%\n",
            "Sampled 1314 nodes, 1937 edges, 140 labeled nodes\n",
            "Epoch: 13, Loss: 0.0368, Train: 100.00%, Valid: 74.00% Test: 76.60%\n",
            "Sampled 1314 nodes, 1940 edges, 140 labeled nodes\n",
            "Epoch: 14, Loss: 0.0145, Train: 100.00%, Valid: 74.20% Test: 76.20%\n",
            "Sampled 1260 nodes, 1859 edges, 140 labeled nodes\n",
            "Epoch: 15, Loss: 0.0187, Train: 100.00%, Valid: 74.20% Test: 76.30%\n",
            "Sampled 1326 nodes, 1969 edges, 140 labeled nodes\n",
            "Epoch: 16, Loss: 0.0159, Train: 100.00%, Valid: 74.60% Test: 76.20%\n",
            "Sampled 1287 nodes, 1892 edges, 140 labeled nodes\n",
            "Epoch: 17, Loss: 0.0048, Train: 100.00%, Valid: 74.60% Test: 76.00%\n",
            "Sampled 1273 nodes, 1885 edges, 140 labeled nodes\n",
            "Epoch: 18, Loss: 0.0086, Train: 100.00%, Valid: 74.60% Test: 76.20%\n",
            "Sampled 1269 nodes, 1893 edges, 140 labeled nodes\n",
            "Epoch: 19, Loss: 0.0060, Train: 100.00%, Valid: 74.80% Test: 76.10%\n",
            "Sampled 1366 nodes, 2052 edges, 140 labeled nodes\n",
            "Epoch: 20, Loss: 0.0158, Train: 100.00%, Valid: 74.60% Test: 75.40%\n",
            "Sampled 1330 nodes, 2043 edges, 140 labeled nodes\n",
            "Epoch: 21, Loss: 0.0098, Train: 100.00%, Valid: 74.60% Test: 75.40%\n",
            "Sampled 1353 nodes, 1995 edges, 140 labeled nodes\n",
            "Epoch: 22, Loss: 0.0082, Train: 100.00%, Valid: 74.60% Test: 75.40%\n",
            "Sampled 1356 nodes, 1999 edges, 140 labeled nodes\n",
            "Epoch: 23, Loss: 0.0062, Train: 100.00%, Valid: 74.60% Test: 75.50%\n",
            "Sampled 1329 nodes, 2002 edges, 140 labeled nodes\n",
            "Epoch: 24, Loss: 0.0022, Train: 100.00%, Valid: 74.60% Test: 75.60%\n",
            "Sampled 1292 nodes, 1889 edges, 140 labeled nodes\n",
            "Epoch: 25, Loss: 0.0012, Train: 100.00%, Valid: 74.20% Test: 76.00%\n",
            "Sampled 1289 nodes, 1908 edges, 140 labeled nodes\n",
            "Epoch: 26, Loss: 0.0005, Train: 100.00%, Valid: 74.20% Test: 75.90%\n",
            "Sampled 1287 nodes, 1938 edges, 140 labeled nodes\n",
            "Epoch: 27, Loss: 0.0007, Train: 100.00%, Valid: 74.00% Test: 75.60%\n",
            "Sampled 1329 nodes, 1992 edges, 140 labeled nodes\n",
            "Epoch: 28, Loss: 0.0046, Train: 100.00%, Valid: 74.00% Test: 75.80%\n",
            "Sampled 1333 nodes, 2017 edges, 140 labeled nodes\n",
            "Epoch: 29, Loss: 0.0077, Train: 100.00%, Valid: 74.00% Test: 75.70%\n",
            "Sampled 1330 nodes, 2015 edges, 140 labeled nodes\n",
            "Epoch: 30, Loss: 0.0024, Train: 100.00%, Valid: 74.00% Test: 75.60%\n",
            "Sampled 1335 nodes, 2022 edges, 140 labeled nodes\n",
            "Epoch: 31, Loss: 0.0142, Train: 100.00%, Valid: 74.00% Test: 75.60%\n",
            "Sampled 1316 nodes, 1906 edges, 140 labeled nodes\n",
            "Epoch: 32, Loss: 0.0009, Train: 100.00%, Valid: 74.00% Test: 75.60%\n",
            "Sampled 1280 nodes, 1937 edges, 140 labeled nodes\n",
            "Epoch: 33, Loss: 0.0049, Train: 100.00%, Valid: 74.00% Test: 75.50%\n",
            "Sampled 1276 nodes, 1900 edges, 140 labeled nodes\n",
            "Epoch: 34, Loss: 0.0018, Train: 100.00%, Valid: 73.60% Test: 75.40%\n",
            "Sampled 1320 nodes, 1975 edges, 140 labeled nodes\n",
            "Epoch: 35, Loss: 0.0009, Train: 100.00%, Valid: 73.40% Test: 75.40%\n",
            "Sampled 1275 nodes, 1925 edges, 140 labeled nodes\n",
            "Epoch: 36, Loss: 0.0009, Train: 100.00%, Valid: 73.00% Test: 75.40%\n",
            "Sampled 1313 nodes, 1952 edges, 140 labeled nodes\n",
            "Epoch: 37, Loss: 0.0014, Train: 100.00%, Valid: 73.00% Test: 75.40%\n",
            "Sampled 1304 nodes, 1949 edges, 140 labeled nodes\n",
            "Epoch: 38, Loss: 0.0011, Train: 100.00%, Valid: 72.80% Test: 75.30%\n",
            "Sampled 1278 nodes, 1924 edges, 140 labeled nodes\n",
            "Epoch: 39, Loss: 0.0004, Train: 100.00%, Valid: 72.80% Test: 75.30%\n",
            "Sampled 1256 nodes, 1871 edges, 140 labeled nodes\n",
            "Epoch: 40, Loss: 0.0011, Train: 100.00%, Valid: 72.80% Test: 75.20%\n",
            "Sampled 1272 nodes, 1886 edges, 140 labeled nodes\n",
            "Epoch: 41, Loss: 0.0017, Train: 100.00%, Valid: 73.00% Test: 75.20%\n",
            "Sampled 1330 nodes, 1993 edges, 140 labeled nodes\n",
            "Epoch: 42, Loss: 0.0049, Train: 100.00%, Valid: 73.00% Test: 75.20%\n",
            "Sampled 1335 nodes, 1957 edges, 140 labeled nodes\n",
            "Epoch: 43, Loss: 0.0001, Train: 100.00%, Valid: 73.00% Test: 75.20%\n",
            "Sampled 1329 nodes, 1950 edges, 140 labeled nodes\n",
            "Epoch: 44, Loss: 0.0075, Train: 100.00%, Valid: 72.80% Test: 75.10%\n",
            "Sampled 1292 nodes, 1941 edges, 140 labeled nodes\n",
            "Epoch: 45, Loss: 0.0022, Train: 100.00%, Valid: 72.40% Test: 74.90%\n",
            "Sampled 1333 nodes, 1977 edges, 140 labeled nodes\n",
            "Epoch: 46, Loss: 0.0018, Train: 100.00%, Valid: 72.40% Test: 74.80%\n",
            "Sampled 1294 nodes, 1921 edges, 140 labeled nodes\n",
            "Epoch: 47, Loss: 0.0079, Train: 100.00%, Valid: 72.60% Test: 74.70%\n",
            "Sampled 1290 nodes, 1964 edges, 140 labeled nodes\n",
            "Epoch: 48, Loss: 0.0005, Train: 100.00%, Valid: 73.20% Test: 74.80%\n",
            "Sampled 1312 nodes, 1978 edges, 140 labeled nodes\n",
            "Epoch: 49, Loss: 0.0001, Train: 100.00%, Valid: 73.20% Test: 74.80%\n",
            "Sampled 1335 nodes, 2034 edges, 140 labeled nodes\n",
            "Epoch: 50, Loss: 0.0010, Train: 100.00%, Valid: 73.20% Test: 74.90%\n",
            "Sampled 1329 nodes, 2020 edges, 140 labeled nodes\n",
            "Epoch: 51, Loss: 0.0030, Train: 100.00%, Valid: 73.20% Test: 74.80%\n",
            "Sampled 1359 nodes, 2030 edges, 140 labeled nodes\n",
            "Epoch: 52, Loss: 0.0079, Train: 100.00%, Valid: 73.20% Test: 74.80%\n",
            "Sampled 1318 nodes, 1936 edges, 140 labeled nodes\n",
            "Epoch: 53, Loss: 0.0001, Train: 100.00%, Valid: 73.20% Test: 74.90%\n",
            "Sampled 1256 nodes, 1841 edges, 140 labeled nodes\n",
            "Epoch: 54, Loss: 0.0027, Train: 100.00%, Valid: 73.20% Test: 75.10%\n",
            "Sampled 1305 nodes, 1918 edges, 140 labeled nodes\n",
            "Epoch: 55, Loss: 0.0007, Train: 100.00%, Valid: 73.20% Test: 75.10%\n",
            "Sampled 1263 nodes, 1907 edges, 140 labeled nodes\n",
            "Epoch: 56, Loss: 0.0002, Train: 100.00%, Valid: 73.20% Test: 74.90%\n",
            "Sampled 1297 nodes, 1986 edges, 140 labeled nodes\n",
            "Epoch: 57, Loss: 0.0002, Train: 100.00%, Valid: 73.20% Test: 74.90%\n",
            "Sampled 1293 nodes, 1952 edges, 140 labeled nodes\n",
            "Epoch: 58, Loss: 0.0002, Train: 100.00%, Valid: 73.00% Test: 75.00%\n",
            "Sampled 1342 nodes, 2058 edges, 140 labeled nodes\n",
            "Epoch: 59, Loss: 0.0002, Train: 100.00%, Valid: 73.20% Test: 75.00%\n",
            "Sampled 1345 nodes, 2029 edges, 140 labeled nodes\n",
            "Epoch: 60, Loss: 0.0006, Train: 100.00%, Valid: 73.20% Test: 75.40%\n",
            "Sampled 1315 nodes, 1898 edges, 140 labeled nodes\n",
            "Epoch: 61, Loss: 0.0040, Train: 100.00%, Valid: 73.00% Test: 75.30%\n",
            "Sampled 1298 nodes, 2007 edges, 140 labeled nodes\n",
            "Epoch: 62, Loss: 0.0006, Train: 100.00%, Valid: 73.00% Test: 75.50%\n",
            "Sampled 1315 nodes, 1990 edges, 140 labeled nodes\n",
            "Epoch: 63, Loss: 0.0007, Train: 100.00%, Valid: 72.80% Test: 75.50%\n",
            "Sampled 1308 nodes, 2026 edges, 140 labeled nodes\n",
            "Epoch: 64, Loss: 0.0001, Train: 100.00%, Valid: 72.80% Test: 75.50%\n",
            "Sampled 1335 nodes, 2003 edges, 140 labeled nodes\n",
            "Epoch: 65, Loss: 0.0002, Train: 100.00%, Valid: 72.80% Test: 75.50%\n",
            "Sampled 1354 nodes, 2017 edges, 140 labeled nodes\n",
            "Epoch: 66, Loss: 0.0035, Train: 100.00%, Valid: 73.00% Test: 75.50%\n",
            "Sampled 1348 nodes, 2001 edges, 140 labeled nodes\n",
            "Epoch: 67, Loss: 0.0001, Train: 100.00%, Valid: 72.80% Test: 75.50%\n",
            "Sampled 1309 nodes, 1925 edges, 140 labeled nodes\n",
            "Epoch: 68, Loss: 0.0004, Train: 100.00%, Valid: 72.80% Test: 75.70%\n",
            "Sampled 1295 nodes, 1949 edges, 140 labeled nodes\n",
            "Epoch: 69, Loss: 0.0014, Train: 100.00%, Valid: 72.60% Test: 75.70%\n",
            "Sampled 1276 nodes, 1883 edges, 140 labeled nodes\n",
            "Epoch: 70, Loss: 0.0002, Train: 100.00%, Valid: 72.60% Test: 75.70%\n",
            "Sampled 1310 nodes, 1960 edges, 140 labeled nodes\n",
            "Epoch: 71, Loss: 0.0005, Train: 100.00%, Valid: 72.60% Test: 75.60%\n",
            "Sampled 1291 nodes, 1937 edges, 140 labeled nodes\n",
            "Epoch: 72, Loss: 0.0002, Train: 100.00%, Valid: 72.60% Test: 75.50%\n",
            "Sampled 1346 nodes, 1956 edges, 140 labeled nodes\n",
            "Epoch: 73, Loss: 0.0016, Train: 100.00%, Valid: 72.60% Test: 75.40%\n",
            "Sampled 1342 nodes, 2049 edges, 140 labeled nodes\n",
            "Epoch: 74, Loss: 0.0002, Train: 100.00%, Valid: 72.40% Test: 75.40%\n",
            "Sampled 1284 nodes, 1932 edges, 140 labeled nodes\n",
            "Epoch: 75, Loss: 0.0001, Train: 100.00%, Valid: 72.60% Test: 75.20%\n",
            "Sampled 1252 nodes, 1869 edges, 140 labeled nodes\n",
            "Epoch: 76, Loss: 0.0001, Train: 100.00%, Valid: 72.60% Test: 75.30%\n",
            "Sampled 1315 nodes, 1905 edges, 140 labeled nodes\n",
            "Epoch: 77, Loss: 0.0003, Train: 100.00%, Valid: 72.60% Test: 75.40%\n",
            "Sampled 1330 nodes, 1923 edges, 140 labeled nodes\n",
            "Epoch: 78, Loss: 0.0045, Train: 100.00%, Valid: 72.60% Test: 75.30%\n",
            "Sampled 1312 nodes, 1956 edges, 140 labeled nodes\n",
            "Epoch: 79, Loss: 0.0000, Train: 100.00%, Valid: 72.80% Test: 75.40%\n",
            "Sampled 1323 nodes, 1971 edges, 140 labeled nodes\n",
            "Epoch: 80, Loss: 0.0001, Train: 100.00%, Valid: 72.80% Test: 75.30%\n",
            "Sampled 1259 nodes, 1815 edges, 140 labeled nodes\n",
            "Epoch: 81, Loss: 0.0001, Train: 100.00%, Valid: 73.00% Test: 75.20%\n",
            "Sampled 1309 nodes, 2005 edges, 140 labeled nodes\n",
            "Epoch: 82, Loss: 0.0004, Train: 100.00%, Valid: 73.00% Test: 75.50%\n",
            "Sampled 1336 nodes, 2026 edges, 140 labeled nodes\n",
            "Epoch: 83, Loss: 0.0005, Train: 100.00%, Valid: 73.00% Test: 75.40%\n",
            "Sampled 1340 nodes, 2007 edges, 140 labeled nodes\n",
            "Epoch: 84, Loss: 0.0002, Train: 100.00%, Valid: 73.00% Test: 75.30%\n",
            "Sampled 1298 nodes, 1955 edges, 140 labeled nodes\n",
            "Epoch: 85, Loss: 0.0007, Train: 100.00%, Valid: 72.80% Test: 75.30%\n",
            "Sampled 1266 nodes, 1877 edges, 140 labeled nodes\n",
            "Epoch: 86, Loss: 0.0002, Train: 100.00%, Valid: 72.80% Test: 75.10%\n",
            "Sampled 1278 nodes, 1879 edges, 140 labeled nodes\n",
            "Epoch: 87, Loss: 0.0002, Train: 100.00%, Valid: 72.80% Test: 74.90%\n",
            "Sampled 1297 nodes, 1931 edges, 140 labeled nodes\n",
            "Epoch: 88, Loss: 0.0076, Train: 100.00%, Valid: 73.00% Test: 74.90%\n",
            "Sampled 1335 nodes, 1991 edges, 140 labeled nodes\n",
            "Epoch: 89, Loss: 0.0000, Train: 100.00%, Valid: 73.00% Test: 74.60%\n",
            "Sampled 1279 nodes, 1895 edges, 140 labeled nodes\n",
            "Epoch: 90, Loss: 0.0003, Train: 100.00%, Valid: 72.40% Test: 74.40%\n",
            "Sampled 1270 nodes, 1897 edges, 140 labeled nodes\n",
            "Epoch: 91, Loss: 0.0002, Train: 100.00%, Valid: 72.40% Test: 74.40%\n",
            "Sampled 1323 nodes, 2005 edges, 140 labeled nodes\n",
            "Epoch: 92, Loss: 0.0000, Train: 100.00%, Valid: 72.40% Test: 74.40%\n",
            "Sampled 1337 nodes, 1996 edges, 140 labeled nodes\n",
            "Epoch: 93, Loss: 0.0003, Train: 100.00%, Valid: 72.60% Test: 74.30%\n",
            "Sampled 1312 nodes, 1983 edges, 140 labeled nodes\n",
            "Epoch: 94, Loss: 0.0001, Train: 100.00%, Valid: 72.60% Test: 74.20%\n",
            "Sampled 1357 nodes, 2053 edges, 140 labeled nodes\n",
            "Epoch: 95, Loss: 0.0002, Train: 100.00%, Valid: 72.60% Test: 74.20%\n",
            "Sampled 1291 nodes, 1890 edges, 140 labeled nodes\n",
            "Epoch: 96, Loss: 0.0001, Train: 100.00%, Valid: 72.80% Test: 74.20%\n",
            "Sampled 1237 nodes, 1887 edges, 140 labeled nodes\n",
            "Epoch: 97, Loss: 0.0015, Train: 100.00%, Valid: 72.80% Test: 74.30%\n",
            "Sampled 1277 nodes, 1927 edges, 140 labeled nodes\n",
            "Epoch: 98, Loss: 0.0004, Train: 100.00%, Valid: 72.80% Test: 74.20%\n",
            "Sampled 1283 nodes, 1939 edges, 140 labeled nodes\n",
            "Epoch: 99, Loss: 0.0002, Train: 100.00%, Valid: 72.80% Test: 74.30%\n",
            "Sampled 1306 nodes, 1946 edges, 140 labeled nodes\n",
            "Epoch: 100, Loss: 0.0005, Train: 100.00%, Valid: 72.80% Test: 74.50%\n",
            "Sampled 1303 nodes, 1924 edges, 140 labeled nodes\n",
            "Epoch: 101, Loss: 0.0002, Train: 100.00%, Valid: 72.40% Test: 74.60%\n",
            "Sampled 1280 nodes, 1965 edges, 140 labeled nodes\n",
            "Epoch: 102, Loss: 0.0003, Train: 100.00%, Valid: 72.60% Test: 74.60%\n",
            "Sampled 1312 nodes, 1927 edges, 140 labeled nodes\n",
            "Epoch: 103, Loss: 0.0005, Train: 100.00%, Valid: 72.60% Test: 74.60%\n",
            "Sampled 1310 nodes, 1929 edges, 140 labeled nodes\n",
            "Epoch: 104, Loss: 0.0008, Train: 100.00%, Valid: 72.60% Test: 74.60%\n",
            "Sampled 1260 nodes, 1822 edges, 140 labeled nodes\n",
            "Epoch: 105, Loss: 0.0002, Train: 100.00%, Valid: 72.60% Test: 74.90%\n",
            "Sampled 1366 nodes, 2041 edges, 140 labeled nodes\n",
            "Epoch: 106, Loss: 0.0000, Train: 100.00%, Valid: 72.80% Test: 75.00%\n",
            "Sampled 1356 nodes, 2041 edges, 140 labeled nodes\n",
            "Epoch: 107, Loss: 0.0001, Train: 100.00%, Valid: 73.20% Test: 75.00%\n",
            "Sampled 1325 nodes, 1964 edges, 140 labeled nodes\n",
            "Epoch: 108, Loss: 0.0001, Train: 100.00%, Valid: 73.20% Test: 75.00%\n",
            "Sampled 1326 nodes, 1968 edges, 140 labeled nodes\n",
            "Epoch: 109, Loss: 0.0006, Train: 100.00%, Valid: 73.20% Test: 75.00%\n",
            "Sampled 1258 nodes, 1921 edges, 140 labeled nodes\n",
            "Epoch: 110, Loss: 0.0021, Train: 100.00%, Valid: 73.00% Test: 75.40%\n",
            "Sampled 1299 nodes, 2007 edges, 140 labeled nodes\n",
            "Epoch: 111, Loss: 0.0003, Train: 100.00%, Valid: 73.00% Test: 75.60%\n",
            "Sampled 1267 nodes, 1899 edges, 140 labeled nodes\n",
            "Epoch: 112, Loss: 0.0003, Train: 100.00%, Valid: 72.80% Test: 75.40%\n",
            "Sampled 1318 nodes, 2015 edges, 140 labeled nodes\n",
            "Epoch: 113, Loss: 0.0003, Train: 100.00%, Valid: 72.60% Test: 75.40%\n",
            "Sampled 1344 nodes, 2020 edges, 140 labeled nodes\n",
            "Epoch: 114, Loss: 0.0000, Train: 100.00%, Valid: 72.40% Test: 75.50%\n",
            "Sampled 1298 nodes, 1958 edges, 140 labeled nodes\n",
            "Epoch: 115, Loss: 0.0001, Train: 100.00%, Valid: 72.40% Test: 75.60%\n",
            "Sampled 1314 nodes, 1921 edges, 140 labeled nodes\n",
            "Epoch: 116, Loss: 0.0003, Train: 100.00%, Valid: 72.40% Test: 75.70%\n",
            "Sampled 1303 nodes, 1941 edges, 140 labeled nodes\n",
            "Epoch: 117, Loss: 0.0002, Train: 100.00%, Valid: 72.40% Test: 75.50%\n",
            "Sampled 1262 nodes, 1916 edges, 140 labeled nodes\n",
            "Epoch: 118, Loss: 0.0002, Train: 100.00%, Valid: 72.20% Test: 75.50%\n",
            "Sampled 1321 nodes, 2008 edges, 140 labeled nodes\n",
            "Epoch: 119, Loss: 0.0003, Train: 100.00%, Valid: 72.20% Test: 75.40%\n",
            "Sampled 1307 nodes, 1968 edges, 140 labeled nodes\n",
            "Epoch: 120, Loss: 0.0003, Train: 100.00%, Valid: 72.20% Test: 75.40%\n",
            "Sampled 1292 nodes, 1927 edges, 140 labeled nodes\n",
            "Epoch: 121, Loss: 0.0002, Train: 100.00%, Valid: 72.20% Test: 75.50%\n",
            "Sampled 1329 nodes, 1954 edges, 140 labeled nodes\n",
            "Epoch: 122, Loss: 0.0005, Train: 100.00%, Valid: 72.00% Test: 75.40%\n",
            "Sampled 1276 nodes, 1898 edges, 140 labeled nodes\n",
            "Epoch: 123, Loss: 0.0006, Train: 100.00%, Valid: 71.80% Test: 75.40%\n",
            "Sampled 1310 nodes, 2031 edges, 140 labeled nodes\n",
            "Epoch: 124, Loss: 0.0004, Train: 100.00%, Valid: 71.80% Test: 75.30%\n",
            "Sampled 1331 nodes, 1944 edges, 140 labeled nodes\n",
            "Epoch: 125, Loss: 0.0001, Train: 100.00%, Valid: 71.80% Test: 75.40%\n",
            "Sampled 1278 nodes, 1941 edges, 140 labeled nodes\n",
            "Epoch: 126, Loss: 0.0003, Train: 100.00%, Valid: 71.80% Test: 75.40%\n",
            "Sampled 1228 nodes, 1786 edges, 140 labeled nodes\n",
            "Epoch: 127, Loss: 0.0002, Train: 100.00%, Valid: 71.80% Test: 75.30%\n",
            "Sampled 1295 nodes, 1930 edges, 140 labeled nodes\n",
            "Epoch: 128, Loss: 0.0001, Train: 100.00%, Valid: 71.60% Test: 75.30%\n",
            "Sampled 1300 nodes, 1959 edges, 140 labeled nodes\n",
            "Epoch: 129, Loss: 0.0010, Train: 100.00%, Valid: 71.60% Test: 75.30%\n",
            "Sampled 1308 nodes, 1901 edges, 140 labeled nodes\n",
            "Epoch: 130, Loss: 0.0010, Train: 100.00%, Valid: 71.60% Test: 75.30%\n",
            "Sampled 1268 nodes, 1899 edges, 140 labeled nodes\n",
            "Epoch: 131, Loss: 0.0004, Train: 100.00%, Valid: 71.60% Test: 75.30%\n",
            "Sampled 1342 nodes, 1972 edges, 140 labeled nodes\n",
            "Epoch: 132, Loss: 0.0015, Train: 100.00%, Valid: 71.60% Test: 75.30%\n",
            "Sampled 1278 nodes, 1951 edges, 140 labeled nodes\n",
            "Epoch: 133, Loss: 0.0004, Train: 100.00%, Valid: 71.60% Test: 75.50%\n",
            "Sampled 1264 nodes, 1880 edges, 140 labeled nodes\n",
            "Epoch: 134, Loss: 0.0001, Train: 100.00%, Valid: 71.60% Test: 75.60%\n",
            "Sampled 1321 nodes, 1958 edges, 140 labeled nodes\n",
            "Epoch: 135, Loss: 0.0005, Train: 100.00%, Valid: 71.60% Test: 75.70%\n",
            "Sampled 1304 nodes, 1985 edges, 140 labeled nodes\n",
            "Epoch: 136, Loss: 0.0004, Train: 100.00%, Valid: 71.60% Test: 75.80%\n",
            "Sampled 1308 nodes, 1948 edges, 140 labeled nodes\n",
            "Epoch: 137, Loss: 0.0006, Train: 100.00%, Valid: 71.60% Test: 75.70%\n",
            "Sampled 1294 nodes, 1922 edges, 140 labeled nodes\n",
            "Epoch: 138, Loss: 0.0010, Train: 100.00%, Valid: 71.60% Test: 75.80%\n",
            "Sampled 1335 nodes, 1959 edges, 140 labeled nodes\n",
            "Epoch: 139, Loss: 0.0011, Train: 100.00%, Valid: 71.80% Test: 76.00%\n",
            "Sampled 1300 nodes, 1963 edges, 140 labeled nodes\n",
            "Epoch: 140, Loss: 0.0003, Train: 100.00%, Valid: 71.80% Test: 75.90%\n",
            "Sampled 1325 nodes, 1983 edges, 140 labeled nodes\n",
            "Epoch: 141, Loss: 0.0001, Train: 100.00%, Valid: 71.80% Test: 75.80%\n",
            "Sampled 1307 nodes, 1957 edges, 140 labeled nodes\n",
            "Epoch: 142, Loss: 0.0003, Train: 100.00%, Valid: 71.80% Test: 75.80%\n",
            "Sampled 1295 nodes, 1993 edges, 140 labeled nodes\n",
            "Epoch: 143, Loss: 0.0001, Train: 100.00%, Valid: 71.80% Test: 75.80%\n",
            "Sampled 1310 nodes, 1977 edges, 140 labeled nodes\n",
            "Epoch: 144, Loss: 0.0004, Train: 100.00%, Valid: 71.80% Test: 75.70%\n",
            "Sampled 1308 nodes, 1950 edges, 140 labeled nodes\n",
            "Epoch: 145, Loss: 0.0008, Train: 100.00%, Valid: 71.80% Test: 75.70%\n",
            "Sampled 1312 nodes, 1978 edges, 140 labeled nodes\n",
            "Epoch: 146, Loss: 0.0007, Train: 100.00%, Valid: 72.00% Test: 75.80%\n",
            "Sampled 1344 nodes, 1994 edges, 140 labeled nodes\n",
            "Epoch: 147, Loss: 0.0001, Train: 100.00%, Valid: 72.00% Test: 75.70%\n",
            "Sampled 1319 nodes, 1931 edges, 140 labeled nodes\n",
            "Epoch: 148, Loss: 0.0001, Train: 100.00%, Valid: 72.00% Test: 75.70%\n",
            "Sampled 1294 nodes, 1980 edges, 140 labeled nodes\n",
            "Epoch: 149, Loss: 0.0009, Train: 100.00%, Valid: 72.00% Test: 75.70%\n",
            "Sampled 1368 nodes, 2060 edges, 140 labeled nodes\n",
            "Epoch: 150, Loss: 0.0005, Train: 100.00%, Valid: 72.00% Test: 75.70%\n",
            "Saving Model Predictions for Model: batch (0.7,0.9,1)\n",
            "Best model: Train: 100.00%, Valid: 74.80% Test: 76.10%\n"
          ]
        }
      ],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "\n",
        "  set_seed()\n",
        "\n",
        "  args['ratios'] = (0.7, 0.9, 1)\n",
        "\n",
        "  graphs_train, graphs_val, graphs_test = \\\n",
        "      GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "  graph_train = graphs_train[0]\n",
        "  graph_val = graphs_val[0]\n",
        "  graph_test = graphs_test[0]\n",
        "\n",
        "  model = GNN(graph_train.num_node_features, args['hidden_size'], graph_train.num_node_labels, args).to(args['device'])\n",
        "\n",
        "  # Disable compile as this does not seem to work yet in PyTorch 2.0.1/PyG 2.3.1\n",
        "  # try:\n",
        "  #   model = torch_geometric.compile(model)\n",
        "  #   print(f\"GNN Model compiled\")\n",
        "  # except Exception as err:\n",
        "  #   print(f\"Model compile not supported: {err}\")\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "  graphs = [graph_train, graph_val, graph_test]\n",
        "  batch_best_model, batch_accs = train(graphs, graphs, args, model, optimizer)\n",
        "  train_acc, val_acc, test_acc = test([graph_train, graph_val, graph_test], batch_best_model, save_model_results=True, batch_type=\"batch\", title=\"(0.7,0.9,1)\")\n",
        "  print('Best model:',\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * val_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_FjkNHDT4c6"
      },
      "source": [
        "## **Question 1.2b:** What is the maximum test accuracy using samping ratios = (0.3, 0.5, 1)? (12 points)\n",
        "\n",
        "Running the cell below will show the results of your best model and save your best model's predictions to a file named CORA_Node_batch_(0.3, 0.5, 1).csv'.\n",
        "\n",
        "As we have seen before you can view this file by clicking on the Folder icon on the left side pannel. When you submit your assignment, you will have to download this file and attach it to your submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "booJ6DASTjO4",
        "outputId": "cbcabf66-9b82-4636-dfdc-64e86cd70435"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index fields: train_mask ignored.\n",
            "Index fields: test_mask ignored.\n",
            "Index fields: val_mask ignored.\n",
            "Sampled 527 nodes, 409 edges, 140 labeled nodes\n",
            "Epoch: 01, Loss: 2.0055, Train: 38.57%, Valid: 15.00% Test: 20.40%\n",
            "Sampled 568 nodes, 455 edges, 140 labeled nodes\n",
            "Epoch: 02, Loss: 1.6164, Train: 76.43%, Valid: 35.20% Test: 38.30%\n",
            "Sampled 544 nodes, 439 edges, 140 labeled nodes\n",
            "Epoch: 03, Loss: 1.2620, Train: 88.57%, Valid: 46.80% Test: 48.80%\n",
            "Sampled 557 nodes, 451 edges, 140 labeled nodes\n",
            "Epoch: 04, Loss: 1.0145, Train: 90.71%, Valid: 52.60% Test: 54.70%\n",
            "Sampled 554 nodes, 460 edges, 140 labeled nodes\n",
            "Epoch: 05, Loss: 0.8165, Train: 94.29%, Valid: 56.60% Test: 58.50%\n",
            "Sampled 599 nodes, 517 edges, 140 labeled nodes\n",
            "Epoch: 06, Loss: 0.6137, Train: 97.14%, Valid: 59.60% Test: 62.90%\n",
            "Sampled 562 nodes, 454 edges, 140 labeled nodes\n",
            "Epoch: 07, Loss: 0.4521, Train: 97.86%, Valid: 62.80% Test: 64.80%\n",
            "Sampled 592 nodes, 499 edges, 140 labeled nodes\n",
            "Epoch: 08, Loss: 0.3187, Train: 99.29%, Valid: 62.80% Test: 65.80%\n",
            "Sampled 553 nodes, 474 edges, 140 labeled nodes\n",
            "Epoch: 09, Loss: 0.2385, Train: 99.29%, Valid: 64.80% Test: 67.20%\n",
            "Sampled 559 nodes, 471 edges, 140 labeled nodes\n",
            "Epoch: 10, Loss: 0.1560, Train: 99.29%, Valid: 66.80% Test: 67.70%\n",
            "Sampled 592 nodes, 509 edges, 140 labeled nodes\n",
            "Epoch: 11, Loss: 0.1463, Train: 99.29%, Valid: 67.20% Test: 68.30%\n",
            "Sampled 568 nodes, 453 edges, 140 labeled nodes\n",
            "Epoch: 12, Loss: 0.0738, Train: 99.29%, Valid: 67.60% Test: 69.00%\n",
            "Sampled 594 nodes, 486 edges, 140 labeled nodes\n",
            "Epoch: 13, Loss: 0.0867, Train: 99.29%, Valid: 67.80% Test: 69.40%\n",
            "Sampled 556 nodes, 465 edges, 140 labeled nodes\n",
            "Epoch: 14, Loss: 0.0733, Train: 100.00%, Valid: 68.20% Test: 69.70%\n",
            "Sampled 620 nodes, 550 edges, 140 labeled nodes\n",
            "Epoch: 15, Loss: 0.0720, Train: 100.00%, Valid: 68.20% Test: 70.20%\n",
            "Sampled 573 nodes, 493 edges, 140 labeled nodes\n",
            "Epoch: 16, Loss: 0.0307, Train: 100.00%, Valid: 68.80% Test: 70.80%\n",
            "Sampled 529 nodes, 437 edges, 140 labeled nodes\n",
            "Epoch: 17, Loss: 0.0415, Train: 100.00%, Valid: 69.20% Test: 71.00%\n",
            "Sampled 565 nodes, 458 edges, 140 labeled nodes\n",
            "Epoch: 18, Loss: 0.0267, Train: 100.00%, Valid: 69.20% Test: 71.30%\n",
            "Sampled 545 nodes, 433 edges, 140 labeled nodes\n",
            "Epoch: 19, Loss: 0.0214, Train: 100.00%, Valid: 68.80% Test: 71.30%\n",
            "Sampled 569 nodes, 469 edges, 140 labeled nodes\n",
            "Epoch: 20, Loss: 0.0202, Train: 100.00%, Valid: 69.00% Test: 71.40%\n",
            "Sampled 553 nodes, 462 edges, 140 labeled nodes\n",
            "Epoch: 21, Loss: 0.0249, Train: 100.00%, Valid: 69.20% Test: 71.40%\n",
            "Sampled 476 nodes, 359 edges, 140 labeled nodes\n",
            "Epoch: 22, Loss: 0.0288, Train: 100.00%, Valid: 69.00% Test: 71.40%\n",
            "Sampled 557 nodes, 470 edges, 140 labeled nodes\n",
            "Epoch: 23, Loss: 0.0212, Train: 100.00%, Valid: 69.40% Test: 71.30%\n",
            "Sampled 594 nodes, 523 edges, 140 labeled nodes\n",
            "Epoch: 24, Loss: 0.0148, Train: 100.00%, Valid: 69.20% Test: 71.20%\n",
            "Sampled 570 nodes, 469 edges, 140 labeled nodes\n",
            "Epoch: 25, Loss: 0.0100, Train: 100.00%, Valid: 69.20% Test: 71.20%\n",
            "Sampled 563 nodes, 438 edges, 140 labeled nodes\n",
            "Epoch: 26, Loss: 0.0093, Train: 100.00%, Valid: 68.80% Test: 71.00%\n",
            "Sampled 515 nodes, 415 edges, 140 labeled nodes\n",
            "Epoch: 27, Loss: 0.0111, Train: 100.00%, Valid: 68.80% Test: 70.40%\n",
            "Sampled 553 nodes, 455 edges, 140 labeled nodes\n",
            "Epoch: 28, Loss: 0.0214, Train: 100.00%, Valid: 68.60% Test: 70.40%\n",
            "Sampled 538 nodes, 420 edges, 140 labeled nodes\n",
            "Epoch: 29, Loss: 0.0075, Train: 100.00%, Valid: 68.80% Test: 70.20%\n",
            "Sampled 563 nodes, 446 edges, 140 labeled nodes\n",
            "Epoch: 30, Loss: 0.0101, Train: 100.00%, Valid: 68.60% Test: 70.30%\n",
            "Sampled 603 nodes, 540 edges, 140 labeled nodes\n",
            "Epoch: 31, Loss: 0.0121, Train: 100.00%, Valid: 68.80% Test: 70.40%\n",
            "Sampled 551 nodes, 477 edges, 140 labeled nodes\n",
            "Epoch: 32, Loss: 0.0076, Train: 100.00%, Valid: 68.60% Test: 70.50%\n",
            "Sampled 552 nodes, 458 edges, 140 labeled nodes\n",
            "Epoch: 33, Loss: 0.0031, Train: 100.00%, Valid: 68.20% Test: 70.50%\n",
            "Sampled 503 nodes, 418 edges, 140 labeled nodes\n",
            "Epoch: 34, Loss: 0.0232, Train: 100.00%, Valid: 69.20% Test: 70.80%\n",
            "Sampled 529 nodes, 426 edges, 140 labeled nodes\n",
            "Epoch: 35, Loss: 0.0028, Train: 100.00%, Valid: 69.40% Test: 70.90%\n",
            "Sampled 543 nodes, 440 edges, 140 labeled nodes\n",
            "Epoch: 36, Loss: 0.0015, Train: 100.00%, Valid: 69.00% Test: 70.90%\n",
            "Sampled 505 nodes, 392 edges, 140 labeled nodes\n",
            "Epoch: 37, Loss: 0.0084, Train: 100.00%, Valid: 69.00% Test: 71.10%\n",
            "Sampled 576 nodes, 483 edges, 140 labeled nodes\n",
            "Epoch: 38, Loss: 0.0019, Train: 100.00%, Valid: 69.00% Test: 71.30%\n",
            "Sampled 593 nodes, 505 edges, 140 labeled nodes\n",
            "Epoch: 39, Loss: 0.0031, Train: 100.00%, Valid: 69.20% Test: 71.30%\n",
            "Sampled 560 nodes, 469 edges, 140 labeled nodes\n",
            "Epoch: 40, Loss: 0.0020, Train: 100.00%, Valid: 69.80% Test: 71.40%\n",
            "Sampled 492 nodes, 371 edges, 140 labeled nodes\n",
            "Epoch: 41, Loss: 0.0059, Train: 100.00%, Valid: 69.40% Test: 71.30%\n",
            "Sampled 515 nodes, 428 edges, 140 labeled nodes\n",
            "Epoch: 42, Loss: 0.0161, Train: 100.00%, Valid: 69.20% Test: 71.30%\n",
            "Sampled 534 nodes, 429 edges, 140 labeled nodes\n",
            "Epoch: 43, Loss: 0.0064, Train: 100.00%, Valid: 69.00% Test: 71.60%\n",
            "Sampled 605 nodes, 534 edges, 140 labeled nodes\n",
            "Epoch: 44, Loss: 0.0024, Train: 100.00%, Valid: 69.80% Test: 71.70%\n",
            "Sampled 588 nodes, 487 edges, 140 labeled nodes\n",
            "Epoch: 45, Loss: 0.0286, Train: 100.00%, Valid: 69.00% Test: 71.50%\n",
            "Sampled 543 nodes, 446 edges, 140 labeled nodes\n",
            "Epoch: 46, Loss: 0.0015, Train: 100.00%, Valid: 69.00% Test: 71.40%\n",
            "Sampled 496 nodes, 374 edges, 140 labeled nodes\n",
            "Epoch: 47, Loss: 0.0030, Train: 100.00%, Valid: 68.80% Test: 71.40%\n",
            "Sampled 567 nodes, 453 edges, 140 labeled nodes\n",
            "Epoch: 48, Loss: 0.0007, Train: 100.00%, Valid: 68.80% Test: 71.20%\n",
            "Sampled 577 nodes, 491 edges, 140 labeled nodes\n",
            "Epoch: 49, Loss: 0.0020, Train: 100.00%, Valid: 68.60% Test: 71.10%\n",
            "Sampled 585 nodes, 483 edges, 140 labeled nodes\n",
            "Epoch: 50, Loss: 0.0150, Train: 100.00%, Valid: 68.20% Test: 70.90%\n",
            "Saving Model Predictions for Model: batch (0.3,0.5,1)\n",
            "Best model: Train: 100.00%, Valid: 69.80% Test: 71.40%\n"
          ]
        }
      ],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "\n",
        "  set_seed()\n",
        "\n",
        "  # Change the ratio to 0.3\n",
        "  args['ratios'] = (0.3, 0.5, 1)\n",
        "\n",
        "  graphs_train, graphs_val, graphs_test = \\\n",
        "      GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "  graph_train = graphs_train[0]\n",
        "  graph_val = graphs_val[0]\n",
        "  graph_test = graphs_test[0]\n",
        "\n",
        "  model = GNN(graph_train.num_node_features, args['hidden_size'], graph_train.num_node_labels, args).to(args['device'])\n",
        "\n",
        "  # Disable compile as this does not seem to work yet in PyTorch 2.0.1/PyG 2.3.1\n",
        "  # try:\n",
        "  #   model = torch_geometric.compile(model)\n",
        "  #   print(f\"GNN Model compiled\")\n",
        "  # except Exception as err:\n",
        "  #   print(f\"Model compile not supported: {err}\")\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "  graphs = [graph_train, graph_val, graph_test]\n",
        "  batch_best_model, batch_accs_1 = train(graphs, graphs, args, model, optimizer)\n",
        "  train_acc, val_acc, test_acc = test([graph_train, graph_val, graph_test], batch_best_model, save_model_results=True, batch_type=\"batch\", title=\"(0.3,0.5,1)\")\n",
        "  print('Best model:',\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * val_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EePAvNlGUM2K"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "7etNAkXAT55d",
        "outputId": "fa4517f1-befd-4721-934d-55aea708d17c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'all_accs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-4093230328>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mbatch_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_accs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mbatch_results_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_accs_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mall_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_accs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m51\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'all_accs' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  batch_results = np.array(batch_accs)\n",
        "  batch_results_1 = np.array(batch_accs_1)\n",
        "  all_results = np.array(all_accs)\n",
        "\n",
        "  x = np.arange(1, 51)\n",
        "\n",
        "  plt.figure(figsize=(9, 7))\n",
        "\n",
        "  plt.plot(x, batch_results[:, 0], label=\"Batch 0.8 Train\")\n",
        "  plt.plot(x, batch_results[:, 1], label=\"Batch 0.8 Validation\")\n",
        "  plt.plot(x, batch_results[:, 2], label=\"Batch 0.8 Test\")\n",
        "  plt.plot(x, batch_results_1[:, 0], label=\"Batch 0.3 Train\")\n",
        "  plt.plot(x, batch_results_1[:, 1], label=\"Batch 0.3 Validation\")\n",
        "  plt.plot(x, batch_results_1[:, 2], label=\"Batch 0.3 Test\")\n",
        "  plt.plot(x, all_results[:, 0], label=\"All Train\")\n",
        "  plt.plot(x, all_results[:, 1], label=\"All Validation\")\n",
        "  plt.plot(x, all_results[:, 2], label=\"All Test\")\n",
        "  plt.title('Model Accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkA7-0groq7q"
      },
      "source": [
        "**NOTE:** We always evaluate accuracy in full-batch mode. Namely, only during training do we use neighborhood sub-sampling. We do this for a couple reason: 1) fairness of comparison, 2) we worry most about computational cost during training as compared to evaluation, and 3) during the inference phase we want to leverage as much neighborhood information as possible!  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iee0U8KGURc8"
      },
      "source": [
        "# 2) Cluster Sampling\n",
        "\n",
        "As discussed in Module 7.2, we can also use subgraph (cluster) sampling to scale up GNN. Specifically, we will explore the methods proposed in Cluster-GCN ([Chiang et al. (2019)](https://arxiv.org/abs/1905.07953)), where we break our graph into smaller subgraphs to avoid the computational cost of training on the entire graph at once.\n",
        "\n",
        "In this final section, we will implement a vanilla Cluster-GCN and experiment with 3 different community partition algorithms.\n",
        "\n",
        "**NOTE:** the code in this section requires that you have run the `Setup`, `GNN Model` and `Training and Testing` cells of section 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BXjP79gUYir"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UGQ_VKp8UOEm"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "import community.community_louvain as community_louvain\n",
        "\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch.nn import Sequential, Linear, ReLU\n",
        "from deepsnap.dataset import GraphDataset\n",
        "from deepsnap.graph import Graph\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  pyg_dataset = Planetoid('./tmp', \"Cora\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "bzMatyCSUaB6"
      },
      "outputs": [],
      "source": [
        "args = {\n",
        "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "    'dropout': 0.5,\n",
        "    'num_layers': 2,\n",
        "    'hidden_size': 64,\n",
        "    'lr': 0.005,\n",
        "    'epochs': 150,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekV-sokSUeLc"
      },
      "source": [
        "## Partitioning the Graph into Clusters\n",
        "\n",
        "We will experiment with three community detection / partition algorithms to partition our graph into different clusters:\n",
        "* [Kernighan–Lin algorithm (bisection)](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.kernighan_lin.kernighan_lin_bisection.html)\n",
        "* [Clauset-Newman-Moore greedy modularity maximization](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.modularity_max.greedy_modularity_communities.html#networkx.algorithms.community.modularity_max.greedy_modularity_communities)\n",
        "* [Louvain algorithm](https://python-louvain.readthedocs.io/en/latest/api.html)\n",
        "\n",
        "\n",
        "As a preprocessing step, we partition our graph into a list of seperate subgraphs using one of the three communitiy detection algorithms above. Then during training we iteratively train our vanilla Cluster-GNN model on a randomly selected subgraph, rather than on over the entire graph at once. To make training more stable, we discard any communities that have less than 10 nodes.\n",
        "\n",
        "Let's begin by defining the three partition algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "N8XeT005UcKh"
      },
      "outputs": [],
      "source": [
        "def partition(G, method=\"louvain\"):\n",
        "    # TODO: Implement a function that takes a Networkx graph G and\n",
        "    # partitions the graph into communities using the specified graph\n",
        "    # partition algorithm.\n",
        "    #\n",
        "    # Return: A list of sets of nodes, one for each community!\n",
        "\n",
        "    communities = None\n",
        "\n",
        "    if method == \"louvain\":\n",
        "        ############# Your code here #############\n",
        "        ## (~9 line of code)\n",
        "        ## Note:\n",
        "        ## 1. Find a community mapping corresponding to the partition of the\n",
        "        ##    graph nodes which maximizes the modularity for the Louvain algorithm.\n",
        "        ##    Set your resolution to 10.\n",
        "        ## 2. Create a mapping of communities to a set of member nodes.\n",
        "        ## 3. Extract the node sets from each community and return\n",
        "        ##    as a list of sets.\n",
        "        ##    Hint: Perhaps a dictionary structure can assist.\n",
        "        ## 4. SET random_state = 8\n",
        "        nodes_map = community_louvain.best_partition(G, resolution=10, random_state=8) # dict w nodes as keys and community as values\n",
        "        communities_map = {}\n",
        "        for community in set(nodes_map.values()):\n",
        "          communities_map[community]=set()\n",
        "        for node in nodes_map:\n",
        "          communities_map[nodes_map[node]].add(node)\n",
        "        # print(\"Communities: \", len(communities_map))\n",
        "        # print(communities_map)\n",
        "        communities = list(communities_map.values())\n",
        "        ##########################################\n",
        "    elif method == \"bisection\":\n",
        "        ############# Your code here #############\n",
        "        ## (~1 line of code)\n",
        "        ## Note:\n",
        "        ## 1. The Kernigan-Lin algorithm ensures that nodes are partitioned into two\n",
        "        ##    primary communities.\n",
        "        ## 2. Ensure that the resultant data structure is consistent with expected\n",
        "        ##    output.\n",
        "        ## 3. SET seed = 8\n",
        "        communities = nx.algorithms.community.kernighan_lin_bisection(G, seed=8)\n",
        "        ##########################################\n",
        "    elif method == \"greedy\":\n",
        "        ############# Your code here #############\n",
        "        ## (~1 line of code)\n",
        "        ## Note:\n",
        "        ## 1. Clauset-Newman-Moore greedy modularity maximization joins pair\n",
        "        ##    of communities nodes that most increases modularity until no such\n",
        "        ##    pair exists.\n",
        "        communities = nx.community.greedy_modularity_communities(G)\n",
        "        ##########################################\n",
        "\n",
        "    return communities\n",
        "\n",
        "\n",
        "def preprocess(G, node_label_index, method=\"louvain\"):\n",
        "    graphs = []\n",
        "    labeled_nodes = set(node_label_index.tolist())\n",
        "    communities = partition(G, method)\n",
        "    for community in communities:\n",
        "        nodes = set(community)\n",
        "        subgraph = G.subgraph(nodes)\n",
        "        # Make sure each subgraph has more than 10 nodes\n",
        "        if subgraph.number_of_nodes() > 10:\n",
        "            node_mapping = {node : i for i, node in enumerate(subgraph.nodes())}\n",
        "            subgraph = nx.relabel_nodes(subgraph, node_mapping)\n",
        "            # Get the id of the training set labeled node in the new graph\n",
        "            train_label_index = []\n",
        "            for node in labeled_nodes:\n",
        "                if node in node_mapping:\n",
        "                    # Append relabeled labeled node index\n",
        "                    train_label_index.append(node_mapping[node])\n",
        "\n",
        "            # Make sure the subgraph contains at least one training set labeled node\n",
        "            if len(train_label_index) > 0:\n",
        "                dg = Graph(subgraph)\n",
        "                # Update node_label_index\n",
        "                dg.node_label_index = torch.tensor(train_label_index, dtype=torch.long)\n",
        "                graphs.append(dg)\n",
        "    return graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKrUdkhC1-A3"
      },
      "source": [
        "## Experimenting with different graph partition algorithms\n",
        "\n",
        "We will now experiment with the three graph partition algorithms, using the resulting graph clusters to train our vanilla Cluster-GNN. We will first observe how each parition algorithm partitions the original graph. Then we will compare the perfomance of our vanilla Cluster-GNN trained using the different graph clustering techniques. Lastly, we will compare against training a vanilla GCN over the entire graph (refered to as Full-Batch training).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CYEamCAU-TJ"
      },
      "source": [
        "## **Question 2.1a:** How does the Louvain algorithm partition our graph? (3 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TrC6ybWU7eO",
        "outputId": "a1bd6184-e24f-42b9-f0fb-2ec50d34849a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index fields: train_mask ignored.\n",
            "Index fields: test_mask ignored.\n",
            "Index fields: val_mask ignored.\n",
            "\n",
            "Partitioning the graph in to 77 communities\n",
            "Each community has 21 nodes in average\n",
            "Each community has 30 edges in average\n"
          ]
        }
      ],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "\n",
        "  set_seed()\n",
        "\n",
        "  graphs_train, graphs_val, graphs_test = \\\n",
        "      GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "  graph_train = graphs_train[0]\n",
        "  graph_val = graphs_val[0]\n",
        "  graph_test = graphs_test[0]\n",
        "  graphs = preprocess(graph_train.G, graph_train.node_label_index, method=\"louvain\")\n",
        "  print()\n",
        "  print(\"Partitioning the graph in to {} communities\".format(len(graphs)))\n",
        "  avg_num_nodes = 0\n",
        "  avg_num_edges = 0\n",
        "  for graph in graphs:\n",
        "      avg_num_nodes += graph.num_nodes\n",
        "      avg_num_edges += graph.num_edges\n",
        "  avg_num_nodes = int(avg_num_nodes / len(graphs))\n",
        "  avg_num_edges = int(avg_num_edges / len(graphs))\n",
        "  print(\"Each community has {} nodes in average\".format(avg_num_nodes))\n",
        "  print(\"Each community has {} edges in average\".format(avg_num_edges))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O03uXIuGVIgJ"
      },
      "source": [
        "## **Question 2.1b:** Using Louvain clustering, what is the maximum test accuracy obtained by your vanilla Cluster-GCN? (6 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSbGf5ADVFQq",
        "outputId": "1d3792b7-3914-479a-a430-e36723396be4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 2.2475, Train: 13.57%, Valid: 7.20% Test: 9.00%\n",
            "Epoch: 02, Loss: 2.0039, Train: 20.71%, Valid: 9.00% Test: 11.60%\n",
            "Epoch: 03, Loss: 1.7505, Train: 24.29%, Valid: 10.00% Test: 11.90%\n",
            "Epoch: 04, Loss: 1.5134, Train: 22.14%, Valid: 10.00% Test: 14.00%\n",
            "Epoch: 05, Loss: 2.8713, Train: 23.57%, Valid: 13.20% Test: 15.80%\n",
            "Epoch: 06, Loss: 2.7092, Train: 20.71%, Valid: 16.40% Test: 18.70%\n",
            "Epoch: 07, Loss: 3.1109, Train: 21.43%, Valid: 18.20% Test: 21.20%\n",
            "Epoch: 08, Loss: 3.2883, Train: 20.71%, Valid: 19.20% Test: 22.10%\n",
            "Epoch: 09, Loss: 2.2536, Train: 21.43%, Valid: 21.40% Test: 23.40%\n",
            "Epoch: 10, Loss: 2.3986, Train: 22.86%, Valid: 21.80% Test: 24.10%\n",
            "Epoch: 11, Loss: 1.7020, Train: 22.86%, Valid: 22.20% Test: 24.40%\n",
            "Epoch: 12, Loss: 2.1391, Train: 25.00%, Valid: 21.20% Test: 23.70%\n",
            "Epoch: 13, Loss: 2.0591, Train: 25.00%, Valid: 19.40% Test: 21.50%\n",
            "Epoch: 14, Loss: 2.7683, Train: 27.86%, Valid: 19.40% Test: 18.70%\n",
            "Epoch: 15, Loss: 2.6974, Train: 25.71%, Valid: 15.40% Test: 15.70%\n",
            "Epoch: 16, Loss: 1.9519, Train: 25.00%, Valid: 16.20% Test: 16.70%\n",
            "Epoch: 17, Loss: 1.2484, Train: 21.43%, Valid: 16.60% Test: 15.30%\n",
            "Epoch: 18, Loss: 3.0184, Train: 22.14%, Valid: 17.20% Test: 16.90%\n",
            "Epoch: 19, Loss: 2.0287, Train: 24.29%, Valid: 16.80% Test: 16.90%\n",
            "Epoch: 20, Loss: 1.0429, Train: 25.71%, Valid: 18.20% Test: 18.20%\n",
            "Epoch: 21, Loss: 2.2880, Train: 24.29%, Valid: 19.60% Test: 20.20%\n",
            "Epoch: 22, Loss: 0.0142, Train: 22.86%, Valid: 22.00% Test: 22.20%\n",
            "Epoch: 23, Loss: 2.2529, Train: 24.29%, Valid: 21.00% Test: 22.20%\n",
            "Epoch: 24, Loss: 1.9711, Train: 24.29%, Valid: 20.40% Test: 20.30%\n",
            "Epoch: 25, Loss: 1.3175, Train: 26.43%, Valid: 18.20% Test: 18.20%\n",
            "Epoch: 26, Loss: 1.5133, Train: 27.14%, Valid: 15.60% Test: 16.60%\n",
            "Epoch: 27, Loss: 1.7005, Train: 25.71%, Valid: 16.60% Test: 17.80%\n",
            "Epoch: 28, Loss: 1.0370, Train: 25.00%, Valid: 16.40% Test: 18.60%\n",
            "Epoch: 29, Loss: 0.7846, Train: 23.57%, Valid: 15.00% Test: 16.90%\n",
            "Epoch: 30, Loss: 3.4770, Train: 23.57%, Valid: 14.00% Test: 16.60%\n",
            "Epoch: 31, Loss: 1.0228, Train: 24.29%, Valid: 11.60% Test: 14.20%\n",
            "Epoch: 32, Loss: 2.6351, Train: 25.71%, Valid: 11.60% Test: 14.40%\n",
            "Epoch: 33, Loss: 1.1958, Train: 25.71%, Valid: 12.00% Test: 14.70%\n",
            "Epoch: 34, Loss: 2.6549, Train: 25.00%, Valid: 11.20% Test: 14.70%\n",
            "Epoch: 35, Loss: 2.3429, Train: 25.00%, Valid: 10.80% Test: 14.20%\n",
            "Epoch: 36, Loss: 1.0369, Train: 25.71%, Valid: 11.00% Test: 14.00%\n",
            "Epoch: 37, Loss: 3.2316, Train: 26.43%, Valid: 11.80% Test: 13.60%\n",
            "Epoch: 38, Loss: 1.9108, Train: 26.43%, Valid: 11.40% Test: 13.80%\n",
            "Epoch: 39, Loss: 1.0909, Train: 29.29%, Valid: 13.40% Test: 14.70%\n",
            "Epoch: 40, Loss: 0.5284, Train: 30.00%, Valid: 14.60% Test: 15.50%\n",
            "Epoch: 41, Loss: 1.3503, Train: 29.29%, Valid: 16.40% Test: 15.80%\n",
            "Epoch: 42, Loss: 3.4909, Train: 30.00%, Valid: 16.20% Test: 16.90%\n",
            "Epoch: 43, Loss: 1.7600, Train: 30.71%, Valid: 15.80% Test: 16.20%\n",
            "Epoch: 44, Loss: 3.1561, Train: 29.29%, Valid: 16.20% Test: 16.60%\n",
            "Epoch: 45, Loss: 2.2311, Train: 28.57%, Valid: 16.40% Test: 15.80%\n",
            "Epoch: 46, Loss: 2.4734, Train: 30.71%, Valid: 15.40% Test: 14.30%\n",
            "Epoch: 47, Loss: 2.0860, Train: 33.57%, Valid: 15.00% Test: 13.30%\n",
            "Epoch: 48, Loss: 4.9242, Train: 34.29%, Valid: 14.80% Test: 13.00%\n",
            "Epoch: 49, Loss: 1.8130, Train: 32.14%, Valid: 15.60% Test: 11.80%\n",
            "Epoch: 50, Loss: 0.7634, Train: 33.57%, Valid: 13.80% Test: 11.30%\n",
            "Epoch: 51, Loss: 0.1081, Train: 32.86%, Valid: 13.60% Test: 11.40%\n",
            "Epoch: 52, Loss: 2.3540, Train: 35.71%, Valid: 13.40% Test: 11.80%\n",
            "Epoch: 53, Loss: 0.0937, Train: 36.43%, Valid: 13.80% Test: 12.60%\n",
            "Epoch: 54, Loss: 1.6738, Train: 37.86%, Valid: 13.80% Test: 13.10%\n",
            "Epoch: 55, Loss: 0.2558, Train: 40.71%, Valid: 14.00% Test: 12.60%\n",
            "Epoch: 56, Loss: 0.0953, Train: 41.43%, Valid: 13.60% Test: 12.90%\n",
            "Epoch: 57, Loss: 0.9082, Train: 40.71%, Valid: 13.80% Test: 13.50%\n",
            "Epoch: 58, Loss: 0.3644, Train: 40.71%, Valid: 13.00% Test: 12.80%\n",
            "Epoch: 59, Loss: 0.4921, Train: 40.71%, Valid: 12.40% Test: 12.40%\n",
            "Epoch: 60, Loss: 3.1989, Train: 40.00%, Valid: 13.40% Test: 13.30%\n",
            "Epoch: 61, Loss: 0.6096, Train: 37.14%, Valid: 14.40% Test: 13.00%\n",
            "Epoch: 62, Loss: 2.8727, Train: 35.71%, Valid: 14.00% Test: 13.30%\n",
            "Epoch: 63, Loss: 2.3926, Train: 35.71%, Valid: 13.20% Test: 12.50%\n",
            "Epoch: 64, Loss: 0.0556, Train: 35.00%, Valid: 13.20% Test: 13.00%\n",
            "Epoch: 65, Loss: 2.1258, Train: 34.29%, Valid: 14.20% Test: 12.80%\n",
            "Epoch: 66, Loss: 0.3781, Train: 34.29%, Valid: 13.80% Test: 13.50%\n",
            "Epoch: 67, Loss: 3.3539, Train: 37.14%, Valid: 14.00% Test: 13.80%\n",
            "Epoch: 68, Loss: 0.9819, Train: 40.71%, Valid: 13.40% Test: 14.20%\n",
            "Epoch: 69, Loss: 0.2947, Train: 41.43%, Valid: 14.20% Test: 15.00%\n",
            "Epoch: 70, Loss: 2.1079, Train: 44.29%, Valid: 14.80% Test: 14.90%\n",
            "Epoch: 71, Loss: 4.1766, Train: 46.43%, Valid: 15.20% Test: 17.40%\n",
            "Epoch: 72, Loss: 1.4720, Train: 45.00%, Valid: 16.20% Test: 18.40%\n",
            "Epoch: 73, Loss: 0.8904, Train: 42.86%, Valid: 17.20% Test: 20.40%\n",
            "Epoch: 74, Loss: 1.9322, Train: 42.86%, Valid: 18.80% Test: 22.50%\n",
            "Epoch: 75, Loss: 2.7072, Train: 41.43%, Valid: 19.40% Test: 22.70%\n",
            "Epoch: 76, Loss: 0.0001, Train: 42.14%, Valid: 18.40% Test: 22.50%\n",
            "Epoch: 77, Loss: 1.7009, Train: 42.86%, Valid: 18.40% Test: 22.10%\n",
            "Epoch: 78, Loss: 0.0086, Train: 43.57%, Valid: 20.20% Test: 22.10%\n",
            "Epoch: 79, Loss: 4.3699, Train: 43.57%, Valid: 19.60% Test: 21.40%\n",
            "Epoch: 80, Loss: 0.3977, Train: 43.57%, Valid: 19.60% Test: 20.20%\n",
            "Epoch: 81, Loss: 2.3403, Train: 45.71%, Valid: 20.20% Test: 20.60%\n",
            "Epoch: 82, Loss: 3.0614, Train: 45.00%, Valid: 19.00% Test: 19.00%\n",
            "Epoch: 83, Loss: 1.4902, Train: 45.00%, Valid: 19.40% Test: 18.50%\n",
            "Epoch: 84, Loss: 0.1279, Train: 46.43%, Valid: 19.00% Test: 19.20%\n",
            "Epoch: 85, Loss: 0.0118, Train: 48.57%, Valid: 19.60% Test: 18.50%\n",
            "Epoch: 86, Loss: 0.0853, Train: 50.00%, Valid: 18.60% Test: 18.60%\n",
            "Epoch: 87, Loss: 0.0559, Train: 48.57%, Valid: 18.80% Test: 18.60%\n",
            "Epoch: 88, Loss: 0.8222, Train: 47.14%, Valid: 18.20% Test: 18.60%\n",
            "Epoch: 89, Loss: 0.8275, Train: 44.29%, Valid: 18.20% Test: 18.70%\n",
            "Epoch: 90, Loss: 0.3523, Train: 46.43%, Valid: 18.00% Test: 18.60%\n",
            "Epoch: 91, Loss: 0.0596, Train: 45.00%, Valid: 16.40% Test: 16.90%\n",
            "Epoch: 92, Loss: 0.0714, Train: 45.00%, Valid: 16.20% Test: 17.10%\n",
            "Epoch: 93, Loss: 1.2698, Train: 44.29%, Valid: 14.20% Test: 16.20%\n",
            "Epoch: 94, Loss: 3.9684, Train: 44.29%, Valid: 14.20% Test: 15.70%\n",
            "Epoch: 95, Loss: 2.9273, Train: 45.00%, Valid: 14.00% Test: 16.00%\n",
            "Epoch: 96, Loss: 0.6866, Train: 45.71%, Valid: 16.80% Test: 16.90%\n",
            "Epoch: 97, Loss: 0.7945, Train: 45.00%, Valid: 16.80% Test: 16.30%\n",
            "Epoch: 98, Loss: 2.9485, Train: 46.43%, Valid: 16.00% Test: 16.60%\n",
            "Epoch: 99, Loss: 3.5197, Train: 46.43%, Valid: 14.80% Test: 16.00%\n",
            "Epoch: 100, Loss: 1.6308, Train: 48.57%, Valid: 14.40% Test: 14.90%\n",
            "Epoch: 101, Loss: 2.3361, Train: 48.57%, Valid: 13.60% Test: 13.40%\n",
            "Epoch: 102, Loss: 0.8736, Train: 46.43%, Valid: 11.40% Test: 11.10%\n",
            "Epoch: 103, Loss: 1.7034, Train: 47.86%, Valid: 11.20% Test: 11.00%\n",
            "Epoch: 104, Loss: 0.6563, Train: 47.14%, Valid: 10.20% Test: 10.20%\n",
            "Epoch: 105, Loss: 1.0407, Train: 47.14%, Valid: 10.60% Test: 9.70%\n",
            "Epoch: 106, Loss: 1.8665, Train: 47.14%, Valid: 10.40% Test: 9.60%\n",
            "Epoch: 107, Loss: 0.0005, Train: 45.00%, Valid: 9.40% Test: 8.90%\n",
            "Epoch: 108, Loss: 1.1058, Train: 43.57%, Valid: 8.40% Test: 8.10%\n",
            "Epoch: 109, Loss: 0.9636, Train: 43.57%, Valid: 8.60% Test: 8.10%\n",
            "Epoch: 110, Loss: 2.0704, Train: 46.43%, Valid: 9.40% Test: 8.40%\n",
            "Epoch: 111, Loss: 3.6783, Train: 44.29%, Valid: 9.00% Test: 9.10%\n",
            "Epoch: 112, Loss: 1.9582, Train: 44.29%, Valid: 10.00% Test: 9.40%\n",
            "Epoch: 113, Loss: 0.0869, Train: 45.00%, Valid: 10.20% Test: 9.30%\n",
            "Epoch: 114, Loss: 0.0910, Train: 45.00%, Valid: 9.00% Test: 9.10%\n",
            "Epoch: 115, Loss: 2.6882, Train: 42.14%, Valid: 9.40% Test: 8.80%\n",
            "Epoch: 116, Loss: 1.8344, Train: 44.29%, Valid: 9.80% Test: 9.10%\n",
            "Epoch: 117, Loss: 0.3408, Train: 45.00%, Valid: 9.80% Test: 9.50%\n",
            "Epoch: 118, Loss: 1.9239, Train: 48.57%, Valid: 10.00% Test: 10.90%\n",
            "Epoch: 119, Loss: 0.3193, Train: 46.43%, Valid: 9.40% Test: 10.70%\n",
            "Epoch: 120, Loss: 0.9170, Train: 47.86%, Valid: 10.60% Test: 12.60%\n",
            "Epoch: 121, Loss: 1.9858, Train: 48.57%, Valid: 10.40% Test: 12.40%\n",
            "Epoch: 122, Loss: 1.9944, Train: 51.43%, Valid: 11.20% Test: 13.50%\n",
            "Epoch: 123, Loss: 1.1367, Train: 53.57%, Valid: 11.80% Test: 13.20%\n",
            "Epoch: 124, Loss: 0.1535, Train: 55.00%, Valid: 11.00% Test: 13.00%\n",
            "Epoch: 125, Loss: 0.5269, Train: 56.43%, Valid: 11.20% Test: 11.90%\n",
            "Epoch: 126, Loss: 1.6778, Train: 57.14%, Valid: 10.60% Test: 11.60%\n",
            "Epoch: 127, Loss: 0.0598, Train: 59.29%, Valid: 10.40% Test: 12.50%\n",
            "Epoch: 128, Loss: 4.3471, Train: 55.00%, Valid: 10.80% Test: 12.50%\n",
            "Epoch: 129, Loss: 4.1808, Train: 55.71%, Valid: 11.80% Test: 12.90%\n",
            "Epoch: 130, Loss: 0.0106, Train: 53.57%, Valid: 12.40% Test: 14.10%\n",
            "Epoch: 131, Loss: 0.6133, Train: 55.00%, Valid: 13.60% Test: 14.50%\n",
            "Epoch: 132, Loss: 0.0257, Train: 55.00%, Valid: 13.20% Test: 13.70%\n",
            "Epoch: 133, Loss: 1.6001, Train: 55.71%, Valid: 13.80% Test: 14.10%\n",
            "Epoch: 134, Loss: 2.0169, Train: 57.86%, Valid: 14.00% Test: 14.60%\n",
            "Epoch: 135, Loss: 0.2523, Train: 59.29%, Valid: 14.80% Test: 15.10%\n",
            "Epoch: 136, Loss: 1.6154, Train: 61.43%, Valid: 14.60% Test: 15.90%\n",
            "Epoch: 137, Loss: 3.1848, Train: 62.86%, Valid: 15.00% Test: 17.00%\n",
            "Epoch: 138, Loss: 0.1691, Train: 62.14%, Valid: 15.20% Test: 17.30%\n",
            "Epoch: 139, Loss: 0.0016, Train: 64.29%, Valid: 15.40% Test: 17.20%\n",
            "Epoch: 140, Loss: 0.2673, Train: 64.29%, Valid: 16.00% Test: 17.40%\n",
            "Epoch: 141, Loss: 0.0007, Train: 63.57%, Valid: 16.20% Test: 18.50%\n",
            "Epoch: 142, Loss: 0.8379, Train: 64.29%, Valid: 16.20% Test: 17.70%\n",
            "Epoch: 143, Loss: 1.5785, Train: 65.00%, Valid: 16.20% Test: 16.90%\n",
            "Epoch: 144, Loss: 0.0046, Train: 65.00%, Valid: 15.80% Test: 16.40%\n",
            "Epoch: 145, Loss: 4.2629, Train: 65.00%, Valid: 16.20% Test: 16.30%\n",
            "Epoch: 146, Loss: 0.1890, Train: 65.71%, Valid: 16.20% Test: 15.40%\n",
            "Epoch: 147, Loss: 0.5981, Train: 67.14%, Valid: 16.80% Test: 16.20%\n",
            "Epoch: 148, Loss: 0.1457, Train: 66.43%, Valid: 16.60% Test: 16.10%\n",
            "Epoch: 149, Loss: 0.5407, Train: 65.71%, Valid: 17.00% Test: 16.80%\n",
            "Epoch: 150, Loss: 0.9108, Train: 65.71%, Valid: 16.60% Test: 16.80%\n",
            "Saving Model Predictions for Model: cluster louvain\n",
            "Best model: Train: 22.86%, Valid: 22.20% Test: 24.40%\n"
          ]
        }
      ],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "\n",
        "  set_seed()\n",
        "\n",
        "  model = GNN(graph_train.num_node_features, args['hidden_size'], graph_train.num_node_labels, args).to(args['device'])\n",
        "\n",
        "  # Disable compile as this does not seem to work yet in PyTorch 2.0.1/PyG 2.3.1\n",
        "  # try:\n",
        "  #   model = torch_geometric.compile(model)\n",
        "  #   print(f\"GNN Model compiled\")\n",
        "  # except Exception as err:\n",
        "  #   print(f\"Model compile not supported: {err}\")\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "  louvain_best_model, louvain_accs = train(graphs, [graph_train, graph_val, graph_test], args, model, optimizer, mode=\"community\")\n",
        "  train_acc, val_acc, test_acc = test([graph_train, graph_val, graph_test], louvain_best_model, save_model_results=True, batch_type=\"cluster\", title=\"louvain\")\n",
        "  print('Best model:',\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * val_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CvTf0ANVO9U"
      },
      "source": [
        "## **Question 2.2a:** How does the Bisection algorithm partition our graph? (3 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkV0zlhgVJ7u",
        "outputId": "ca49b9c4-a060-4b24-eb56-e553f5c3153a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index fields: val_mask ignored.\n",
            "Index fields: train_mask ignored.\n",
            "Index fields: test_mask ignored.\n",
            "Partition the graph in to 2 communities\n",
            "Each community has 1354 nodes in average\n",
            "Each community has 2397 edges in average\n"
          ]
        }
      ],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "\n",
        "  set_seed()\n",
        "\n",
        "  graphs_train, graphs_val, graphs_test = \\\n",
        "      GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "  graph_train = graphs_train[0]\n",
        "  graph_val = graphs_val[0]\n",
        "  graph_test = graphs_test[0]\n",
        "  graphs = preprocess(graph_train.G, graph_train.node_label_index, method=\"bisection\")\n",
        "  print(\"Partition the graph in to {} communities\".format(len(graphs)))\n",
        "  avg_num_nodes = 0\n",
        "  avg_num_edges = 0\n",
        "  for graph in graphs:\n",
        "      avg_num_nodes += graph.num_nodes\n",
        "      avg_num_edges += graph.num_edges\n",
        "  avg_num_nodes = int(avg_num_nodes / len(graphs))\n",
        "  avg_num_edges = int(avg_num_edges / len(graphs))\n",
        "  print(\"Each community has {} nodes in average\".format(avg_num_nodes))\n",
        "  print(\"Each community has {} edges in average\".format(avg_num_edges))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqMCvP8wVVms"
      },
      "source": [
        "## **Question 2.2b:** Using the Bisection algorithm to partition the graph, what is the maximum test accuracy obtained by your vanilla Cluster-GCN? (6 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1wgFg1bVRGY",
        "outputId": "298d9cc6-6504-4d68-c9a3-97d3a85f8c70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 2.0529, Train: 27.86%, Valid: 8.00% Test: 11.40%\n",
            "Epoch: 02, Loss: 0.9727, Train: 39.29%, Valid: 11.60% Test: 14.40%\n",
            "Epoch: 03, Loss: 1.9658, Train: 60.71%, Valid: 24.20% Test: 28.10%\n",
            "Epoch: 04, Loss: 0.3075, Train: 81.43%, Valid: 39.60% Test: 42.80%\n",
            "Epoch: 05, Loss: 0.2130, Train: 92.86%, Valid: 52.80% Test: 51.80%\n",
            "Epoch: 06, Loss: 0.1439, Train: 96.43%, Valid: 56.60% Test: 58.00%\n",
            "Epoch: 07, Loss: 0.5617, Train: 97.86%, Valid: 63.00% Test: 63.00%\n",
            "Epoch: 08, Loss: 0.0469, Train: 99.29%, Valid: 65.40% Test: 65.40%\n",
            "Epoch: 09, Loss: 0.0303, Train: 99.29%, Valid: 66.20% Test: 67.10%\n",
            "Epoch: 10, Loss: 0.2082, Train: 100.00%, Valid: 68.20% Test: 68.80%\n",
            "Epoch: 11, Loss: 0.1262, Train: 100.00%, Valid: 69.20% Test: 69.60%\n",
            "Epoch: 12, Loss: 0.0142, Train: 100.00%, Valid: 70.40% Test: 70.20%\n",
            "Epoch: 13, Loss: 0.0139, Train: 100.00%, Valid: 70.80% Test: 70.50%\n",
            "Epoch: 14, Loss: 0.0037, Train: 100.00%, Valid: 70.80% Test: 70.90%\n",
            "Epoch: 15, Loss: 0.0561, Train: 100.00%, Valid: 70.40% Test: 71.60%\n",
            "Epoch: 16, Loss: 0.0288, Train: 100.00%, Valid: 71.20% Test: 72.10%\n",
            "Epoch: 17, Loss: 0.0029, Train: 100.00%, Valid: 71.40% Test: 71.70%\n",
            "Epoch: 18, Loss: 0.0114, Train: 100.00%, Valid: 71.40% Test: 72.10%\n",
            "Epoch: 19, Loss: 0.0056, Train: 100.00%, Valid: 71.00% Test: 72.30%\n",
            "Epoch: 20, Loss: 0.0184, Train: 100.00%, Valid: 70.80% Test: 72.60%\n",
            "Epoch: 21, Loss: 0.0061, Train: 100.00%, Valid: 70.80% Test: 72.20%\n",
            "Epoch: 22, Loss: 0.0107, Train: 100.00%, Valid: 70.80% Test: 72.90%\n",
            "Epoch: 23, Loss: 0.0021, Train: 100.00%, Valid: 71.00% Test: 73.10%\n",
            "Epoch: 24, Loss: 0.0014, Train: 100.00%, Valid: 71.00% Test: 73.10%\n",
            "Epoch: 25, Loss: 0.0011, Train: 100.00%, Valid: 71.00% Test: 73.10%\n",
            "Epoch: 26, Loss: 0.0022, Train: 100.00%, Valid: 71.40% Test: 73.10%\n",
            "Epoch: 27, Loss: 0.0035, Train: 100.00%, Valid: 71.00% Test: 73.10%\n",
            "Epoch: 28, Loss: 0.0029, Train: 100.00%, Valid: 71.00% Test: 73.30%\n",
            "Epoch: 29, Loss: 0.0005, Train: 100.00%, Valid: 71.00% Test: 73.40%\n",
            "Epoch: 30, Loss: 0.0136, Train: 100.00%, Valid: 71.40% Test: 73.60%\n",
            "Epoch: 31, Loss: 0.0007, Train: 100.00%, Valid: 71.60% Test: 73.80%\n",
            "Epoch: 32, Loss: 0.0017, Train: 100.00%, Valid: 71.80% Test: 73.80%\n",
            "Epoch: 33, Loss: 0.0012, Train: 100.00%, Valid: 72.40% Test: 74.10%\n",
            "Epoch: 34, Loss: 0.0004, Train: 100.00%, Valid: 72.80% Test: 74.00%\n",
            "Epoch: 35, Loss: 0.0007, Train: 100.00%, Valid: 72.60% Test: 74.10%\n",
            "Epoch: 36, Loss: 0.0011, Train: 100.00%, Valid: 72.60% Test: 74.10%\n",
            "Epoch: 37, Loss: 0.0016, Train: 100.00%, Valid: 72.80% Test: 74.20%\n",
            "Epoch: 38, Loss: 0.0026, Train: 100.00%, Valid: 72.60% Test: 74.40%\n",
            "Epoch: 39, Loss: 0.0001, Train: 100.00%, Valid: 73.00% Test: 74.30%\n",
            "Epoch: 40, Loss: 0.0007, Train: 100.00%, Valid: 72.60% Test: 74.40%\n",
            "Epoch: 41, Loss: 0.0002, Train: 100.00%, Valid: 72.40% Test: 74.20%\n",
            "Epoch: 42, Loss: 0.0015, Train: 100.00%, Valid: 72.60% Test: 74.40%\n",
            "Epoch: 43, Loss: 0.0006, Train: 100.00%, Valid: 72.80% Test: 74.30%\n",
            "Epoch: 44, Loss: 0.0005, Train: 100.00%, Valid: 72.80% Test: 74.20%\n",
            "Epoch: 45, Loss: 0.0004, Train: 100.00%, Valid: 73.00% Test: 74.30%\n",
            "Epoch: 46, Loss: 0.0017, Train: 100.00%, Valid: 72.80% Test: 74.20%\n",
            "Epoch: 47, Loss: 0.0035, Train: 100.00%, Valid: 73.00% Test: 74.40%\n",
            "Epoch: 48, Loss: 0.0001, Train: 100.00%, Valid: 73.00% Test: 74.60%\n",
            "Epoch: 49, Loss: 0.0001, Train: 100.00%, Valid: 72.80% Test: 74.40%\n",
            "Epoch: 50, Loss: 0.0004, Train: 100.00%, Valid: 73.00% Test: 74.60%\n",
            "Epoch: 51, Loss: 0.0002, Train: 100.00%, Valid: 72.60% Test: 74.40%\n",
            "Epoch: 52, Loss: 0.0007, Train: 100.00%, Valid: 72.60% Test: 74.40%\n",
            "Epoch: 53, Loss: 0.0004, Train: 100.00%, Valid: 72.40% Test: 74.50%\n",
            "Epoch: 54, Loss: 0.0002, Train: 100.00%, Valid: 72.40% Test: 74.40%\n",
            "Epoch: 55, Loss: 0.0000, Train: 100.00%, Valid: 72.60% Test: 74.40%\n",
            "Epoch: 56, Loss: 0.0002, Train: 100.00%, Valid: 72.40% Test: 74.40%\n",
            "Epoch: 57, Loss: 0.0002, Train: 100.00%, Valid: 72.60% Test: 74.40%\n",
            "Epoch: 58, Loss: 0.0000, Train: 100.00%, Valid: 72.60% Test: 74.00%\n",
            "Epoch: 59, Loss: 0.0007, Train: 100.00%, Valid: 72.60% Test: 74.40%\n",
            "Epoch: 60, Loss: 0.0002, Train: 100.00%, Valid: 72.40% Test: 74.30%\n",
            "Epoch: 61, Loss: 0.0002, Train: 100.00%, Valid: 72.60% Test: 74.30%\n",
            "Epoch: 62, Loss: 0.0013, Train: 100.00%, Valid: 72.60% Test: 74.30%\n",
            "Epoch: 63, Loss: 0.0004, Train: 100.00%, Valid: 72.40% Test: 74.20%\n",
            "Epoch: 64, Loss: 0.0033, Train: 100.00%, Valid: 72.20% Test: 74.30%\n",
            "Epoch: 65, Loss: 0.0057, Train: 100.00%, Valid: 72.40% Test: 74.20%\n",
            "Epoch: 66, Loss: 0.0024, Train: 100.00%, Valid: 72.20% Test: 74.30%\n",
            "Epoch: 67, Loss: 0.0012, Train: 100.00%, Valid: 72.20% Test: 74.00%\n",
            "Epoch: 68, Loss: 0.0002, Train: 100.00%, Valid: 72.60% Test: 74.00%\n",
            "Epoch: 69, Loss: 0.0001, Train: 100.00%, Valid: 72.40% Test: 73.80%\n",
            "Epoch: 70, Loss: 0.0002, Train: 100.00%, Valid: 72.40% Test: 73.80%\n",
            "Epoch: 71, Loss: 0.0001, Train: 100.00%, Valid: 72.00% Test: 73.80%\n",
            "Epoch: 72, Loss: 0.0002, Train: 100.00%, Valid: 72.00% Test: 73.90%\n",
            "Epoch: 73, Loss: 0.0001, Train: 100.00%, Valid: 72.00% Test: 74.00%\n",
            "Epoch: 74, Loss: 0.0002, Train: 100.00%, Valid: 72.20% Test: 73.70%\n",
            "Epoch: 75, Loss: 0.0002, Train: 100.00%, Valid: 72.20% Test: 73.80%\n",
            "Epoch: 76, Loss: 0.0001, Train: 100.00%, Valid: 72.20% Test: 74.10%\n",
            "Epoch: 77, Loss: 0.0000, Train: 100.00%, Valid: 72.20% Test: 73.70%\n",
            "Epoch: 78, Loss: 0.0034, Train: 100.00%, Valid: 72.20% Test: 73.50%\n",
            "Epoch: 79, Loss: 0.0002, Train: 100.00%, Valid: 72.00% Test: 73.60%\n",
            "Epoch: 80, Loss: 0.0001, Train: 100.00%, Valid: 71.80% Test: 73.60%\n",
            "Epoch: 81, Loss: 0.0006, Train: 100.00%, Valid: 72.20% Test: 73.40%\n",
            "Epoch: 82, Loss: 0.0001, Train: 100.00%, Valid: 72.20% Test: 73.30%\n",
            "Epoch: 83, Loss: 0.0001, Train: 100.00%, Valid: 72.00% Test: 73.30%\n",
            "Epoch: 84, Loss: 0.0000, Train: 100.00%, Valid: 72.00% Test: 73.20%\n",
            "Epoch: 85, Loss: 0.0011, Train: 100.00%, Valid: 72.20% Test: 73.20%\n",
            "Epoch: 86, Loss: 0.0000, Train: 100.00%, Valid: 72.00% Test: 73.30%\n",
            "Epoch: 87, Loss: 0.0003, Train: 100.00%, Valid: 72.20% Test: 73.30%\n",
            "Epoch: 88, Loss: 0.0002, Train: 100.00%, Valid: 71.60% Test: 73.20%\n",
            "Epoch: 89, Loss: 0.0012, Train: 100.00%, Valid: 71.60% Test: 73.00%\n",
            "Epoch: 90, Loss: 0.0002, Train: 100.00%, Valid: 71.40% Test: 73.10%\n",
            "Epoch: 91, Loss: 0.0011, Train: 100.00%, Valid: 71.60% Test: 73.20%\n",
            "Epoch: 92, Loss: 0.0004, Train: 100.00%, Valid: 71.60% Test: 73.30%\n",
            "Epoch: 93, Loss: 0.0002, Train: 100.00%, Valid: 71.40% Test: 73.20%\n",
            "Epoch: 94, Loss: 0.0069, Train: 100.00%, Valid: 71.40% Test: 73.40%\n",
            "Epoch: 95, Loss: 0.0002, Train: 100.00%, Valid: 71.40% Test: 73.40%\n",
            "Epoch: 96, Loss: 0.0000, Train: 100.00%, Valid: 71.60% Test: 73.30%\n",
            "Epoch: 97, Loss: 0.0003, Train: 100.00%, Valid: 71.40% Test: 73.40%\n",
            "Epoch: 98, Loss: 0.0016, Train: 100.00%, Valid: 71.20% Test: 73.20%\n",
            "Epoch: 99, Loss: 0.0000, Train: 100.00%, Valid: 71.40% Test: 73.10%\n",
            "Epoch: 100, Loss: 0.0002, Train: 100.00%, Valid: 71.40% Test: 73.30%\n",
            "Epoch: 101, Loss: 0.0002, Train: 100.00%, Valid: 71.60% Test: 73.40%\n",
            "Epoch: 102, Loss: 0.0001, Train: 100.00%, Valid: 71.40% Test: 73.70%\n",
            "Epoch: 103, Loss: 0.0001, Train: 100.00%, Valid: 71.40% Test: 74.00%\n",
            "Epoch: 104, Loss: 0.0000, Train: 100.00%, Valid: 71.40% Test: 73.70%\n",
            "Epoch: 105, Loss: 0.0001, Train: 100.00%, Valid: 71.40% Test: 73.60%\n",
            "Epoch: 106, Loss: 0.0104, Train: 100.00%, Valid: 71.00% Test: 73.80%\n",
            "Epoch: 107, Loss: 0.0004, Train: 100.00%, Valid: 71.20% Test: 73.80%\n",
            "Epoch: 108, Loss: 0.0000, Train: 100.00%, Valid: 71.00% Test: 73.80%\n",
            "Epoch: 109, Loss: 0.0005, Train: 100.00%, Valid: 71.20% Test: 73.80%\n",
            "Epoch: 110, Loss: 0.0001, Train: 100.00%, Valid: 70.80% Test: 73.70%\n",
            "Epoch: 111, Loss: 0.0005, Train: 100.00%, Valid: 71.20% Test: 73.80%\n",
            "Epoch: 112, Loss: 0.0001, Train: 100.00%, Valid: 71.40% Test: 73.70%\n",
            "Epoch: 113, Loss: 0.0002, Train: 100.00%, Valid: 72.00% Test: 73.50%\n",
            "Epoch: 114, Loss: 0.0002, Train: 100.00%, Valid: 71.80% Test: 73.60%\n",
            "Epoch: 115, Loss: 0.0000, Train: 100.00%, Valid: 71.80% Test: 73.60%\n",
            "Epoch: 116, Loss: 0.0001, Train: 100.00%, Valid: 71.80% Test: 73.60%\n",
            "Epoch: 117, Loss: 0.0004, Train: 100.00%, Valid: 72.20% Test: 73.70%\n",
            "Epoch: 118, Loss: 0.0000, Train: 100.00%, Valid: 71.80% Test: 73.70%\n",
            "Epoch: 119, Loss: 0.0004, Train: 100.00%, Valid: 72.40% Test: 73.70%\n",
            "Epoch: 120, Loss: 0.0040, Train: 100.00%, Valid: 72.20% Test: 73.50%\n",
            "Epoch: 121, Loss: 0.0000, Train: 100.00%, Valid: 71.60% Test: 73.60%\n",
            "Epoch: 122, Loss: 0.0001, Train: 100.00%, Valid: 72.20% Test: 73.60%\n",
            "Epoch: 123, Loss: 0.0002, Train: 100.00%, Valid: 72.20% Test: 73.70%\n",
            "Epoch: 124, Loss: 0.0002, Train: 100.00%, Valid: 72.20% Test: 73.60%\n",
            "Epoch: 125, Loss: 0.0001, Train: 100.00%, Valid: 72.00% Test: 73.60%\n",
            "Epoch: 126, Loss: 0.0003, Train: 100.00%, Valid: 72.00% Test: 73.60%\n",
            "Epoch: 127, Loss: 0.0000, Train: 100.00%, Valid: 71.60% Test: 73.70%\n",
            "Epoch: 128, Loss: 0.0000, Train: 100.00%, Valid: 71.20% Test: 73.60%\n",
            "Epoch: 129, Loss: 0.0000, Train: 100.00%, Valid: 71.20% Test: 73.70%\n",
            "Epoch: 130, Loss: 0.0004, Train: 100.00%, Valid: 71.20% Test: 73.60%\n",
            "Epoch: 131, Loss: 0.0001, Train: 100.00%, Valid: 71.20% Test: 73.60%\n",
            "Epoch: 132, Loss: 0.0002, Train: 100.00%, Valid: 71.40% Test: 73.60%\n",
            "Epoch: 133, Loss: 0.0002, Train: 100.00%, Valid: 71.20% Test: 73.60%\n",
            "Epoch: 134, Loss: 0.0001, Train: 100.00%, Valid: 71.60% Test: 73.70%\n",
            "Epoch: 135, Loss: 0.0002, Train: 100.00%, Valid: 71.40% Test: 73.60%\n",
            "Epoch: 136, Loss: 0.0005, Train: 100.00%, Valid: 71.80% Test: 73.60%\n",
            "Epoch: 137, Loss: 0.0001, Train: 100.00%, Valid: 71.80% Test: 73.60%\n",
            "Epoch: 138, Loss: 0.0000, Train: 100.00%, Valid: 71.20% Test: 73.60%\n",
            "Epoch: 139, Loss: 0.0004, Train: 100.00%, Valid: 71.80% Test: 73.70%\n",
            "Epoch: 140, Loss: 0.0001, Train: 100.00%, Valid: 71.80% Test: 73.60%\n",
            "Epoch: 141, Loss: 0.0001, Train: 100.00%, Valid: 71.40% Test: 73.70%\n",
            "Epoch: 142, Loss: 0.0001, Train: 100.00%, Valid: 72.20% Test: 73.60%\n",
            "Epoch: 143, Loss: 0.0004, Train: 100.00%, Valid: 71.60% Test: 73.80%\n",
            "Epoch: 144, Loss: 0.0000, Train: 100.00%, Valid: 71.40% Test: 73.70%\n",
            "Epoch: 145, Loss: 0.0000, Train: 100.00%, Valid: 71.20% Test: 73.70%\n",
            "Epoch: 146, Loss: 0.0001, Train: 100.00%, Valid: 71.40% Test: 73.70%\n",
            "Epoch: 147, Loss: 0.0000, Train: 100.00%, Valid: 71.20% Test: 73.70%\n",
            "Epoch: 148, Loss: 0.0002, Train: 100.00%, Valid: 71.60% Test: 73.60%\n",
            "Epoch: 149, Loss: 0.0000, Train: 100.00%, Valid: 71.80% Test: 73.70%\n",
            "Epoch: 150, Loss: 0.0000, Train: 100.00%, Valid: 71.60% Test: 73.50%\n",
            "Saving Model Predictions for Model: cluster bisection\n",
            "Best model: Train: 100.00%, Valid: 73.00% Test: 74.30%\n"
          ]
        }
      ],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "\n",
        "  set_seed()\n",
        "\n",
        "  model = GNN(graph_train.num_node_features, args['hidden_size'], graph_train.num_node_labels, args).to(args['device'])\n",
        "\n",
        "  # Disable compile as this does not seem to work yet in PyTorch 2.0.1/PyG 2.3.1\n",
        "  # try:\n",
        "  #   model = torch_geometric.compile(model)\n",
        "  #   print(f\"GNN Model compiled\")\n",
        "  # except Exception as err:\n",
        "  #   print(f\"Model compile not supported: {err}\")\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "  bisection_best_model, bisection_accs = train(graphs, [graph_train, graph_val, graph_test], args, model, optimizer, mode=\"community\")\n",
        "  train_acc, val_acc, test_acc = test([graph_train, graph_val, graph_test], bisection_best_model, save_model_results=True, batch_type=\"cluster\", title=\"bisection\")\n",
        "  print('Best model:',\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * val_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PROPwoOVcJy"
      },
      "source": [
        "## **Question 2.3a:** How does Greedy preprocessing partition our graph? (3 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3DVamWqVT92",
        "outputId": "4580da03-6313-43db-fee4-82df2f2d8f9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index fields: val_mask ignored.\n",
            "Index fields: train_mask ignored.\n",
            "Index fields: test_mask ignored.\n",
            "Partition the graph in to 20 communities\n",
            "Each community has 121 nodes in average\n",
            "Each community has 222 edges in average\n"
          ]
        }
      ],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "\n",
        "  set_seed()\n",
        "\n",
        "  graphs_train, graphs_val, graphs_test = \\\n",
        "      GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "  graph_train = graphs_train[0]\n",
        "  graph_val = graphs_val[0]\n",
        "  graph_test = graphs_test[0]\n",
        "  graphs = preprocess(graph_train.G, graph_train.node_label_index, method=\"greedy\")\n",
        "  print(\"Partition the graph in to {} communities\".format(len(graphs)))\n",
        "  avg_num_nodes = 0\n",
        "  avg_num_edges = 0\n",
        "  for graph in graphs:\n",
        "      avg_num_nodes += graph.num_nodes\n",
        "      avg_num_edges += graph.num_edges\n",
        "  avg_num_nodes = int(avg_num_nodes / len(graphs))\n",
        "  avg_num_edges = int(avg_num_edges / len(graphs))\n",
        "  print(\"Each community has {} nodes in average\".format(avg_num_nodes))\n",
        "  print(\"Each community has {} edges in average\".format(avg_num_edges))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93pR_-kxVgma"
      },
      "source": [
        "## **Question 2.3b:** Using Greedy preprocessing to partition the graph, what is the maximum test accuracy obtained by your vanilla Cluster-GCN? (6 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQgQY-jPVd_U",
        "outputId": "60075ae4-36b9-4d49-838d-8e2569eb5559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 2.0095, Train: 19.29%, Valid: 19.20% Test: 21.80%\n",
            "Epoch: 02, Loss: 2.5608, Train: 17.86%, Valid: 27.00% Test: 28.80%\n",
            "Epoch: 03, Loss: 2.2371, Train: 23.57%, Valid: 21.00% Test: 25.60%\n",
            "Epoch: 04, Loss: 0.3521, Train: 22.14%, Valid: 15.60% Test: 17.90%\n",
            "Epoch: 05, Loss: 1.3821, Train: 22.86%, Valid: 16.00% Test: 19.60%\n",
            "Epoch: 06, Loss: 2.1014, Train: 22.14%, Valid: 23.20% Test: 26.00%\n",
            "Epoch: 07, Loss: 2.3317, Train: 22.14%, Valid: 25.60% Test: 27.80%\n",
            "Epoch: 08, Loss: 1.2040, Train: 24.29%, Valid: 29.40% Test: 30.20%\n",
            "Epoch: 09, Loss: 3.6556, Train: 24.29%, Valid: 30.00% Test: 31.30%\n",
            "Epoch: 10, Loss: 2.5275, Train: 23.57%, Valid: 30.60% Test: 31.50%\n",
            "Epoch: 11, Loss: 2.3284, Train: 24.29%, Valid: 31.00% Test: 31.40%\n",
            "Epoch: 12, Loss: 2.7586, Train: 28.57%, Valid: 31.40% Test: 31.20%\n",
            "Epoch: 13, Loss: 2.1814, Train: 36.43%, Valid: 30.40% Test: 30.70%\n",
            "Epoch: 14, Loss: 0.0951, Train: 40.00%, Valid: 29.40% Test: 30.60%\n",
            "Epoch: 15, Loss: 0.5107, Train: 45.00%, Valid: 28.20% Test: 30.40%\n",
            "Epoch: 16, Loss: 1.0868, Train: 47.14%, Valid: 25.40% Test: 28.60%\n",
            "Epoch: 17, Loss: 0.8627, Train: 50.00%, Valid: 25.20% Test: 28.00%\n",
            "Epoch: 18, Loss: 0.1379, Train: 50.00%, Valid: 24.60% Test: 27.40%\n",
            "Epoch: 19, Loss: 1.9814, Train: 50.71%, Valid: 23.80% Test: 26.70%\n",
            "Epoch: 20, Loss: 1.1666, Train: 52.14%, Valid: 23.20% Test: 25.40%\n",
            "Epoch: 21, Loss: 0.0550, Train: 52.14%, Valid: 22.80% Test: 25.30%\n",
            "Epoch: 22, Loss: 1.0411, Train: 53.57%, Valid: 23.00% Test: 24.30%\n",
            "Epoch: 23, Loss: 1.4959, Train: 54.29%, Valid: 23.00% Test: 23.30%\n",
            "Epoch: 24, Loss: 0.5718, Train: 57.14%, Valid: 21.40% Test: 22.20%\n",
            "Epoch: 25, Loss: 2.0629, Train: 59.29%, Valid: 22.60% Test: 21.60%\n",
            "Epoch: 26, Loss: 0.6075, Train: 58.57%, Valid: 21.80% Test: 20.30%\n",
            "Epoch: 27, Loss: 3.6004, Train: 60.00%, Valid: 21.80% Test: 20.10%\n",
            "Epoch: 28, Loss: 0.0178, Train: 60.71%, Valid: 23.00% Test: 21.30%\n",
            "Epoch: 29, Loss: 0.0046, Train: 60.71%, Valid: 23.40% Test: 21.10%\n",
            "Epoch: 30, Loss: 0.0418, Train: 62.14%, Valid: 22.40% Test: 21.30%\n",
            "Epoch: 31, Loss: 2.3802, Train: 63.57%, Valid: 22.80% Test: 22.40%\n",
            "Epoch: 32, Loss: 3.0202, Train: 61.43%, Valid: 22.80% Test: 22.80%\n",
            "Epoch: 33, Loss: 0.4674, Train: 62.14%, Valid: 24.00% Test: 23.20%\n",
            "Epoch: 34, Loss: 0.8272, Train: 62.86%, Valid: 23.40% Test: 23.00%\n",
            "Epoch: 35, Loss: 4.9271, Train: 62.86%, Valid: 23.80% Test: 24.20%\n",
            "Epoch: 36, Loss: 3.8685, Train: 65.00%, Valid: 23.60% Test: 23.90%\n",
            "Epoch: 37, Loss: 2.7154, Train: 66.43%, Valid: 22.60% Test: 23.40%\n",
            "Epoch: 38, Loss: 1.0707, Train: 72.14%, Valid: 24.40% Test: 24.20%\n",
            "Epoch: 39, Loss: 0.0161, Train: 75.00%, Valid: 24.00% Test: 24.80%\n",
            "Epoch: 40, Loss: 0.5703, Train: 76.43%, Valid: 23.60% Test: 25.60%\n",
            "Epoch: 41, Loss: 0.1386, Train: 77.86%, Valid: 23.60% Test: 25.20%\n",
            "Epoch: 42, Loss: 0.1709, Train: 76.43%, Valid: 23.80% Test: 25.00%\n",
            "Epoch: 43, Loss: 1.4777, Train: 74.29%, Valid: 23.80% Test: 24.30%\n",
            "Epoch: 44, Loss: 0.4652, Train: 74.29%, Valid: 23.80% Test: 23.50%\n",
            "Epoch: 45, Loss: 1.0997, Train: 75.00%, Valid: 22.40% Test: 22.00%\n",
            "Epoch: 46, Loss: 3.0380, Train: 76.43%, Valid: 22.00% Test: 21.90%\n",
            "Epoch: 47, Loss: 0.2692, Train: 75.00%, Valid: 21.40% Test: 21.00%\n",
            "Epoch: 48, Loss: 0.1940, Train: 78.57%, Valid: 21.00% Test: 20.80%\n",
            "Epoch: 49, Loss: 0.7722, Train: 75.71%, Valid: 20.60% Test: 20.20%\n",
            "Epoch: 50, Loss: 0.4287, Train: 75.00%, Valid: 20.20% Test: 19.40%\n",
            "Epoch: 51, Loss: 0.1512, Train: 75.00%, Valid: 19.60% Test: 19.50%\n",
            "Epoch: 52, Loss: 1.5759, Train: 76.43%, Valid: 19.80% Test: 20.50%\n",
            "Epoch: 53, Loss: 0.0539, Train: 78.57%, Valid: 20.20% Test: 20.70%\n",
            "Epoch: 54, Loss: 0.8778, Train: 80.71%, Valid: 21.20% Test: 21.70%\n",
            "Epoch: 55, Loss: 0.1856, Train: 82.86%, Valid: 21.60% Test: 22.00%\n",
            "Epoch: 56, Loss: 0.9112, Train: 83.57%, Valid: 20.20% Test: 22.60%\n",
            "Epoch: 57, Loss: 0.0704, Train: 85.71%, Valid: 21.80% Test: 23.40%\n",
            "Epoch: 58, Loss: 1.0891, Train: 85.71%, Valid: 21.80% Test: 23.60%\n",
            "Epoch: 59, Loss: 0.0762, Train: 88.57%, Valid: 22.60% Test: 23.60%\n",
            "Epoch: 60, Loss: 0.7806, Train: 88.57%, Valid: 23.00% Test: 23.70%\n",
            "Epoch: 61, Loss: 0.5402, Train: 89.29%, Valid: 22.60% Test: 24.10%\n",
            "Epoch: 62, Loss: 0.0276, Train: 88.57%, Valid: 22.60% Test: 23.80%\n",
            "Epoch: 63, Loss: 0.3594, Train: 87.86%, Valid: 20.60% Test: 22.80%\n",
            "Epoch: 64, Loss: 0.1882, Train: 88.57%, Valid: 21.60% Test: 23.50%\n",
            "Epoch: 65, Loss: 0.5336, Train: 87.86%, Valid: 20.80% Test: 22.80%\n",
            "Epoch: 66, Loss: 0.5698, Train: 88.57%, Valid: 20.40% Test: 23.00%\n",
            "Epoch: 67, Loss: 0.4943, Train: 87.86%, Valid: 20.40% Test: 22.50%\n",
            "Epoch: 68, Loss: 0.1822, Train: 87.14%, Valid: 20.80% Test: 22.80%\n",
            "Epoch: 69, Loss: 0.5435, Train: 87.14%, Valid: 20.00% Test: 21.00%\n",
            "Epoch: 70, Loss: 0.0144, Train: 86.43%, Valid: 19.80% Test: 22.30%\n",
            "Epoch: 71, Loss: 0.1848, Train: 87.14%, Valid: 21.20% Test: 23.30%\n",
            "Epoch: 72, Loss: 0.3813, Train: 87.14%, Valid: 20.60% Test: 22.80%\n",
            "Epoch: 73, Loss: 0.0550, Train: 86.43%, Valid: 21.20% Test: 23.10%\n",
            "Epoch: 74, Loss: 0.0942, Train: 86.43%, Valid: 21.80% Test: 23.20%\n",
            "Epoch: 75, Loss: 0.0000, Train: 85.71%, Valid: 22.00% Test: 22.50%\n",
            "Epoch: 76, Loss: 1.5465, Train: 87.14%, Valid: 22.60% Test: 23.00%\n",
            "Epoch: 77, Loss: 0.0010, Train: 87.86%, Valid: 23.40% Test: 24.10%\n",
            "Epoch: 78, Loss: 0.3932, Train: 88.57%, Valid: 24.20% Test: 24.80%\n",
            "Epoch: 79, Loss: 0.0536, Train: 88.57%, Valid: 26.20% Test: 26.20%\n",
            "Epoch: 80, Loss: 0.2431, Train: 89.29%, Valid: 25.40% Test: 27.10%\n",
            "Epoch: 81, Loss: 0.0443, Train: 87.86%, Valid: 24.80% Test: 26.60%\n",
            "Epoch: 82, Loss: 0.1283, Train: 87.86%, Valid: 26.00% Test: 26.80%\n",
            "Epoch: 83, Loss: 0.0598, Train: 89.29%, Valid: 26.20% Test: 27.10%\n",
            "Epoch: 84, Loss: 0.0008, Train: 90.00%, Valid: 27.00% Test: 28.10%\n",
            "Epoch: 85, Loss: 0.1248, Train: 89.29%, Valid: 26.20% Test: 28.10%\n",
            "Epoch: 86, Loss: 0.0001, Train: 90.00%, Valid: 26.80% Test: 27.40%\n",
            "Epoch: 87, Loss: 0.0292, Train: 89.29%, Valid: 26.40% Test: 28.00%\n",
            "Epoch: 88, Loss: 0.0669, Train: 87.86%, Valid: 27.20% Test: 27.60%\n",
            "Epoch: 89, Loss: 0.0002, Train: 88.57%, Valid: 26.60% Test: 28.80%\n",
            "Epoch: 90, Loss: 0.0456, Train: 89.29%, Valid: 25.80% Test: 28.60%\n",
            "Epoch: 91, Loss: 0.1480, Train: 88.57%, Valid: 26.80% Test: 28.80%\n",
            "Epoch: 92, Loss: 0.0504, Train: 89.29%, Valid: 27.20% Test: 28.80%\n",
            "Epoch: 93, Loss: 0.0000, Train: 88.57%, Valid: 27.40% Test: 29.40%\n",
            "Epoch: 94, Loss: 0.0114, Train: 87.86%, Valid: 26.80% Test: 29.40%\n",
            "Epoch: 95, Loss: 0.0371, Train: 87.86%, Valid: 27.20% Test: 29.60%\n",
            "Epoch: 96, Loss: 0.2234, Train: 89.29%, Valid: 27.40% Test: 30.10%\n",
            "Epoch: 97, Loss: 0.1775, Train: 89.29%, Valid: 28.00% Test: 29.80%\n",
            "Epoch: 98, Loss: 0.1924, Train: 89.29%, Valid: 29.00% Test: 29.70%\n",
            "Epoch: 99, Loss: 0.2451, Train: 89.29%, Valid: 28.80% Test: 29.30%\n",
            "Epoch: 100, Loss: 0.0608, Train: 90.00%, Valid: 28.80% Test: 29.60%\n",
            "Epoch: 101, Loss: 0.0144, Train: 89.29%, Valid: 28.20% Test: 29.00%\n",
            "Epoch: 102, Loss: 0.0122, Train: 89.29%, Valid: 28.60% Test: 28.80%\n",
            "Epoch: 103, Loss: 0.0015, Train: 90.00%, Valid: 29.00% Test: 29.70%\n",
            "Epoch: 104, Loss: 0.0570, Train: 90.00%, Valid: 29.20% Test: 29.90%\n",
            "Epoch: 105, Loss: 0.0525, Train: 90.00%, Valid: 29.80% Test: 30.00%\n",
            "Epoch: 106, Loss: 0.0077, Train: 90.00%, Valid: 29.80% Test: 31.30%\n",
            "Epoch: 107, Loss: 0.0031, Train: 90.00%, Valid: 30.60% Test: 30.90%\n",
            "Epoch: 108, Loss: 0.0000, Train: 90.00%, Valid: 29.80% Test: 30.10%\n",
            "Epoch: 109, Loss: 0.0010, Train: 90.00%, Valid: 29.40% Test: 29.80%\n",
            "Epoch: 110, Loss: 0.0000, Train: 90.00%, Valid: 29.60% Test: 29.30%\n",
            "Epoch: 111, Loss: 0.0043, Train: 90.00%, Valid: 29.20% Test: 29.00%\n",
            "Epoch: 112, Loss: 0.0019, Train: 90.00%, Valid: 30.60% Test: 30.50%\n",
            "Epoch: 113, Loss: 0.0042, Train: 90.00%, Valid: 30.60% Test: 30.00%\n",
            "Epoch: 114, Loss: 0.0004, Train: 90.00%, Valid: 29.60% Test: 29.50%\n",
            "Epoch: 115, Loss: 0.0389, Train: 90.00%, Valid: 30.40% Test: 29.80%\n",
            "Epoch: 116, Loss: 0.0752, Train: 90.00%, Valid: 30.20% Test: 28.70%\n",
            "Epoch: 117, Loss: 0.0000, Train: 90.00%, Valid: 29.20% Test: 29.10%\n",
            "Epoch: 118, Loss: 0.0014, Train: 90.00%, Valid: 29.80% Test: 28.80%\n",
            "Epoch: 119, Loss: 0.0033, Train: 90.00%, Valid: 29.20% Test: 29.00%\n",
            "Epoch: 120, Loss: 0.0464, Train: 90.00%, Valid: 29.80% Test: 30.40%\n",
            "Epoch: 121, Loss: 0.0193, Train: 90.00%, Valid: 29.80% Test: 30.00%\n",
            "Epoch: 122, Loss: 0.0004, Train: 90.00%, Valid: 30.60% Test: 31.00%\n",
            "Epoch: 123, Loss: 0.0176, Train: 90.00%, Valid: 30.20% Test: 30.60%\n",
            "Epoch: 124, Loss: 0.1004, Train: 90.00%, Valid: 29.60% Test: 30.80%\n",
            "Epoch: 125, Loss: 0.0416, Train: 90.00%, Valid: 28.60% Test: 29.90%\n",
            "Epoch: 126, Loss: 0.0687, Train: 90.00%, Valid: 26.80% Test: 28.60%\n",
            "Epoch: 127, Loss: 0.0013, Train: 90.00%, Valid: 28.20% Test: 28.50%\n",
            "Epoch: 128, Loss: 0.0004, Train: 90.71%, Valid: 27.60% Test: 29.10%\n",
            "Epoch: 129, Loss: 0.0001, Train: 90.00%, Valid: 29.00% Test: 29.30%\n",
            "Epoch: 130, Loss: 0.2494, Train: 90.00%, Valid: 28.60% Test: 29.60%\n",
            "Epoch: 131, Loss: 0.1116, Train: 90.00%, Valid: 28.40% Test: 29.60%\n",
            "Epoch: 132, Loss: 0.0012, Train: 90.00%, Valid: 28.80% Test: 29.70%\n",
            "Epoch: 133, Loss: 0.0004, Train: 90.00%, Valid: 29.00% Test: 29.50%\n",
            "Epoch: 134, Loss: 0.0000, Train: 90.00%, Valid: 29.40% Test: 29.70%\n",
            "Epoch: 135, Loss: 0.1102, Train: 90.00%, Valid: 28.20% Test: 29.10%\n",
            "Epoch: 136, Loss: 0.0328, Train: 90.00%, Valid: 29.00% Test: 29.10%\n",
            "Epoch: 137, Loss: 0.0005, Train: 90.00%, Valid: 28.20% Test: 28.40%\n",
            "Epoch: 138, Loss: 0.0056, Train: 90.00%, Valid: 28.80% Test: 28.90%\n",
            "Epoch: 139, Loss: 0.1002, Train: 90.00%, Valid: 28.80% Test: 29.10%\n",
            "Epoch: 140, Loss: 0.0035, Train: 88.57%, Valid: 27.20% Test: 28.80%\n",
            "Epoch: 141, Loss: 0.1383, Train: 88.57%, Valid: 27.60% Test: 28.40%\n",
            "Epoch: 142, Loss: 0.0001, Train: 88.57%, Valid: 26.80% Test: 28.50%\n",
            "Epoch: 143, Loss: 0.0156, Train: 88.57%, Valid: 26.80% Test: 28.70%\n",
            "Epoch: 144, Loss: 0.0103, Train: 88.57%, Valid: 27.00% Test: 28.90%\n",
            "Epoch: 145, Loss: 0.1265, Train: 89.29%, Valid: 26.20% Test: 28.40%\n",
            "Epoch: 146, Loss: 0.0128, Train: 89.29%, Valid: 26.80% Test: 28.20%\n",
            "Epoch: 147, Loss: 0.0001, Train: 88.57%, Valid: 27.60% Test: 27.80%\n",
            "Epoch: 148, Loss: 0.0000, Train: 89.29%, Valid: 27.00% Test: 28.70%\n",
            "Epoch: 149, Loss: 0.0036, Train: 89.29%, Valid: 27.60% Test: 28.70%\n",
            "Epoch: 150, Loss: 0.0152, Train: 88.57%, Valid: 26.60% Test: 27.70%\n",
            "Saving Model Predictions for Model: cluster greedy\n",
            "Best model: Train: 28.57%, Valid: 31.40% Test: 31.20%\n"
          ]
        }
      ],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "\n",
        "  set_seed()\n",
        "\n",
        "  model = GNN(graph_train.num_node_features, args['hidden_size'], graph_train.num_node_labels, args).to(args['device'])\n",
        "\n",
        "  # Disable compile as this does not seem to work yet in PyTorch 2.0.1/PyG 2.3.1\n",
        "  # try:\n",
        "  #   model = torch_geometric.compile(model)\n",
        "  #   print(f\"GNN Model compiled\")\n",
        "  # except Exception as err:\n",
        "  #   print(f\"Model compile not supported: {err}\")\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "  greedy_best_model, greedy_accs = train(graphs, [graph_train, graph_val, graph_test], args, model, optimizer, mode=\"community\")\n",
        "  train_acc, val_acc, test_acc = test([graph_train, graph_val, graph_test], greedy_best_model, save_model_results=True, batch_type=\"cluster\", title=\"greedy\")\n",
        "  print('Best model:',\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * val_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5edKKT6Vk1C"
      },
      "source": [
        "## Full-Batch Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5tIXxC8ViFD",
        "outputId": "5619f8ea-9d70-4c69-de4a-ece7ef042d85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index fields: val_mask ignored.\n",
            "Index fields: train_mask ignored.\n",
            "Index fields: test_mask ignored.\n",
            "Epoch: 01, Loss: 2.0470, Train: 59.29%, Valid: 26.80% Test: 30.60%\n",
            "Epoch: 02, Loss: 1.1333, Train: 97.14%, Valid: 53.00% Test: 57.20%\n",
            "Epoch: 03, Loss: 0.6012, Train: 100.00%, Valid: 67.80% Test: 69.10%\n",
            "Epoch: 04, Loss: 0.2909, Train: 100.00%, Valid: 72.40% Test: 73.80%\n",
            "Epoch: 05, Loss: 0.1524, Train: 100.00%, Valid: 75.20% Test: 75.00%\n",
            "Epoch: 06, Loss: 0.0756, Train: 100.00%, Valid: 75.00% Test: 76.10%\n",
            "Epoch: 07, Loss: 0.0394, Train: 100.00%, Valid: 76.00% Test: 76.00%\n",
            "Epoch: 08, Loss: 0.0200, Train: 100.00%, Valid: 76.00% Test: 75.90%\n",
            "Epoch: 09, Loss: 0.0105, Train: 100.00%, Valid: 75.60% Test: 75.70%\n",
            "Epoch: 10, Loss: 0.0046, Train: 100.00%, Valid: 76.00% Test: 75.80%\n",
            "Epoch: 11, Loss: 0.0048, Train: 100.00%, Valid: 76.00% Test: 76.10%\n",
            "Epoch: 12, Loss: 0.0061, Train: 100.00%, Valid: 75.80% Test: 76.40%\n",
            "Epoch: 13, Loss: 0.0033, Train: 100.00%, Valid: 76.00% Test: 76.20%\n",
            "Epoch: 14, Loss: 0.0018, Train: 100.00%, Valid: 75.40% Test: 76.20%\n",
            "Epoch: 15, Loss: 0.0013, Train: 100.00%, Valid: 75.40% Test: 76.10%\n",
            "Epoch: 16, Loss: 0.0014, Train: 100.00%, Valid: 75.20% Test: 75.80%\n",
            "Epoch: 17, Loss: 0.0007, Train: 100.00%, Valid: 75.20% Test: 76.00%\n",
            "Epoch: 18, Loss: 0.0006, Train: 100.00%, Valid: 75.00% Test: 76.20%\n",
            "Epoch: 19, Loss: 0.0004, Train: 100.00%, Valid: 75.00% Test: 76.40%\n",
            "Epoch: 20, Loss: 0.0007, Train: 100.00%, Valid: 74.80% Test: 76.40%\n",
            "Epoch: 21, Loss: 0.0002, Train: 100.00%, Valid: 74.40% Test: 76.20%\n",
            "Epoch: 22, Loss: 0.0002, Train: 100.00%, Valid: 74.20% Test: 76.10%\n",
            "Epoch: 23, Loss: 0.0003, Train: 100.00%, Valid: 74.20% Test: 76.10%\n",
            "Epoch: 24, Loss: 0.0003, Train: 100.00%, Valid: 74.00% Test: 76.30%\n",
            "Epoch: 25, Loss: 0.0007, Train: 100.00%, Valid: 74.00% Test: 76.30%\n",
            "Epoch: 26, Loss: 0.0004, Train: 100.00%, Valid: 74.00% Test: 76.30%\n",
            "Epoch: 27, Loss: 0.0001, Train: 100.00%, Valid: 73.80% Test: 76.30%\n",
            "Epoch: 28, Loss: 0.0001, Train: 100.00%, Valid: 73.80% Test: 76.50%\n",
            "Epoch: 29, Loss: 0.0002, Train: 100.00%, Valid: 73.80% Test: 76.50%\n",
            "Epoch: 30, Loss: 0.0001, Train: 100.00%, Valid: 73.80% Test: 76.50%\n",
            "Epoch: 31, Loss: 0.0002, Train: 100.00%, Valid: 73.80% Test: 76.60%\n",
            "Epoch: 32, Loss: 0.0009, Train: 100.00%, Valid: 73.80% Test: 76.60%\n",
            "Epoch: 33, Loss: 0.0002, Train: 100.00%, Valid: 74.20% Test: 76.60%\n",
            "Epoch: 34, Loss: 0.0003, Train: 100.00%, Valid: 74.00% Test: 76.60%\n",
            "Epoch: 35, Loss: 0.0003, Train: 100.00%, Valid: 73.80% Test: 76.50%\n",
            "Epoch: 36, Loss: 0.0003, Train: 100.00%, Valid: 73.80% Test: 76.60%\n",
            "Epoch: 37, Loss: 0.0001, Train: 100.00%, Valid: 73.80% Test: 76.60%\n",
            "Epoch: 38, Loss: 0.0000, Train: 100.00%, Valid: 73.80% Test: 76.60%\n",
            "Epoch: 39, Loss: 0.0000, Train: 100.00%, Valid: 73.80% Test: 76.70%\n",
            "Epoch: 40, Loss: 0.0003, Train: 100.00%, Valid: 73.80% Test: 76.70%\n",
            "Epoch: 41, Loss: 0.0002, Train: 100.00%, Valid: 73.80% Test: 76.70%\n",
            "Epoch: 42, Loss: 0.0002, Train: 100.00%, Valid: 74.00% Test: 76.70%\n",
            "Epoch: 43, Loss: 0.0000, Train: 100.00%, Valid: 74.00% Test: 76.60%\n",
            "Epoch: 44, Loss: 0.0002, Train: 100.00%, Valid: 74.00% Test: 76.60%\n",
            "Epoch: 45, Loss: 0.0001, Train: 100.00%, Valid: 74.00% Test: 76.60%\n",
            "Epoch: 46, Loss: 0.0001, Train: 100.00%, Valid: 74.00% Test: 76.60%\n",
            "Epoch: 47, Loss: 0.0001, Train: 100.00%, Valid: 74.00% Test: 76.60%\n",
            "Epoch: 48, Loss: 0.0000, Train: 100.00%, Valid: 74.00% Test: 76.70%\n",
            "Epoch: 49, Loss: 0.0000, Train: 100.00%, Valid: 74.00% Test: 76.70%\n",
            "Epoch: 50, Loss: 0.0003, Train: 100.00%, Valid: 73.80% Test: 76.90%\n",
            "Epoch: 51, Loss: 0.0000, Train: 100.00%, Valid: 73.80% Test: 76.80%\n",
            "Epoch: 52, Loss: 0.0002, Train: 100.00%, Valid: 73.80% Test: 76.80%\n",
            "Epoch: 53, Loss: 0.0001, Train: 100.00%, Valid: 73.80% Test: 76.90%\n",
            "Epoch: 54, Loss: 0.0000, Train: 100.00%, Valid: 73.80% Test: 76.80%\n",
            "Epoch: 55, Loss: 0.0003, Train: 100.00%, Valid: 73.80% Test: 76.90%\n",
            "Epoch: 56, Loss: 0.0000, Train: 100.00%, Valid: 73.80% Test: 76.90%\n",
            "Epoch: 57, Loss: 0.0000, Train: 100.00%, Valid: 73.60% Test: 76.90%\n",
            "Epoch: 58, Loss: 0.0000, Train: 100.00%, Valid: 73.60% Test: 77.00%\n",
            "Epoch: 59, Loss: 0.0000, Train: 100.00%, Valid: 73.60% Test: 77.00%\n",
            "Epoch: 60, Loss: 0.0001, Train: 100.00%, Valid: 73.60% Test: 77.10%\n",
            "Epoch: 61, Loss: 0.0000, Train: 100.00%, Valid: 73.60% Test: 77.10%\n",
            "Epoch: 62, Loss: 0.0000, Train: 100.00%, Valid: 73.60% Test: 77.10%\n",
            "Epoch: 63, Loss: 0.0000, Train: 100.00%, Valid: 73.40% Test: 77.10%\n",
            "Epoch: 64, Loss: 0.0002, Train: 100.00%, Valid: 73.60% Test: 77.00%\n",
            "Epoch: 65, Loss: 0.0001, Train: 100.00%, Valid: 73.60% Test: 76.90%\n",
            "Epoch: 66, Loss: 0.0002, Train: 100.00%, Valid: 73.60% Test: 76.90%\n",
            "Epoch: 67, Loss: 0.0000, Train: 100.00%, Valid: 73.60% Test: 76.90%\n",
            "Epoch: 68, Loss: 0.0000, Train: 100.00%, Valid: 73.60% Test: 76.90%\n",
            "Epoch: 69, Loss: 0.0001, Train: 100.00%, Valid: 73.60% Test: 76.90%\n",
            "Epoch: 70, Loss: 0.0005, Train: 100.00%, Valid: 73.80% Test: 76.80%\n",
            "Epoch: 71, Loss: 0.0001, Train: 100.00%, Valid: 73.80% Test: 76.80%\n",
            "Epoch: 72, Loss: 0.0000, Train: 100.00%, Valid: 73.80% Test: 76.80%\n",
            "Epoch: 73, Loss: 0.0000, Train: 100.00%, Valid: 73.80% Test: 76.80%\n",
            "Epoch: 74, Loss: 0.0002, Train: 100.00%, Valid: 73.80% Test: 76.70%\n",
            "Epoch: 75, Loss: 0.0002, Train: 100.00%, Valid: 73.80% Test: 76.70%\n",
            "Epoch: 76, Loss: 0.0001, Train: 100.00%, Valid: 73.80% Test: 76.70%\n",
            "Epoch: 77, Loss: 0.0003, Train: 100.00%, Valid: 73.80% Test: 76.70%\n",
            "Epoch: 78, Loss: 0.0000, Train: 100.00%, Valid: 73.80% Test: 76.70%\n",
            "Epoch: 79, Loss: 0.0001, Train: 100.00%, Valid: 73.80% Test: 76.60%\n",
            "Epoch: 80, Loss: 0.0001, Train: 100.00%, Valid: 73.80% Test: 76.50%\n",
            "Epoch: 81, Loss: 0.0002, Train: 100.00%, Valid: 73.80% Test: 76.50%\n",
            "Epoch: 82, Loss: 0.0000, Train: 100.00%, Valid: 73.80% Test: 76.50%\n",
            "Epoch: 83, Loss: 0.0000, Train: 100.00%, Valid: 73.80% Test: 76.60%\n",
            "Epoch: 84, Loss: 0.0000, Train: 100.00%, Valid: 73.80% Test: 76.60%\n",
            "Epoch: 85, Loss: 0.0003, Train: 100.00%, Valid: 73.80% Test: 76.60%\n",
            "Epoch: 86, Loss: 0.0000, Train: 100.00%, Valid: 73.80% Test: 76.70%\n",
            "Epoch: 87, Loss: 0.0001, Train: 100.00%, Valid: 73.80% Test: 76.80%\n",
            "Epoch: 88, Loss: 0.0000, Train: 100.00%, Valid: 73.80% Test: 76.80%\n",
            "Epoch: 89, Loss: 0.0000, Train: 100.00%, Valid: 73.80% Test: 76.80%\n",
            "Epoch: 90, Loss: 0.0000, Train: 100.00%, Valid: 73.80% Test: 76.80%\n",
            "Epoch: 91, Loss: 0.0000, Train: 100.00%, Valid: 73.80% Test: 76.80%\n",
            "Epoch: 92, Loss: 0.0003, Train: 100.00%, Valid: 74.00% Test: 76.80%\n",
            "Epoch: 93, Loss: 0.0000, Train: 100.00%, Valid: 74.00% Test: 76.80%\n",
            "Epoch: 94, Loss: 0.0000, Train: 100.00%, Valid: 74.00% Test: 76.80%\n",
            "Epoch: 95, Loss: 0.0000, Train: 100.00%, Valid: 74.00% Test: 76.70%\n",
            "Epoch: 96, Loss: 0.0000, Train: 100.00%, Valid: 73.80% Test: 76.70%\n",
            "Epoch: 97, Loss: 0.0001, Train: 100.00%, Valid: 73.80% Test: 76.70%\n",
            "Epoch: 98, Loss: 0.0000, Train: 100.00%, Valid: 74.00% Test: 76.80%\n",
            "Epoch: 99, Loss: 0.0004, Train: 100.00%, Valid: 74.00% Test: 76.80%\n",
            "Epoch: 100, Loss: 0.0000, Train: 100.00%, Valid: 74.00% Test: 76.80%\n",
            "Epoch: 101, Loss: 0.0003, Train: 100.00%, Valid: 74.00% Test: 76.70%\n",
            "Epoch: 102, Loss: 0.0000, Train: 100.00%, Valid: 74.00% Test: 76.70%\n",
            "Epoch: 103, Loss: 0.0001, Train: 100.00%, Valid: 74.20% Test: 76.70%\n",
            "Epoch: 104, Loss: 0.0000, Train: 100.00%, Valid: 74.20% Test: 76.70%\n",
            "Epoch: 105, Loss: 0.0000, Train: 100.00%, Valid: 74.20% Test: 76.70%\n",
            "Epoch: 106, Loss: 0.0000, Train: 100.00%, Valid: 74.20% Test: 76.70%\n",
            "Epoch: 107, Loss: 0.0011, Train: 100.00%, Valid: 74.20% Test: 76.70%\n",
            "Epoch: 108, Loss: 0.0000, Train: 100.00%, Valid: 74.20% Test: 76.70%\n",
            "Epoch: 109, Loss: 0.0000, Train: 100.00%, Valid: 74.20% Test: 76.70%\n",
            "Epoch: 110, Loss: 0.0001, Train: 100.00%, Valid: 74.00% Test: 76.40%\n",
            "Epoch: 111, Loss: 0.0000, Train: 100.00%, Valid: 74.20% Test: 76.40%\n",
            "Epoch: 112, Loss: 0.0000, Train: 100.00%, Valid: 74.20% Test: 76.40%\n",
            "Epoch: 113, Loss: 0.0000, Train: 100.00%, Valid: 74.20% Test: 76.40%\n",
            "Epoch: 114, Loss: 0.0001, Train: 100.00%, Valid: 74.20% Test: 76.40%\n",
            "Epoch: 115, Loss: 0.0000, Train: 100.00%, Valid: 74.20% Test: 76.40%\n",
            "Epoch: 116, Loss: 0.0000, Train: 100.00%, Valid: 74.20% Test: 76.40%\n",
            "Epoch: 117, Loss: 0.0001, Train: 100.00%, Valid: 74.20% Test: 76.40%\n",
            "Epoch: 118, Loss: 0.0001, Train: 100.00%, Valid: 74.20% Test: 76.40%\n",
            "Epoch: 119, Loss: 0.0000, Train: 100.00%, Valid: 74.20% Test: 76.40%\n",
            "Epoch: 120, Loss: 0.0000, Train: 100.00%, Valid: 74.20% Test: 76.40%\n",
            "Epoch: 121, Loss: 0.0001, Train: 100.00%, Valid: 74.20% Test: 76.40%\n",
            "Epoch: 122, Loss: 0.0001, Train: 100.00%, Valid: 74.20% Test: 76.40%\n",
            "Epoch: 123, Loss: 0.0001, Train: 100.00%, Valid: 74.20% Test: 76.40%\n",
            "Epoch: 124, Loss: 0.0000, Train: 100.00%, Valid: 74.00% Test: 76.40%\n",
            "Epoch: 125, Loss: 0.0001, Train: 100.00%, Valid: 74.00% Test: 76.40%\n",
            "Epoch: 126, Loss: 0.0000, Train: 100.00%, Valid: 74.00% Test: 76.40%\n",
            "Epoch: 127, Loss: 0.0001, Train: 100.00%, Valid: 74.00% Test: 76.30%\n",
            "Epoch: 128, Loss: 0.0000, Train: 100.00%, Valid: 74.00% Test: 76.30%\n",
            "Epoch: 129, Loss: 0.0000, Train: 100.00%, Valid: 74.00% Test: 76.30%\n",
            "Epoch: 130, Loss: 0.0001, Train: 100.00%, Valid: 74.00% Test: 76.30%\n",
            "Epoch: 131, Loss: 0.0000, Train: 100.00%, Valid: 74.00% Test: 76.20%\n",
            "Epoch: 132, Loss: 0.0001, Train: 100.00%, Valid: 74.00% Test: 76.20%\n",
            "Epoch: 133, Loss: 0.0003, Train: 100.00%, Valid: 74.20% Test: 76.20%\n",
            "Epoch: 134, Loss: 0.0000, Train: 100.00%, Valid: 74.20% Test: 76.30%\n",
            "Epoch: 135, Loss: 0.0000, Train: 100.00%, Valid: 74.20% Test: 76.30%\n",
            "Epoch: 136, Loss: 0.0030, Train: 100.00%, Valid: 73.80% Test: 76.40%\n",
            "Epoch: 137, Loss: 0.0000, Train: 100.00%, Valid: 74.00% Test: 76.40%\n",
            "Epoch: 138, Loss: 0.0000, Train: 100.00%, Valid: 74.00% Test: 76.30%\n",
            "Epoch: 139, Loss: 0.0000, Train: 100.00%, Valid: 74.00% Test: 76.30%\n",
            "Epoch: 140, Loss: 0.0004, Train: 100.00%, Valid: 74.00% Test: 76.40%\n",
            "Epoch: 141, Loss: 0.0000, Train: 100.00%, Valid: 73.80% Test: 76.40%\n",
            "Epoch: 142, Loss: 0.0004, Train: 100.00%, Valid: 74.00% Test: 76.40%\n",
            "Epoch: 143, Loss: 0.0001, Train: 100.00%, Valid: 74.00% Test: 76.40%\n",
            "Epoch: 144, Loss: 0.0000, Train: 100.00%, Valid: 73.80% Test: 76.30%\n",
            "Epoch: 145, Loss: 0.0000, Train: 100.00%, Valid: 73.60% Test: 76.30%\n",
            "Epoch: 146, Loss: 0.0001, Train: 100.00%, Valid: 73.60% Test: 76.30%\n",
            "Epoch: 147, Loss: 0.0000, Train: 100.00%, Valid: 73.60% Test: 76.20%\n",
            "Epoch: 148, Loss: 0.0000, Train: 100.00%, Valid: 73.60% Test: 76.20%\n",
            "Epoch: 149, Loss: 0.0000, Train: 100.00%, Valid: 73.60% Test: 76.20%\n",
            "Epoch: 150, Loss: 0.0000, Train: 100.00%, Valid: 73.60% Test: 76.20%\n",
            "Best model: Train: 100.00%, Valid: 76.00% Test: 76.00%\n"
          ]
        }
      ],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "\n",
        "  set_seed()\n",
        "\n",
        "  graphs_train, graphs_val, graphs_test = \\\n",
        "      GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "  graph_train = graphs_train[0]\n",
        "  graph_val = graphs_val[0]\n",
        "  graph_test = graphs_test[0]\n",
        "\n",
        "  model = GNN(graph_train.num_node_features, args['hidden_size'], graph_train.num_node_labels, args).to(args['device'])\n",
        "\n",
        "  # Disable compile as this does not seem to work yet in PyTorch 2.0.1/PyG 2.3.1\n",
        "  # try:\n",
        "  #   model = torch_geometric.compile(model)\n",
        "  #   print(f\"GNN Model compiled\")\n",
        "  # except Exception as err:\n",
        "  #   print(f\"Model compile not supported: {err}\")\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "  graphs = [graph_train, graph_val, graph_test]\n",
        "  all_best_model, all_accs = train(graphs, graphs, args, model, optimizer, mode=\"all\")\n",
        "  train_acc, val_acc, test_acc = test([graph_train, graph_val, graph_test], all_best_model)\n",
        "  print('Best model:',\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * val_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RpuETv7Vpx0"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "PMK33kY5VmF5",
        "outputId": "958e3c3e-e3bc-4624-e128-018bf2273d80"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 900x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  louvain_results = np.array(louvain_accs)\n",
        "  bisection_results = np.array(bisection_accs)\n",
        "  greedy_results = np.array(greedy_accs)\n",
        "  all_results = np.array(all_accs)\n",
        "\n",
        "  x = np.arange(1, 151)\n",
        "\n",
        "  plt.figure(figsize=(9, 7))\n",
        "\n",
        "  plt.plot(x, louvain_results[:, 1], label=\"Louvain Validation\")\n",
        "  plt.plot(x, louvain_results[:, 2], label=\"Louvain Test\")\n",
        "  plt.plot(x, bisection_results[:, 1], label=\"Bisection Validation\")\n",
        "  plt.plot(x, bisection_results[:, 2], label=\"Bisection Test\")\n",
        "  plt.plot(x, greedy_results[:, 1], label=\"Greedy Validation\")\n",
        "  plt.plot(x, greedy_results[:, 2], label=\"Greedy Test\")\n",
        "  plt.plot(x, all_results[:, 1], label=\"All Validation\")\n",
        "  plt.plot(x, all_results[:, 2], label=\"All Test\")\n",
        "  plt.title('Model Accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfucBiYfVYFF"
      },
      "source": [
        "# Submission\n",
        "\n",
        "You will need to submit three files on Gradescope to complete this notebook.\n",
        "\n",
        "1.   Your completed *XCS224W_Colab5.ipynb*. From the \"File\" menu select \"Download .ipynb\" to save a local copy of your completed Colab.\n",
        "2.  *CORA_Node_batch_(0.7,0.9,1).csv*\n",
        "3.  *CORA_Node_batch_(0.3,0.5,1).csv*\n",
        "4.  *CORA_Node_cluster_louvain.csv*\n",
        "5.  *CORA_Node_cluster_greedy.csv*\n",
        "6.  *CORA_Node_cluster_bisection.csv*\n",
        "\n",
        "Download the csv files by selecting the *Folder* icon on the left panel.\n",
        "\n",
        "To submit your work, zip the files downloaded in steps 1-7 above and submit to gradescope. **NOTE:** DO NOT rename any of the downloaded files."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}