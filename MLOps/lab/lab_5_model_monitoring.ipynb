{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Model monitoring\n",
    "\n",
    "In this notebook, you monitor and evaluate the data captured from the endpoint. You create a baseline with which you compare the real-time traffic. Once a baseline is ready, you set up a schedule to continuously evaluate and compare the data against the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1: Environment setup\n",
    "\n",
    "In this task, you set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T15:27:10.906960Z",
     "iopub.status.busy": "2025-06-20T15:27:10.906604Z",
     "iopub.status.idle": "2025-06-20T15:27:16.014733Z",
     "shell.execute_reply": "2025-06-20T15:27:16.013986Z",
     "shell.execute_reply.started": "2025-06-20T15:27:10.906932Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "#install-dependencies\n",
    "%matplotlib inline\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import boto3\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sagemaker import get_execution_role, session\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "from time import sleep\n",
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sm_session = session.Session(boto3.Session())\n",
    "sm = boto3.Session().client(\"sagemaker\")\n",
    "sm_runtime = boto3.Session().client(\"sagemaker-runtime\")\n",
    "cw = boto3.Session().client(\"cloudwatch\")\n",
    "\n",
    "bucket = sm_session.default_bucket()\n",
    "prefix = 'sagemaker/abalone'\n",
    "data_capture_prefix = \"{}/datacapture\".format(prefix)\n",
    "s3_capture_upload_path = \"s3://{}/{}\".format(bucket, data_capture_prefix)\n",
    "capture_modes = [ \"Input\",  \"Output\" ]\n",
    "code_prefix = \"{}/code\".format(prefix)\n",
    "s3_code_preprocessor_uri = \"s3://{}/{}/{}\".format(bucket, code_prefix, \"preprocessor.py\")\n",
    "s3_code_postprocessor_uri = \"s3://{}/{}/{}\".format(bucket, code_prefix, \"postprocessor.py\")\n",
    "reports_prefix = \"{}/reports\".format(prefix)\n",
    "s3_report_path = \"s3://{}/{}\".format(bucket, reports_prefix)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2: Create a production endpoint with Data Capture enabled\n",
    "\n",
    "To log the inputs to your endpoint and the inference outputs from your deployed model to Amazon S3, you can enable a feature called Data Capture. Data Capture records information that can be used for training, debugging, and monitoring. Amazon SageMaker Model Monitor automatically parses this captured data and compares metrics from this data with a baseline that you create for the model.\n",
    "\n",
    "In this task, you upload the pre-trained model to the S3 bucket, create an Amazon Sagemaker model object, configure an Amazon SageMaker real-time endpoint with Data Capture enabled, and create the real-time endpoint.\n",
    "\n",
    "<i class=\"fas fa-sticky-note\" style=\"color:#ff6633\"></i> **Note:** Endpoint creation takes approximately 5 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T15:27:24.013095Z",
     "iopub.status.busy": "2025-06-20T15:27:24.012755Z",
     "iopub.status.idle": "2025-06-20T15:30:31.721348Z",
     "shell.execute_reply": "2025-06-20T15:30:31.718500Z",
     "shell.execute_reply.started": "2025-06-20T15:27:24.013069Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created the Production Model Endpoint Config: Abalone-Endpoint-1-2025-06-20-15-27-24\n",
      "Waiting for Endpoint Creation\n",
      "Waiting for Endpoint Creation\n",
      "Waiting for Endpoint Creation\n",
      "Waiting for Endpoint Creation\n",
      "Waiting for Endpoint Creation\n",
      "Waiting for Endpoint Creation\n",
      "Waiting for Endpoint Creation\n",
      "Waiting for Endpoint Creation\n",
      "Waiting for Endpoint Creation\n",
      "Waiting for Endpoint Creation\n",
      "Waiting for Endpoint Creation\n",
      "Waiting for Endpoint Creation\n",
      "Endpoint arn:aws:sagemaker:us-west-2:701139230119:endpoint/Abalone-2025-06-20-15-27-30 successfully created.\n"
     ]
    }
   ],
   "source": [
    "#create-production-endpoint\n",
    "# Upload models\n",
    "model_url = S3Uploader.upload(\n",
    "    local_path=\"models/model.tar.gz\", desired_s3_uri=f\"s3://{bucket}/{prefix}\"\n",
    ")\n",
    "\n",
    "# Create the model definitions\n",
    "model_name = f\"abalone-A-{datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "image_uri = retrieve(\"xgboost\", boto3.Session().region_name, \"1.5-1\")\n",
    "\n",
    "# Create production model object\n",
    "predictor=sm_session.create_model(\n",
    "    name=model_name, role=role, container_defs={\"Image\": image_uri, \"ModelDataUrl\": model_url}\n",
    ")\n",
    "\n",
    "# Create the endpoint configurations\n",
    "variant_name = 'AllTraffic'\n",
    "\n",
    "endpoint_config_name = f'Abalone-Endpoint-1-{datetime.now():%Y-%m-%d-%H-%M-%S}'\n",
    "endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'ModelName':model_name,\n",
    "            'InstanceType':'ml.m5.xlarge',\n",
    "            'InitialInstanceCount':1,\n",
    "            'VariantName':variant_name\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "        DataCaptureConfig= {\n",
    "        'EnableCapture': True, # Whether data should be captured or not.\n",
    "        'InitialSamplingPercentage' : 100,\n",
    "        'CaptureContentTypeHeader': {'CsvContentTypes': [ 'text/csv' ]},\n",
    "        'DestinationS3Uri': s3_capture_upload_path,\n",
    "        'CaptureOptions': [{\"CaptureMode\" : capture_mode} for capture_mode in capture_modes] # Example - Use list comprehension to capture both Input and Output\n",
    "    }\n",
    ")\n",
    "print(f\"Created the Production Model Endpoint Config: {endpoint_config_name}\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Create the endpoint with the production model\n",
    "endpoint_name = f\"Abalone-{datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "endpoint_response = sm.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n",
    "\n",
    "def wait_for_endpoint_creation_complete(endpoint):\n",
    "    \"\"\"Helper function to wait for the completion of creating an endpoint\"\"\"\n",
    "    response = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = response.get(\"EndpointStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Endpoint Creation\")\n",
    "        time.sleep(15)\n",
    "        response = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "        status = response.get(\"EndpointStatus\")\n",
    "\n",
    "    if status != \"InService\":\n",
    "        print(f\"Failed to create endpoint, response: {response}\")\n",
    "        failureReason = response.get(\"FailureReason\", \"\")\n",
    "        raise SystemExit(\n",
    "            f\"Failed to create endpoint {endpoint_response['EndpointArn']}, status: {status}, reason: {failureReason}\"\n",
    "        )\n",
    "    print(f\"Endpoint {endpoint_response['EndpointArn']} successfully created.\")\n",
    "\n",
    "wait_for_endpoint_creation_complete(endpoint=endpoint_response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "When the cell completes, an endpoint ARN is returned that looks like *arn:aws:sagemaker:us-west-2:012345678910:endpoint/abalone-2040-10-11-10-11-12*.\n",
    "\n",
    "Your endpoint is currently configured with one variant, the production model. You can view the endpoint configuration using *describe_endpoint*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T15:30:31.724439Z",
     "iopub.status.busy": "2025-06-20T15:30:31.723735Z",
     "iopub.status.idle": "2025-06-20T15:30:31.818211Z",
     "shell.execute_reply": "2025-06-20T15:30:31.816814Z",
     "shell.execute_reply.started": "2025-06-20T15:30:31.724399Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointName': 'Abalone-2025-06-20-15-27-30',\n",
       " 'EndpointArn': 'arn:aws:sagemaker:us-west-2:701139230119:endpoint/Abalone-2025-06-20-15-27-30',\n",
       " 'EndpointConfigName': 'Abalone-Endpoint-1-2025-06-20-15-27-24',\n",
       " 'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "   'DeployedImages': [{'SpecifiedImage': '246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-xgboost:1.5-1',\n",
       "     'ResolvedImage': '246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-xgboost@sha256:cd128bd02824075bf2e02ee7923aaa8ab50f6c47a2d86d5747b78ca1c5199813',\n",
       "     'ResolutionTime': datetime.datetime(2025, 6, 20, 15, 27, 31, 413000, tzinfo=tzlocal())}],\n",
       "   'CurrentWeight': 1.0,\n",
       "   'DesiredWeight': 1.0,\n",
       "   'CurrentInstanceCount': 1,\n",
       "   'DesiredInstanceCount': 1}],\n",
       " 'DataCaptureConfig': {'EnableCapture': True,\n",
       "  'CaptureStatus': 'Started',\n",
       "  'CurrentSamplingPercentage': 100,\n",
       "  'DestinationS3Uri': 's3://sagemaker-us-west-2-701139230119/sagemaker/abalone/datacapture'},\n",
       " 'EndpointStatus': 'InService',\n",
       " 'CreationTime': datetime.datetime(2025, 6, 20, 15, 27, 30, 773000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2025, 6, 20, 15, 30, 31, 127000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '041cc9f6-0413-4302-bc04-001a2b163cc1',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '041cc9f6-0413-4302-bc04-001a2b163cc1',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '927',\n",
       "   'date': 'Fri, 20 Jun 2025 15:30:31 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#describe-the-endpoint\n",
    "sm.describe_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.3: View the captured data\n",
    "\n",
    "In this task, you invoke the endpoint created above using production data. Since you already enabled Data Capture on the endpoint, the request payload, response, and additional metadata is saved in the S3 location you specified earlier in the notebook. After invoking the endpoint, you examine the data captured in the S3 bucket.\n",
    "\n",
    "First, use an initialized predictor configured with the endpoint name to invoke the endpoint. Invoking the endpoint with new records helps you confirm your Data Capture configuration is set up correctly. A predictor makes prediction requests to an Amazon SageMaker endpoint. Then, run inference by sending records to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T15:30:31.820980Z",
     "iopub.status.busy": "2025-06-20T15:30:31.820455Z",
     "iopub.status.idle": "2025-06-20T15:31:34.153819Z",
     "shell.execute_reply": "2025-06-20T15:31:34.152801Z",
     "shell.execute_reply.started": "2025-06-20T15:30:31.820954Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................................\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#invoke-the-endpoint\n",
    "predictor = Predictor(endpoint_name=endpoint_name,\n",
    "                        serializer=CSVSerializer(),\n",
    "                        deserializer=CSVDeserializer())\n",
    "\n",
    "validate_dataset = \"abalone_data_new_predictions.csv\"\n",
    "\n",
    "limit = 200  # Need at least 200 samples to compute standard deviations\n",
    "i = 0\n",
    "with open(f\"data/{validate_dataset}\", \"w\") as validation_file:\n",
    "    validation_file.write(\"prediction,label\\n\")  # CSV header\n",
    "    with open(\"data/abalone_data_new.csv\", \"r\") as f:\n",
    "        for row in f:\n",
    "            (label, input_cols) = row.split(\",\", 1)\n",
    "            prediction = predictor.predict(input_cols)[0][0]\n",
    "            validation_file.write(f\"{prediction},{label}\\n\")\n",
    "            i += 1\n",
    "            if i > limit:\n",
    "                break\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            sleep(0.5)\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, list the Data Capture files stored in Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T15:51:34.194211Z",
     "iopub.status.busy": "2025-06-20T15:51:34.193830Z",
     "iopub.status.idle": "2025-06-20T15:51:34.414823Z",
     "shell.execute_reply": "2025-06-20T15:51:34.414021Z",
     "shell.execute_reply.started": "2025-06-20T15:51:34.194185Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Capture Files:\n",
      "sagemaker/abalone/datacapture/Abalone-2025-06-20-15-27-30/AllTraffic/2025/06/20/15/30-32-107-282e6e92-3cc9-4dc5-9920-e9412523137c.jsonl\n",
      " sagemaker/abalone/datacapture/Abalone-2025-06-20-15-27-30/AllTraffic/2025/06/20/15/31-32-611-cc6a3d14-cd74-4fbb-83cb-1175c22665fb.jsonl\n"
     ]
    }
   ],
   "source": [
    "#list-data-capture-files\n",
    "s3_client = boto3.Session().client(\"s3\")\n",
    "current_endpoint_capture_prefix = \"{}/{}\".format(data_capture_prefix, endpoint_name)\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get(\"Contents\")]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, view the contents of a single Data Capture file. You should see all the data captured in an Amazon SageMaker specific JSON-line formatted file. Take a moment to review the first few lines in the captured file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T15:51:43.750515Z",
     "iopub.status.busy": "2025-06-20T15:51:43.750216Z",
     "iopub.status.idle": "2025-06-20T15:51:43.872234Z",
     "shell.execute_reply": "2025-06-20T15:51:43.871452Z",
     "shell.execute_reply.started": "2025-06-20T15:51:43.750492Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"-0.19479093229520963,-0.2609322836255467,-0.6774256966865404,-0.4963820637296466,-0.35624767122022005,-0.48767633072477057,-0.6714313942021038,1.0,0.0,0.0\\n\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"8.268270492553711\\n\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"d1d745a4-6bad-4360-9df9-9f0b61ec0c99\",\"inferenceTime\":\"2025-06-20T15:31:32Z\"},\"eventVersion\":\"0\"}\n",
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"-0.9293075716338962,-0.9238872713386619,-0.6774256966865404,-1.0899727459741797,-1.0982071067690993,-1.1887494921702166,-1.1768129860659535,0.0,1.0,0.0\\n\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"7.6042046546936035\\n\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"e6df6806-e49d-4c41-9f12-ad1fc3e5b6c1\",\"inferenceTime\":\"2025-06-20T15:31:33Z\"},\"eventVersion\":\"0\"}\n",
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"-0.1515840711576398,-0.2609322836255467,-0.6774256966865404,-0.5354746909831861,-0.490175367167671,-0.47845168386364656,-0.6435482718923742,1.0,0.0,0.0\\n\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"8.538948059082031\\n\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"7b1fd6b2-d22f-468f-8a01-ce491edf9a88\",\"inferenceTime\":\"2025-06-20T15:31:33Z\"},\"eventVersion\":\"0\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#view-captured-file-lines\n",
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get(\"Body\").read().decode(\"utf-8\")\n",
    "\n",
    "\n",
    "capture_file = get_obj_body(capture_files[-1])\n",
    "print(capture_file[:2000])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, view the contents of one captured endpoint input and output record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T15:51:51.497809Z",
     "iopub.status.busy": "2025-06-20T15:51:51.497418Z",
     "iopub.status.idle": "2025-06-20T15:51:51.502634Z",
     "shell.execute_reply": "2025-06-20T15:51:51.501601Z",
     "shell.execute_reply.started": "2025-06-20T15:51:51.497776Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"captureData\": {\n",
      "    \"endpointInput\": {\n",
      "      \"observedContentType\": \"text/csv\",\n",
      "      \"mode\": \"INPUT\",\n",
      "      \"data\": \"-0.19479093229520963,-0.2609322836255467,-0.6774256966865404,-0.4963820637296466,-0.35624767122022005,-0.48767633072477057,-0.6714313942021038,1.0,0.0,0.0\\n\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    },\n",
      "    \"endpointOutput\": {\n",
      "      \"observedContentType\": \"text/csv; charset=utf-8\",\n",
      "      \"mode\": \"OUTPUT\",\n",
      "      \"data\": \"8.268270492553711\\n\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    }\n",
      "  },\n",
      "  \"eventMetadata\": {\n",
      "    \"eventId\": \"d1d745a4-6bad-4360-9df9-9f0b61ec0c99\",\n",
      "    \"inferenceTime\": \"2025-06-20T15:31:32Z\"\n",
      "  },\n",
      "  \"eventVersion\": \"0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#print-json-file\n",
    "print(json.dumps(json.loads(capture_file.split(\"\\n\")[0]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enabling Data Capture on your endpoints gives you more flexibility when you want to save information for training, debugging, and monitoring. Since Amazon SageMaker Model Monitor parses this captured data automatically, using Data Capture helps you compare new records to baseline data. \n",
    "\n",
    "You have not configured a baseline yet. In the next task, you use SageMaker Model Monitor to generate baseline statistics and constraints. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.4: Generate baseline statistics and constraints\n",
    "\n",
    "In this task, you create a baseline. Baseline statistics and constraints serve as a standard for detecting data drift and other data quality issues. \n",
    "\n",
    "The test dataset from training the model is often a good baseline dataset. The test dataset schema and the inference dataset schema should exactly match, including the number and order of the features. From the test dataset, you can ask Amazon SageMaker to suggest a set of baseline constraints and generate descriptive statistics to explore the data.\n",
    "\n",
    "First, configure the baseline prefixes and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T15:52:26.100415Z",
     "iopub.status.busy": "2025-06-20T15:52:26.099718Z",
     "iopub.status.idle": "2025-06-20T15:52:26.105747Z",
     "shell.execute_reply": "2025-06-20T15:52:26.104792Z",
     "shell.execute_reply.started": "2025-06-20T15:52:26.100383Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline data uri: s3://sagemaker-us-west-2-701139230119/sagemaker/abalone/baselining/data\n",
      "Baseline results uri: s3://sagemaker-us-west-2-701139230119/sagemaker/abalone/baselining/results\n"
     ]
    }
   ],
   "source": [
    "#configure-baseline-variables\n",
    "baseline_prefix = prefix + \"/baselining\"\n",
    "baseline_data_prefix = baseline_prefix + \"/data\"\n",
    "baseline_results_prefix = baseline_prefix + \"/results\"\n",
    "\n",
    "baseline_data_uri = \"s3://{}/{}\".format(bucket, baseline_data_prefix)\n",
    "baseline_results_uri = \"s3://{}/{}\".format(bucket, baseline_results_prefix)\n",
    "print(\"Baseline data uri: {}\".format(baseline_data_uri))\n",
    "print(\"Baseline results uri: {}\".format(baseline_results_uri))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, start a job to suggest a baseline and constraints. *DefaultModelMonitor.suggest_baseline()* starts a **ProcessingJob** using an Amazon SageMaker provided Model Monitor container to generate the baseline and constraints.\n",
    "\n",
    "<i class=\"fas fa-sticky-note\" style=\"color:#ff6633\"></i> **Note:** A baseline job takes approximately 10 minutes to complete.\n",
    "\n",
    "<i class=\"fas fa-info-circle\" style=\"color:#008296\"></i> **Learn more:** Refer to [Create a Baseline](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-create-baseline.html) for more information about creating baseline calculations of statistics and constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T15:52:37.227835Z",
     "iopub.status.busy": "2025-06-20T15:52:37.227137Z",
     "iopub.status.idle": "2025-06-20T15:58:01.877922Z",
     "shell.execute_reply": "2025-06-20T15:58:01.834610Z",
     "shell.execute_reply.started": "2025-06-20T15:52:37.227804Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2025-06-20-15-52-37-574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................\u001b[34m2025-06-20 15:55:18.993567: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:18.993714: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:21.049734: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:21.049776: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:21.049803: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-220-24.us-west-2.compute.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:21.050158: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:23,134 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:701139230119:processing-job/baseline-suggestion-job-2025-06-20-15-52-37-574', 'ProcessingJobName': 'baseline-suggestion-job-2025-06-20-15-52-37-574', 'Environment': {'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '159807026194.dkr.ecr.us-west-2.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-west-2-701139230119/model-monitor/baselining/baseline-suggestion-job-2025-06-20-15-52-37-574/input/baseline_dataset_input', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-west-2-701139230119/sagemaker/abalone/baselining/results', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.t3.xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::701139230119:role/LabVPC-notebook-role', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:23,134 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:23,134 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:23,134 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:23,134 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:23,135 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:23,199 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:23,200 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:23,200 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.t3.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.t3.xlarge', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0', 'topology': None}\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:23,210 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:23,210 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:23,211 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:23,771 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.220.24\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/h\u001b[0m\n",
      "\u001b[34madoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_392\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:23,784 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:23,788 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-a9eea9a4-5ee3-47b3-ba1f-e9e94e6e0a95\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,382 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,396 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,397 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,400 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,406 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,406 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,406 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,406 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,445 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,459 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,460 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,464 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,468 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Jun 20 15:55:24\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,470 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,470 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,472 INFO util.GSet: 2.0% max memory 3.1 GB = 63.9 MB\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,472 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,553 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,557 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,558 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,558 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,558 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,558 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,558 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,558 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,558 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,558 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,558 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,558 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,589 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,589 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,589 INFO util.GSet: 1.0% max memory 3.1 GB = 32.0 MB\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,590 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,592 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,592 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,592 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,592 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,598 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,602 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,602 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,603 INFO util.GSet: 0.25% max memory 3.1 GB = 8.0 MB\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,603 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,610 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,610 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,610 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,614 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,615 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,616 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,617 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,617 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 982.0 KB\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,617 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,643 INFO namenode.FSImage: Allocated new BlockPoolId: BP-517301070-10.0.220.24-1750434924635\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,659 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,669 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,766 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 386 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,784 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,789 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.220.24\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:24,818 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:26,898 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:26,898 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:29,064 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:29,064 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:31,186 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:31,186 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:33,640 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:33,640 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:35,885 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:35,887 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:45,897 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:48,354 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:49,021 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:49,076 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:49,090 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:49,848 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:49,887 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:49,887 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:49,887 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:49,888 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:49,918 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11588, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:49,934 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:49,938 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:50,016 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:50,017 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:50,018 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:50,018 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:50,018 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:50,541 INFO util.Utils: Successfully started service 'sparkDriver' on port 38801.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:50,578 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:50,631 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:50,654 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:50,655 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:50,718 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:50,748 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-9c77ff6a-d48d-4e51-a8a3-354aeba27735\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:50,772 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:50,825 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:50,875 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.0.220.24:38801/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1750434949843\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:51,525 INFO client.RMProxy: Connecting to ResourceManager at /10.0.220.24:8032\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:52,798 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:52,799 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:52,810 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15802 MB per container)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:52,811 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:52,811 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:52,812 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:52,819 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:52,933 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:55,433 INFO yarn.Client: Uploading resource file:/tmp/spark-a495a5fc-fcdb-4dd5-9272-e7c318acb354/__spark_libs__1400389534988393901.zip -> hdfs://10.0.220.24/user/root/.sparkStaging/application_1750434931174_0001/__spark_libs__1400389534988393901.zip\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:56,973 INFO yarn.Client: Uploading resource file:/tmp/spark-a495a5fc-fcdb-4dd5-9272-e7c318acb354/__spark_conf__9053503962466163238.zip -> hdfs://10.0.220.24/user/root/.sparkStaging/application_1750434931174_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:57,444 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:57,445 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:57,445 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:57,445 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:57,445 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:57,502 INFO yarn.Client: Submitting application application_1750434931174_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:57,797 INFO impl.YarnClientImpl: Submitted application application_1750434931174_0001\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:58,807 INFO yarn.Client: Application report for application_1750434931174_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:58,818 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Fri Jun 20 15:55:58 +0000 2025] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1750434957656\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1750434931174_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-06-20 15:55:59,823 INFO yarn.Client: Application report for application_1750434931174_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:00,828 INFO yarn.Client: Application report for application_1750434931174_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:01,833 INFO yarn.Client: Application report for application_1750434931174_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:02,839 INFO yarn.Client: Application report for application_1750434931174_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:03,844 INFO yarn.Client: Application report for application_1750434931174_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:04,848 INFO yarn.Client: Application report for application_1750434931174_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:04,849 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.220.24\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1750434957656\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1750434931174_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:04,859 INFO cluster.YarnClientSchedulerBackend: Application application_1750434931174_0001 has started running.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:04,878 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46193.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:04,878 INFO netty.NettyBlockTransferService: Server created on 10.0.220.24:46193\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:04,884 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:04,894 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.220.24, 46193, None)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:04,900 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.220.24:46193 with 1458.6 MiB RAM, BlockManagerId(driver, 10.0.220.24, 46193, None)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:04,906 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.220.24, 46193, None)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:04,907 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.220.24, 46193, None)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:05,066 INFO util.log: Logging initialized @18814ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:05,069 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1750434931174_0001), /proxy/application_1750434931174_0001\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:07,205 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:11,845 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.220.24:45164) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:12,135 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:46605 with 5.9 GiB RAM, BlockManagerId(1, algo-1, 46605, None)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:21,398 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:21,659 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:21,738 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:21,745 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:23,325 INFO datasources.InMemoryFileIndex: It took 56 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:23,551 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:24,000 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.3 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:24,005 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.220.24:46193 (size: 39.3 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:24,015 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:24,489 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:24,493 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:24,498 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 11672\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:24,567 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:24,595 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:24,595 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:24,596 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:24,599 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:24,608 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:24,683 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:24,688 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:24,688 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.220.24:46193 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:24,690 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:24,711 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:24,712 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:24,772 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4640 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:25,079 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:46605 (size: 4.2 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:26,019 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:46605 (size: 39.3 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:26,456 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1704 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:26,459 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:26,467 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 1.817 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:26,472 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:26,473 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:26,476 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 1.907379 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:26,684 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.220.24:46193 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:26,691 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:46605 in memory (size: 4.2 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:29,802 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:29,804 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:29,809 INFO datasources.FileSourceStrategy: Output Data Schema: struct<rings: string, length: string, diameter: string, height: string, whole_weight: string ... 9 more fields>\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:30,115 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:30,128 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:30,129 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.220.24:46193 (size: 39.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:30,131 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:30,151 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:30,201 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:30,203 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:30,203 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:30,203 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:30,207 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:30,209 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:30,287 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 16.8 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:30,293 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:30,294 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.220.24:46193 (size: 8.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:30,295 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:30,296 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:30,297 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:30,302 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4968 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:30,362 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:46605 (size: 8.0 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:31,420 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:46605 (size: 39.1 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:31,575 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:46605 (size: 13.0 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:31,800 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1502 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:31,800 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:31,801 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 1.587 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:31,809 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:31,809 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:31,810 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 1.608121 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:32,013 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:46605 in memory (size: 8.0 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:32,048 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.220.24:46193 in memory (size: 8.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:32,318 INFO codegen.CodeGenerator: Code generated in 409.086777 ms\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:33,058 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:33,239 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:33,244 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:33,245 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:33,245 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:33,248 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:33,251 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:33,275 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 113.9 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:33,278 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:33,279 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.220.24:46193 (size: 34.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:33,281 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:33,284 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:33,284 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:33,293 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:33,328 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:46605 (size: 34.8 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:34,959 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1668 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:34,962 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:34,967 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 1.711 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:34,970 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:34,971 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:34,972 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:34,972 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:35,115 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:35,120 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:35,120 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:35,120 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:35,120 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:35,125 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:35,157 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 166.5 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:35,160 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 45.9 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:35,161 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.220.24:46193 (size: 45.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:35,163 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:35,164 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:35,164 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:35,169 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:35,205 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:46605 (size: 45.9 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:35,319 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.220.24:45164\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:35,953 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 785 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:35,953 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:35,954 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.818 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:35,955 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:35,955 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:35,955 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.839692 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:36,005 INFO codegen.CodeGenerator: Code generated in 32.493899 ms\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:36,449 INFO codegen.CodeGenerator: Code generated in 48.207826 ms\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:36,559 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:36,561 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:36,561 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:36,561 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:36,564 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:36,565 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:36,631 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 37.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:36,633 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 16.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:36,635 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.220.24:46193 (size: 16.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:36,636 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:36,637 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:36,638 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:36,641 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4968 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:36,667 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:46605 (size: 16.2 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,024 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 384 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,025 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,026 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 0.458 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,026 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,026 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,027 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 0.467756 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,627 INFO codegen.CodeGenerator: Code generated in 130.124909 ms\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,638 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,638 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,638 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,638 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,639 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,640 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,650 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 73.6 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,654 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 23.6 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,655 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.220.24:46193 (size: 23.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,656 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,657 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,657 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,659 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,685 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:46605 (size: 23.6 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,810 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 151 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,810 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,811 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.168 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,812 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,812 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,812 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:37,812 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,138 INFO codegen.CodeGenerator: Code generated in 160.77184 ms\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,158 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,160 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,160 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,161 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,161 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,162 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,166 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 66.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,169 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,171 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.220.24:46193 (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,172 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,173 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,173 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,175 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,198 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:46605 (size: 19.4 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,208 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.220.24:45164\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,320 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 143 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,321 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,324 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.161 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,326 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,326 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,327 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.168771 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,535 INFO codegen.CodeGenerator: Code generated in 155.167514 ms\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,737 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,743 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,744 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,744 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,744 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,744 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,749 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,792 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 30.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,798 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 13.8 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,800 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.220.24:46193 (size: 13.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,801 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,802 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,802 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,807 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:38,849 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:46605 (size: 13.8 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,767 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 1960 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,767 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,769 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 2.018 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,769 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,769 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,769 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,769 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,770 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,773 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,818 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,821 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.220.24:46193 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,826 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,828 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,828 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:46605 in memory (size: 23.6 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,839 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,846 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,858 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.220.24:46193 in memory (size: 23.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,881 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:46605 (size: 3.0 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,896 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.220.24:45164\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,922 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.220.24:46193 in memory (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,930 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:46605 in memory (size: 19.4 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,982 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 138 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,983 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,984 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.214 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,989 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,989 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.220.24:46193 in memory (size: 16.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,989 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,990 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 2.253194 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:40,993 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:46605 in memory (size: 16.2 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,029 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.220.24:46193 in memory (size: 34.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,045 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:46605 in memory (size: 34.8 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,087 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.220.24:46193 in memory (size: 45.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,099 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:46605 in memory (size: 45.9 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,347 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,347 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,348 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,348 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,349 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,351 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,359 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 82.9 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,361 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 27.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,362 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.220.24:46193 (size: 27.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,363 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,364 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,364 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,366 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,381 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:46605 (size: 27.1 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,584 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 218 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,584 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,592 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.240 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,595 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,595 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,596 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,596 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,698 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,703 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,703 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,703 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,703 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,705 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,714 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 167.7 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,717 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 45.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,718 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.220.24:46193 (size: 45.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,719 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,719 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,719 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,722 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,738 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:46605 (size: 45.9 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,757 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.220.24:45164\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,966 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 245 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,966 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,968 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.262 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,969 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,972 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:41,973 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.274971 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,223 INFO codegen.CodeGenerator: Code generated in 34.057688 ms\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,264 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,266 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,267 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,267 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,268 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,269 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,297 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 37.5 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,299 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 16.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,300 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.220.24:46193 (size: 16.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,300 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,301 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,301 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,303 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4968 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,323 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:46605 (size: 16.2 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,391 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 89 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,391 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,393 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 0.122 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,393 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,394 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,394 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 0.129542 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,811 INFO codegen.CodeGenerator: Code generated in 78.840116 ms\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,820 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,820 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,820 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,821 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,821 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,822 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,827 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 74.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,830 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,831 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.220.24:46193 (size: 24.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,831 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,833 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,833 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,838 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,864 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:46605 (size: 24.0 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,949 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 113 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,949 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,950 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.127 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,950 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,950 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,950 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:42,950 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,164 INFO codegen.CodeGenerator: Code generated in 130.048117 ms\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,187 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,188 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,192 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,193 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,196 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,197 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,201 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 66.8 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,209 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.7 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,209 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.220.24:46193 (size: 19.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,210 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,211 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,211 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,212 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,227 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:46605 (size: 19.7 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,233 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.220.24:45164\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,343 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 131 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,345 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,346 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.146 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,346 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,346 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,347 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.159882 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,478 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,480 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,481 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,481 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,482 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,482 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,483 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,490 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 30.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,493 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 13.8 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,493 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.220.24:46193 (size: 13.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,494 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,495 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,495 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,497 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,511 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:46605 (size: 13.8 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,564 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 67 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,564 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,565 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.080 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,565 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,565 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,566 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,566 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,566 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,568 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,570 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,571 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.220.24:46193 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,572 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,573 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,573 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,575 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,590 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:46605 (size: 3.0 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,594 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.220.24:45164\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,628 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 53 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,628 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,629 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.062 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,632 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,634 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,634 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.155882 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,706 INFO scheduler.DAGScheduler: Registering RDD 86 (collect at AnalysisRunner.scala:326) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,706 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,706 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,706 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,707 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,708 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,712 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 41.7 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,714 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,715 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.220.24:46193 (size: 17.6 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,716 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,717 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,717 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,719 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,736 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:46605 (size: 17.6 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,909 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 190 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,909 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,910 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326) finished in 0.201 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,912 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,912 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,913 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,913 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,948 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,951 INFO scheduler.DAGScheduler: Got job 15 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,952 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,953 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,953 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,956 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,963 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 62.0 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,967 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 23.0 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,968 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.220.24:46193 (size: 23.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,970 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,971 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,972 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,974 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:43,989 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:46605 (size: 23.0 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,000 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.220.24:45164\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,094 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 121 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,094 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,097 INFO scheduler.DAGScheduler: ResultStage 22 (collect at AnalysisRunner.scala:326) finished in 0.138 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,098 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,098 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,099 INFO scheduler.DAGScheduler: Job 15 finished: collect at AnalysisRunner.scala:326, took 0.149567 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,133 INFO codegen.CodeGenerator: Code generated in 32.030081 ms\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,208 INFO codegen.CodeGenerator: Code generated in 18.050618 ms\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,248 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,250 INFO scheduler.DAGScheduler: Got job 16 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,251 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,251 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,253 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,254 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,262 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 33.1 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,267 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 14.9 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,272 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.220.24:46193 (size: 14.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,276 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,277 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,278 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,280 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 18) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4968 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,297 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:46605 (size: 14.9 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,361 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 18) in 82 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,362 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,363 INFO scheduler.DAGScheduler: ResultStage 23 (treeReduce at KLLRunner.scala:107) finished in 0.108 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,363 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,363 INFO cluster.YarnScheduler: Killing all running tasks in stage 23: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,364 INFO scheduler.DAGScheduler: Job 16 finished: treeReduce at KLLRunner.scala:107, took 0.114701 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,516 INFO codegen.CodeGenerator: Code generated in 49.288478 ms\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,534 INFO scheduler.DAGScheduler: Registering RDD 104 (collect at AnalysisRunner.scala:326) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,535 INFO scheduler.DAGScheduler: Got map stage job 17 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,535 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,536 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,537 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,537 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,541 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 32.5 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,544 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 13.2 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,545 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.220.24:46193 (size: 13.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,546 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,554 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,555 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,556 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 19) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,574 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:46605 (size: 13.2 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,636 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 19) in 80 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,636 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,637 INFO scheduler.DAGScheduler: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326) finished in 0.099 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,637 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,637 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,637 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,637 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,678 INFO codegen.CodeGenerator: Code generated in 19.249715 ms\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,690 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,691 INFO scheduler.DAGScheduler: Got job 18 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,691 INFO scheduler.DAGScheduler: Final stage: ResultStage 26 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,691 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,691 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,692 INFO scheduler.DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,694 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 21.4 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,696 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,696 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.220.24:46193 (size: 8.5 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,697 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,698 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,698 INFO cluster.YarnScheduler: Adding task set 26.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,699 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,714 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:46605 (size: 8.5 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,724 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.220.24:45164\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,805 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 106 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,806 INFO cluster.YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,807 INFO scheduler.DAGScheduler: ResultStage 26 (collect at AnalysisRunner.scala:326) finished in 0.115 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,807 INFO scheduler.DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,807 INFO cluster.YarnScheduler: Killing all running tasks in stage 26: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,808 INFO scheduler.DAGScheduler: Job 18 finished: collect at AnalysisRunner.scala:326, took 0.117697 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:44,853 INFO codegen.CodeGenerator: Code generated in 38.950433 ms\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,000 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,002 INFO scheduler.DAGScheduler: Registering RDD 115 (countByKey at ColumnProfiler.scala:592) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,006 INFO scheduler.DAGScheduler: Got job 19 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,007 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,007 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,009 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,012 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,021 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:46605 in memory (size: 3.0 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,034 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.220.24:46193 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,077 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 29.5 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,078 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:46605 in memory (size: 13.2 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,082 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 13.7 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,084 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.220.24:46193 (size: 13.7 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,085 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,085 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,086 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,094 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.220.24:46193 in memory (size: 13.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,102 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,118 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.220.24:46193 in memory (size: 45.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,133 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:46605 in memory (size: 45.9 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,137 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:46605 (size: 13.7 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,167 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.220.24:46193 in memory (size: 13.8 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,180 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:46605 in memory (size: 13.8 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,220 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.220.24:46193 in memory (size: 14.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,224 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 130 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,224 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,225 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (countByKey at ColumnProfiler.scala:592) finished in 0.193 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,225 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,225 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,225 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 28)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,225 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,225 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,230 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 5.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,233 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,234 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:46605 in memory (size: 14.9 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,235 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.220.24:46193 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,237 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,237 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,238 INFO cluster.YarnScheduler: Adding task set 28.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,240 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,264 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:46605 (size: 3.0 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,271 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.0.220.24:45164\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,292 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.220.24:46193 in memory (size: 13.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,294 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 55 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,295 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,296 INFO scheduler.DAGScheduler: ResultStage 28 (countByKey at ColumnProfiler.scala:592) finished in 0.070 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,297 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,298 INFO cluster.YarnScheduler: Killing all running tasks in stage 28: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,299 INFO scheduler.DAGScheduler: Job 19 finished: countByKey at ColumnProfiler.scala:592, took 0.298331 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,301 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:46605 in memory (size: 13.8 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,343 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:46605 in memory (size: 27.1 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,358 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.220.24:46193 in memory (size: 27.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,374 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:46605 in memory (size: 3.0 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,388 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.220.24:46193 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,408 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:46605 in memory (size: 24.0 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,416 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.220.24:46193 in memory (size: 24.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,438 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.220.24:46193 in memory (size: 19.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,442 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:46605 in memory (size: 19.7 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,475 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.220.24:46193 in memory (size: 23.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,486 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:46605 in memory (size: 23.0 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,537 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.220.24:46193 in memory (size: 16.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,550 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:46605 in memory (size: 16.2 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,603 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.220.24:46193 in memory (size: 17.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,604 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:46605 in memory (size: 17.6 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,666 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:46605 in memory (size: 8.5 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,674 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.220.24:46193 in memory (size: 8.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,750 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,832 INFO codegen.CodeGenerator: Code generated in 15.752516 ms\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,842 INFO scheduler.DAGScheduler: Registering RDD 121 (count at StatsGenerator.scala:66) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,845 INFO scheduler.DAGScheduler: Got map stage job 20 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,845 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,846 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,850 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,851 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[121] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,861 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 22.0 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,865 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,867 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.220.24:46193 (size: 10.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,869 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,870 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[121] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,871 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,877 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,900 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:46605 (size: 10.2 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,962 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 85 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,962 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,963 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (count at StatsGenerator.scala:66) finished in 0.111 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,963 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,964 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,964 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:45,964 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,004 INFO codegen.CodeGenerator: Code generated in 23.045514 ms\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,029 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,031 INFO scheduler.DAGScheduler: Got job 21 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,031 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,031 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,032 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,032 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[124] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,035 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 11.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,037 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,038 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.220.24:46193 (size: 5.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,039 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,040 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[124] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,040 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,042 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 24) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,059 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:46605 (size: 5.5 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,067 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.0.220.24:45164\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,111 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 24) in 69 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,112 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,113 INFO scheduler.DAGScheduler: ResultStage 31 (count at StatsGenerator.scala:66) finished in 0.078 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,113 INFO scheduler.DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,114 INFO cluster.YarnScheduler: Killing all running tasks in stage 31: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,115 INFO scheduler.DAGScheduler: Job 21 finished: count at StatsGenerator.scala:66, took 0.085151 s\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,405 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,441 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,477 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,478 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,492 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,516 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,545 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,546 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,550 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,578 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,611 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,612 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,613 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,620 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,621 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-a495a5fc-fcdb-4dd5-9272-e7c318acb354\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,628 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-0089854c-a09d-4700-ae6d-1d0210d2014c\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,720 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2025-06-20 15:56:46,721 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.processing.ProcessingJob at 0x7febee4f8230>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create-baselining-job\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.t3.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=\"data/abalone_data_new_withheader.csv\",\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i class=\"fas fa-sticky-note\" style=\"color:#ec7211\"></i> **NOTE:** This code returns a lengthy response. You can ignore any warnings or error messages.\n",
    "\n",
    "When the cell completes, a message is returned that looks like *2025-10-11 12:13:14,156 - DefaultDataAnalyzer - INFO - Spark job completed*.\n",
    "\n",
    "Now, search for the *constraints.json* and *statistics.json* files to see where they are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T15:59:09.147507Z",
     "iopub.status.busy": "2025-06-20T15:59:09.147028Z",
     "iopub.status.idle": "2025-06-20T15:59:09.369144Z",
     "shell.execute_reply": "2025-06-20T15:59:09.365081Z",
     "shell.execute_reply.started": "2025-06-20T15:59:09.147477Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Files:\n",
      "sagemaker/abalone/baselining/results/constraints.json\n",
      " sagemaker/abalone/baselining/results/statistics.json\n"
     ]
    }
   ],
   "source": [
    "#explore-generated-constraints-and-statistics\n",
    "s3_client = boto3.Session().client(\"s3\")\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=baseline_results_prefix)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get(\"Contents\")]\n",
    "print(\"Found Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, view the generated statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T15:59:12.949427Z",
     "iopub.status.busy": "2025-06-20T15:59:12.949140Z",
     "iopub.status.idle": "2025-06-20T15:59:13.153214Z",
     "shell.execute_reply": "2025-06-20T15:59:13.152455Z",
     "shell.execute_reply.started": "2025-06-20T15:59:12.949405Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>numerical_statistics.common.num_present</th>\n",
       "      <th>numerical_statistics.common.num_missing</th>\n",
       "      <th>numerical_statistics.mean</th>\n",
       "      <th>numerical_statistics.sum</th>\n",
       "      <th>numerical_statistics.std_dev</th>\n",
       "      <th>numerical_statistics.min</th>\n",
       "      <th>numerical_statistics.max</th>\n",
       "      <th>numerical_statistics.approximate_num_distinct_values</th>\n",
       "      <th>numerical_statistics.completeness</th>\n",
       "      <th>numerical_statistics.distribution.kll.buckets</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.c</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.k</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rings</td>\n",
       "      <td>Integral</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>9.991667e+00</td>\n",
       "      <td>1.199000e+03</td>\n",
       "      <td>2.378711</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 4.0, 'upper_bound': 5.3, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[11.0, 12.0, 10.0, 10.0, 13.0, 11.0, 11.0, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>length</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.500000e-11</td>\n",
       "      <td>-3.000000e-09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-3.132857</td>\n",
       "      <td>1.792725</td>\n",
       "      <td>60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': -3.13285749, 'upper_bound': -...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.842173735, 1.40386293, 1.187828624, 0.9285...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>diameter</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>1.666667e-11</td>\n",
       "      <td>2.000000e-09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-3.116738</td>\n",
       "      <td>1.574943</td>\n",
       "      <td>54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': -3.116738385, 'upper_bound': ...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.860991542, 1.523946529, 0.962984617, 0.962...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>height</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.326673e-17</td>\n",
       "      <td>-9.992007e-15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.590157</td>\n",
       "      <td>1.918424</td>\n",
       "      <td>29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': -2.590157076, 'upper_bound': ...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.688811003, 1.645176692, 1.098682012, 1.235...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>whole_weight</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>1.666665e-11</td>\n",
       "      <td>1.999998e-09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.777180</td>\n",
       "      <td>3.316178</td>\n",
       "      <td>119</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': -1.777179983, 'upper_bound': ...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.797789649, 2.556957875, 1.439731739, 1.895...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>shucked_weight</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>1.666675e-11</td>\n",
       "      <td>2.000010e-09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.864274</td>\n",
       "      <td>3.056230</td>\n",
       "      <td>119</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': -1.864273528, 'upper_bound': ...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.819637499, 2.065165072, 1.808023895, 1.861...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>viscera_weight</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.666667e-11</td>\n",
       "      <td>-2.000000e-09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.751453</td>\n",
       "      <td>2.736338</td>\n",
       "      <td>101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': -1.751452951, 'upper_bound': ...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.854509788, 2.736337747, 1.315742131, 1.800...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>shell_weight</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>1.666669e-11</td>\n",
       "      <td>2.000003e-09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.849493</td>\n",
       "      <td>2.974287</td>\n",
       "      <td>75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': -1.849493312, 'upper_bound': ...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.46480584, 2.137793179, 0.848198772, 0.8481...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sex_m</td>\n",
       "      <td>Integral</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>4.083333e-01</td>\n",
       "      <td>4.900000e+01</td>\n",
       "      <td>0.491525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sex_f</td>\n",
       "      <td>Integral</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>1.833333e-01</td>\n",
       "      <td>2.200000e+01</td>\n",
       "      <td>0.386940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name inferred_type  numerical_statistics.common.num_present  \\\n",
       "0           rings      Integral                                      120   \n",
       "1          length    Fractional                                      120   \n",
       "2        diameter    Fractional                                      120   \n",
       "3          height    Fractional                                      120   \n",
       "4    whole_weight    Fractional                                      120   \n",
       "5  shucked_weight    Fractional                                      120   \n",
       "6  viscera_weight    Fractional                                      120   \n",
       "7    shell_weight    Fractional                                      120   \n",
       "8           sex_m      Integral                                      120   \n",
       "9           sex_f      Integral                                      120   \n",
       "\n",
       "   numerical_statistics.common.num_missing  numerical_statistics.mean  \\\n",
       "0                                        0               9.991667e+00   \n",
       "1                                        0              -2.500000e-11   \n",
       "2                                        0               1.666667e-11   \n",
       "3                                        0              -8.326673e-17   \n",
       "4                                        0               1.666665e-11   \n",
       "5                                        0               1.666675e-11   \n",
       "6                                        0              -1.666667e-11   \n",
       "7                                        0               1.666669e-11   \n",
       "8                                        0               4.083333e-01   \n",
       "9                                        0               1.833333e-01   \n",
       "\n",
       "   numerical_statistics.sum  numerical_statistics.std_dev  \\\n",
       "0              1.199000e+03                      2.378711   \n",
       "1             -3.000000e-09                      1.000000   \n",
       "2              2.000000e-09                      1.000000   \n",
       "3             -9.992007e-15                      1.000000   \n",
       "4              1.999998e-09                      1.000000   \n",
       "5              2.000010e-09                      1.000000   \n",
       "6             -2.000000e-09                      1.000000   \n",
       "7              2.000003e-09                      1.000000   \n",
       "8              4.900000e+01                      0.491525   \n",
       "9              2.200000e+01                      0.386940   \n",
       "\n",
       "   numerical_statistics.min  numerical_statistics.max  \\\n",
       "0                  4.000000                 17.000000   \n",
       "1                 -3.132857                  1.792725   \n",
       "2                 -3.116738                  1.574943   \n",
       "3                 -2.590157                  1.918424   \n",
       "4                 -1.777180                  3.316178   \n",
       "5                 -1.864274                  3.056230   \n",
       "6                 -1.751453                  2.736338   \n",
       "7                 -1.849493                  2.974287   \n",
       "8                  0.000000                  1.000000   \n",
       "9                  0.000000                  1.000000   \n",
       "\n",
       "   numerical_statistics.approximate_num_distinct_values  \\\n",
       "0                                                 13      \n",
       "1                                                 60      \n",
       "2                                                 54      \n",
       "3                                                 29      \n",
       "4                                                119      \n",
       "5                                                119      \n",
       "6                                                101      \n",
       "7                                                 75      \n",
       "8                                                  2      \n",
       "9                                                  2      \n",
       "\n",
       "   numerical_statistics.completeness  \\\n",
       "0                                1.0   \n",
       "1                                1.0   \n",
       "2                                1.0   \n",
       "3                                1.0   \n",
       "4                                1.0   \n",
       "5                                1.0   \n",
       "6                                1.0   \n",
       "7                                1.0   \n",
       "8                                1.0   \n",
       "9                                1.0   \n",
       "\n",
       "       numerical_statistics.distribution.kll.buckets  \\\n",
       "0  [{'lower_bound': 4.0, 'upper_bound': 5.3, 'cou...   \n",
       "1  [{'lower_bound': -3.13285749, 'upper_bound': -...   \n",
       "2  [{'lower_bound': -3.116738385, 'upper_bound': ...   \n",
       "3  [{'lower_bound': -2.590157076, 'upper_bound': ...   \n",
       "4  [{'lower_bound': -1.777179983, 'upper_bound': ...   \n",
       "5  [{'lower_bound': -1.864273528, 'upper_bound': ...   \n",
       "6  [{'lower_bound': -1.751452951, 'upper_bound': ...   \n",
       "7  [{'lower_bound': -1.849493312, 'upper_bound': ...   \n",
       "8  [{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...   \n",
       "9  [{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...   \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.parameters.c  \\\n",
       "0                                               0.64           \n",
       "1                                               0.64           \n",
       "2                                               0.64           \n",
       "3                                               0.64           \n",
       "4                                               0.64           \n",
       "5                                               0.64           \n",
       "6                                               0.64           \n",
       "7                                               0.64           \n",
       "8                                               0.64           \n",
       "9                                               0.64           \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.parameters.k  \\\n",
       "0                                             2048.0           \n",
       "1                                             2048.0           \n",
       "2                                             2048.0           \n",
       "3                                             2048.0           \n",
       "4                                             2048.0           \n",
       "5                                             2048.0           \n",
       "6                                             2048.0           \n",
       "7                                             2048.0           \n",
       "8                                             2048.0           \n",
       "9                                             2048.0           \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.data  \n",
       "0  [[11.0, 12.0, 10.0, 10.0, 13.0, 11.0, 11.0, 10...  \n",
       "1  [[0.842173735, 1.40386293, 1.187828624, 0.9285...  \n",
       "2  [[0.860991542, 1.523946529, 0.962984617, 0.962...  \n",
       "3  [[0.688811003, 1.645176692, 1.098682012, 1.235...  \n",
       "4  [[0.797789649, 2.556957875, 1.439731739, 1.895...  \n",
       "5  [[0.819637499, 2.065165072, 1.808023895, 1.861...  \n",
       "6  [[0.854509788, 2.736337747, 1.315742131, 1.800...  \n",
       "7  [[0.46480584, 2.137793179, 0.848198772, 0.8481...  \n",
       "8  [[0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0,...  \n",
       "9  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view-statistics\n",
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistics table shows each feature with its corresponding summary statistics, including the mean, standard deviation, min, max, and other important details.\n",
    "\n",
    "Finally, view the generated constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T15:59:33.078470Z",
     "iopub.status.busy": "2025-06-20T15:59:33.078168Z",
     "iopub.status.idle": "2025-06-20T15:59:33.192235Z",
     "shell.execute_reply": "2025-06-20T15:59:33.191442Z",
     "shell.execute_reply.started": "2025-06-20T15:59:33.078448Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>completeness</th>\n",
       "      <th>num_constraints.is_non_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rings</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>length</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>diameter</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>height</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>whole_weight</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>shucked_weight</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>viscera_weight</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>shell_weight</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sex_m</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sex_f</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name inferred_type  completeness  num_constraints.is_non_negative\n",
       "0           rings      Integral           1.0                             True\n",
       "1          length    Fractional           1.0                            False\n",
       "2        diameter    Fractional           1.0                            False\n",
       "3          height    Fractional           1.0                            False\n",
       "4    whole_weight    Fractional           1.0                            False\n",
       "5  shucked_weight    Fractional           1.0                            False\n",
       "6  viscera_weight    Fractional           1.0                            False\n",
       "7    shell_weight    Fractional           1.0                            False\n",
       "8           sex_m      Integral           1.0                             True\n",
       "9           sex_f      Integral           1.0                             True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view-constraints\n",
    "constraints_df = pd.json_normalize(\n",
    "    baseline_job.suggested_constraints().body_dict[\"features\"]\n",
    ")\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constraints table shows the inferred type of each feature, the record completeness (in this case 1.0 for all features because the file has no missing values), and the fields that have no non-negative values. \n",
    "\n",
    "Now that you have a baseline created and have viewed the statistics and constraints, create a Model Monitor data quality monitoring job to track new inference records against the baseline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.5: Create a Model Monitor data quality monitoring job\n",
    "\n",
    "After you create your baseline, you can call the *create_monitoring_schedule()* method of the *DefaultModelMonitor* class instance to schedule an hourly data quality monitor.\n",
    "\n",
    "In this task, you analyze and monitor the data with a data quality monitoring job.\n",
    "\n",
    "First, use the *create_monitoring_schedule()* method to schedule an hourly data quality monitoring schedule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T15:59:44.112849Z",
     "iopub.status.busy": "2025-06-20T15:59:44.112552Z",
     "iopub.status.idle": "2025-06-20T15:59:45.376508Z",
     "shell.execute_reply": "2025-06-20T15:59:45.375666Z",
     "shell.execute_reply.started": "2025-06-20T15:59:44.112822Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: model-monitor-schedule-2025-06-20-15-59-44\n"
     ]
    }
   ],
   "source": [
    "#create-monitoring-schedule\n",
    "bucket = boto3.Session().resource(\"s3\").Bucket(bucket)\n",
    "bucket.Object(code_prefix + \"/postprocessor.py\").upload_file(\"python/postprocessor.py\")\n",
    "\n",
    "mon_schedule_name = f\"model-monitor-schedule-{datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "\n",
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    endpoint_input=predictor.endpoint_name,\n",
    "    post_analytics_processor_script=s3_code_postprocessor_uri,\n",
    "    output_s3_uri=s3_report_path,\n",
    "    statistics=my_default_monitor.baseline_statistics(),\n",
    "    constraints=my_default_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, send some artificial traffic to the endpoint for the monitoring job to be able to generate the violations report. To simulate data drift, use a set of skewed data. The skewed data, when compared against the baseline, throws an alert with the automated alert triggering system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T16:00:05.771625Z",
     "iopub.status.busy": "2025-06-20T16:00:05.771131Z",
     "iopub.status.idle": "2025-06-20T16:01:48.740939Z",
     "shell.execute_reply": "2025-06-20T16:01:48.739889Z",
     "shell.execute_reply.started": "2025-06-20T16:00:05.771596Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................................................................................................................\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#send-artificial-traffic\n",
    "endpoint_name = predictor.endpoint_name\n",
    "runtime_client = sm_session.sagemaker_runtime_client\n",
    "limit = 200\n",
    "i = 0\n",
    "\n",
    "# repeating code from above to run this section independently\n",
    "def invoke_endpoint(ep_name, file_name, runtime_client):\n",
    "    i = 0\n",
    "    with open(file_name, \"r\") as f:\n",
    "        for row in f:\n",
    "            (label, payload) = row.strip(\"\\n\").split(\",\", 1)  \n",
    "\n",
    "            response = runtime_client.invoke_endpoint(\n",
    "                EndpointName=ep_name, ContentType=\"text/csv\", Body=payload\n",
    "            )\n",
    "            response[\"Body\"].read()\n",
    "            i += 1\n",
    "            if i > limit:\n",
    "                break\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "\n",
    "invoke_endpoint(endpoint_name, \"data/abalone_data_skewed.csv\", runtime_client)\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use *describe_schedule* to view the schedule you just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T16:01:48.743089Z",
     "iopub.status.busy": "2025-06-20T16:01:48.742446Z",
     "iopub.status.idle": "2025-06-20T16:01:48.883411Z",
     "shell.execute_reply": "2025-06-20T16:01:48.882321Z",
     "shell.execute_reply.started": "2025-06-20T16:01:48.743040Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule status: Scheduled\n"
     ]
    }
   ],
   "source": [
    "#model-monitor-schedule-status\n",
    "desc_schedule_result = my_default_monitor.describe_schedule()\n",
    "print(\"Schedule status: {}\".format(desc_schedule_result[\"MonitoringScheduleStatus\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The monitor schedule starts jobs at the previously specified hourly interval. Even for an hourly schedule, Amazon SageMaker has a buffer period of 20 minutes to schedule your execution. You might see your execution start anywhere from 0 to 20 minutes from the hour boundary. This is expected and done for load balancing in the backend.\n",
    "\n",
    "This execution takes approximately one hour to be able to generate the violations report. For the purpose of the lab, the next cells have code snippets for you to view and sample output is shared for reference. In the last step of this task, you view the violations report from a file that was generated and pre-loaded from an earlier monitoring run.\n",
    "\n",
    "When the execution finishes, SageMaker reports the status of the latest completed or failed execution. \n",
    "\n",
    "Here are the possible terminal states:\n",
    "- **Completed** - The monitoring execution completed and no issues were found in the violations report. \n",
    "- **CompletedWithViolations** - The execution completed, but constraint violations were detected. \n",
    "- **Failed** - The monitoring execution failed, maybe due to client error (perhaps incorrect role permissions) or infrastructure issues. Further examination of FailureReason and ExitMessage is necessary to identify what exactly happened. \n",
    "- **Stopped** - The job exceeded max runtime or was manually stopped.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to list and view the current status of an execution, you can use code similar to this:\n",
    "\n",
    "```python \n",
    "# list the current execution\n",
    "mon_executions = my_default_monitor.list_executions()\n",
    "print(\n",
    "    \"We created a hourly schedule above that begins executions ON the hour (plus 0-20 min buffer.\\nWe will have to wait for an hour...\"\n",
    ")\n",
    "\n",
    "while len(mon_executions) == 0:\n",
    "    print(\"Waiting for the first execution to happen...\")\n",
    "    time.sleep(60)\n",
    "    mon_executions = my_default_monitor.list_executions()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "# Latest execution status\n",
    "latest_execution = mon_executions[-1]  # Latest execution's index is -1, second to last is -2, etc\n",
    "time.sleep(60)\n",
    "latest_execution.wait(logs=False)\n",
    "\n",
    "print(\"Latest execution status: {}\".format(latest_execution.describe()[\"ProcessingJobStatus\"]))\n",
    "print(\"Latest execution result: {}\".format(latest_execution.describe()[\"ExitMessage\"]))\n",
    "\n",
    "latest_job = latest_execution.describe()\n",
    "if latest_job[\"ProcessingJobStatus\"] != \"Completed\":\n",
    "    print(\n",
    "        \"====STOP==== \\n No completed executions to inspect further. Please wait till an execution completes or investigate previously reported failures.\"\n",
    "    )\n",
    "```\n",
    "\n",
    "The following is the expected output when the latest execution of the monitoring job completes.\n",
    "\n",
    "```bash\n",
    "!Latest execution status: Completed\n",
    "\n",
    "Latest execution result: CompletedWithViolations: Job completed successfully with 8 violations.\n",
    "```\n",
    "\n",
    "To list the generated violation report, you can use code similar to this:\n",
    "\n",
    "```python\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "report_uri = latest_execution.output.destination\n",
    "s3uri = urlparse(report_uri)\n",
    "report_bucket = s3uri.netloc\n",
    "report_key = s3uri.path.lstrip(\"/\")\n",
    "s3_client = boto3.Session().client(\"s3\")\n",
    "result = s3_client.list_objects(Bucket=report_bucket, Prefix=report_key)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get(\"Contents\")]\n",
    "print(\"Found Report Files:\")\n",
    "print(\"\\n \".join(report_files))\n",
    "```\n",
    "\n",
    "The following is the listing the report files.\n",
    "\n",
    "```bash\n",
    "Found Report Files:\n",
    "sagemaker/abalone/reports/Abalone-2023-09-20-17-05-28/model-monitor-schedule-2023-09-20-17-41-22/2023/09/20/18/constraint_violations.json\n",
    "\n",
    "sagemaker/abalone/reports/Abalone-2023-09-20-17-05-28/model-monitor-schedule-2023-09-20-17-41-22/2023/09/20/18/constraints.json\n",
    "\n",
    "sagemaker/abalone/reports/Abalone-2023-09-20-17-05-28/model-monitor-schedule-2023-09-20-17-41-22/2023/09/20/18/statistics.json\n",
    "```\n",
    "\n",
    "To list violations compare to the baseline, you can use code similar to this:\n",
    "\n",
    "```python\n",
    "violations = my_default_monitor.latest_monitoring_constraint_violations()\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "constraints_df = pd.json_normalize(violations.body_dict[\"violations\"])\n",
    "constraints_df.head(10)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the execution of the monitoring job you started above will not finish for 60-80 minutes, view a violations report from a file that was generated and pre-loaded from an earlier monitoring run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-20T15:31:34.632673Z",
     "iopub.status.idle": "2025-06-20T15:31:34.633048Z",
     "shell.execute_reply": "2025-06-20T15:31:34.632864Z",
     "shell.execute_reply.started": "2025-06-20T15:31:34.632848Z"
    }
   },
   "outputs": [],
   "source": [
    "#print-violations-report\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "violations = json.load(open('data/violations.json'))\n",
    "constraints_df=pd.json_normalize(violations, record_path=['violations'])\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The violations report shows eight violations from the skewed data file, with four **data_type_check** violations for **rings**, **sex_f**, **sex_i**, and **sex_m**, and four **baseline_drift_check** violations for **length**, **diameter**, **whole_weight**, and **shell_weight**.\n",
    "\n",
    "Take a moment to view the description for each violation. Notice that the data type matches were not correct for some of the features. Also, notice that the baseline drift distance exceeded the *0.1* threshold for the four reported features.\n",
    "\n",
    "<i class=\"fas fa-sticky-note\" style=\"color:#ff6633\" aria-hidden=\"true\"></i> **Note:** When you created a baseline job, it produced constraints.json and statistics.json files. In the *constraints.json* file, the *comparison_threshold* is set to *0.1* by default. To learn more about constraints.json file, refer [Schema for Constraints](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-constraints.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.6: Create a CloudWatch alarm\n",
    "\n",
    "When data drift happens, it is helpful to get notifications so you can address any issues. A notification or alarm can also trigger automatic model retraining to address changes that might be occurring with your inference data.\n",
    "\n",
    "In this task, you learn how to create alarm and enable notifications to know when data drifts away from baseline data.\n",
    "\n",
    "First, find your current Amazon Simple Notification Service(Amazon SNS) topics by using **list_topics**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T17:25:20.064635Z",
     "iopub.status.busy": "2025-06-20T17:25:20.064239Z",
     "iopub.status.idle": "2025-06-20T17:25:20.186848Z",
     "shell.execute_reply": "2025-06-20T17:25:20.186008Z",
     "shell.execute_reply.started": "2025-06-20T17:25:20.064607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Topics': [{'TopicArn': 'arn:aws:sns:us-west-2:701139230119:MetricAlertSNSTopic'}], 'ResponseMetadata': {'RequestId': 'd68ad527-f79f-5c04-872c-b5e86c5f8d23', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'd68ad527-f79f-5c04-872c-b5e86c5f8d23', 'date': 'Fri, 20 Jun 2025 17:25:20 GMT', 'content-type': 'text/xml', 'content-length': '384', 'connection': 'keep-alive'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "#list-topics\n",
    "client = boto3.client('sns')\n",
    "topics_list= client.list_topics()\n",
    "print(topics_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set the **sns_notifications_topic** variable with the topic ARN value you found in the prior cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T17:25:24.041120Z",
     "iopub.status.busy": "2025-06-20T17:25:24.040593Z",
     "iopub.status.idle": "2025-06-20T17:25:24.046998Z",
     "shell.execute_reply": "2025-06-20T17:25:24.046129Z",
     "shell.execute_reply.started": "2025-06-20T17:25:24.041010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sns:us-west-2:701139230119:MetricAlertSNSTopic\n"
     ]
    }
   ],
   "source": [
    "#set-variables\n",
    "topic_details = pd.json_normalize(topics_list['Topics'])\n",
    "topic_arn = topic_details['TopicArn']\n",
    "print (topic_arn[0])\n",
    "sns_notifications_topic = topic_arn[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create an alarm using **put_metric_alarm** that triggers a notification when the feature diameter drifts away from the baseline and retrain the model automatically. \n",
    "\n",
    "You use the built-in Amazon SageMaker Model Monitor container for CloudWatch metrics. SageMaker emits the metrics for each feature observed in the dataset in the */aws/sagemaker/Endpoints/data-metric* namespace with *EndpointName* and *ScheduleName* dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T17:25:27.195096Z",
     "iopub.status.busy": "2025-06-20T17:25:27.194732Z",
     "iopub.status.idle": "2025-06-20T17:25:27.416088Z",
     "shell.execute_reply": "2025-06-20T17:25:27.414872Z",
     "shell.execute_reply.started": "2025-06-20T17:25:27.195072Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '8ec46623-759d-4100-8c22-3bd6b895f34e',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '8ec46623-759d-4100-8c22-3bd6b895f34e',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '214',\n",
       "   'date': 'Fri, 20 Jun 2025 17:25:27 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trigger-cloudwatch-alarm-when-it-drifts-from-baseline\n",
    "cw_client = boto3.Session().client('cloudwatch')\n",
    "\n",
    "alarm_name = 'BASELINE_DRIFT_FEATURE_DIAMETER'\n",
    "alarm_desc = 'Trigger a CloudWatch alarm when the feature diameter drifts away from the baseline'\n",
    "feature_diameter_drift_threshold = 0.1  # Setting this threshold purposefully low to see the alarm quickly.\n",
    "metric_name = 'feature_baseline_drift_diameter'\n",
    "namespace = 'aws/sagemaker/Endpoints/data-metrics'\n",
    "\n",
    "cw_client.put_metric_alarm(\n",
    "    AlarmName=alarm_name,\n",
    "    AlarmDescription=alarm_desc,\n",
    "    ActionsEnabled=True,\n",
    "    AlarmActions=[sns_notifications_topic],\n",
    "    MetricName=metric_name,\n",
    "    Namespace=namespace,\n",
    "    Statistic='Sum',\n",
    "    Dimensions=[\n",
    "        {\n",
    "            'Name': 'Endpoint',\n",
    "            'Value': endpoint_name\n",
    "        },\n",
    "        {\n",
    "            'Name': 'MonitoringSchedule',\n",
    "            'Value': mon_schedule_name\n",
    "        }\n",
    "    ],\n",
    "    Period=600,\n",
    "    EvaluationPeriods=1,\n",
    "    DatapointsToAlarm=1,\n",
    "    Threshold=feature_diameter_drift_threshold,\n",
    "    ComparisonOperator='GreaterThanOrEqualToThreshold',\n",
    "    TreatMissingData='breaching'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You created a CloudWatch alarm. You can use this alarm to notify you of any data drift issues and trigger automatic model retraining. However, this alarm is set to become an alert state immediately after creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file.\n",
    "- Return to the lab session and continue with **Task 3**."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
